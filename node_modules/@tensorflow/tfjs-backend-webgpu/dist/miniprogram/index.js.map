{"version":3,"file":"tf-backend-webgpu.min.js","sources":["../../../../node_modules/tslib/tslib.es6.js","../../../../tfjs-backend-webgpu/src/flags_webgpu.ts","../../../../tfjs-backend-webgpu/src/adapter_info.ts","../../../../tfjs-backend-webgpu/src/buffer_manager.ts","../../../../tfjs-backend-webgpu/src/texture_manager.ts","../../../../tfjs-backend-webgpu/src/shader_util.ts","../../../../tfjs-backend-webgpu/src/webgpu_program.ts","../../../../tfjs-backend-webgpu/src/webgpu_util.ts","../../../../tfjs-backend-webgpu/src/backend_webgpu.ts","../../../../tfjs-backend-webgpu/src/base.ts","../../../../tfjs-backend-webgpu/src/binary_op_util.ts","../../../../tfjs-backend-webgpu/src/unary_op_util.ts","../../../../tfjs-backend-webgpu/src/activation_util.ts","../../../../tfjs-backend-webgpu/src/matmul_packed_webgpu.ts","../../../../tfjs-backend-webgpu/src/matmul_reduce_webgpu.ts","../../../../tfjs-backend-webgpu/src/matmul_small_output_size_webgpu.ts","../../../../tfjs-backend-webgpu/src/matmul_splitK_webgpu.ts","../../../../tfjs-backend-webgpu/src/fill_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Fill.ts","../../../../tfjs-backend-webgpu/src/kernels/Reshape.ts","../../../../tfjs-backend-webgpu/src/kernels/BatchMatMul_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/_FusedMatMul.ts","../../../../tfjs-backend-webgpu/src/binary_op_complex_webgpu.ts","../../../../tfjs-backend-webgpu/src/binary_op_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Identity.ts","../../../../tfjs-backend-webgpu/src/kernels/Complex.ts","../../../../tfjs-backend-webgpu/src/unary_op_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernel_utils/kernel_funcs_utils.ts","../../../../../tfjs-backend-cpu/src/utils/binary_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Add.ts","../../../../../tfjs-backend-cpu/src/utils/unary_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Ceil.ts","../../../../../tfjs-backend-cpu/src/kernels/Equal.ts","../../../../../tfjs-backend-cpu/src/kernels/Exp.ts","../../../../../tfjs-backend-cpu/src/kernels/Expm1.ts","../../../../../tfjs-backend-cpu/src/kernels/Floor.ts","../../../../../tfjs-backend-cpu/src/kernels/Greater.ts","../../../../../tfjs-backend-cpu/src/kernels/GreaterEqual.ts","../../../../../tfjs-backend-cpu/src/kernels/Less.ts","../../../../../tfjs-backend-cpu/src/kernels/LessEqual.ts","../../../../../tfjs-backend-cpu/src/kernels/Log.ts","../../../../../tfjs-backend-cpu/src/kernels/Maximum.ts","../../../../../tfjs-backend-cpu/src/kernels/Minimum.ts","../../../../../tfjs-backend-cpu/src/kernels/Multiply.ts","../../../../../tfjs-backend-cpu/src/kernels/NotEqual.ts","../../../../../tfjs-backend-cpu/src/kernels/RaggedTensorToTensor_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Rsqrt.ts","../../../../../tfjs-backend-cpu/src/kernels/StringNGrams_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Sub.ts","../../../../../tfjs-backend-cpu/src/kernels/TopK_impl.ts","../../../../tfjs-backend-webgpu/src/kernel_utils/shared.ts","../../../../../tfjs-backend-cpu/src/kernels/Cast.ts","../../../../../tfjs-backend-cpu/src/kernels/Concat_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/GatherNd_Impl.ts","../../../../../tfjs-backend-cpu/src/kernels/GatherV2_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Max_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Neg.ts","../../../../../tfjs-backend-cpu/src/kernels/Prod.ts","../../../../../tfjs-backend-cpu/src/kernels/Range_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Scatter_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Abs.ts","../../../../../tfjs-backend-cpu/src/kernels/Slice.ts","../../../../../tfjs-backend-cpu/src/kernels/StridedSlice_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Tile_impl.ts","../../../../../tfjs-backend-cpu/src/kernels/Transpose_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/Abs.ts","../../../../tfjs-backend-webgpu/src/kernels/Add.ts","../../../../tfjs-backend-webgpu/src/addn_packed_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/AddN.ts","../../../../tfjs-backend-webgpu/src/argminmax_webgpu.ts","../../../../tfjs-backend-webgpu/src/transpose_shared_webgpu.ts","../../../../tfjs-backend-webgpu/src/transpose_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Transpose.ts","../../../../tfjs-backend-webgpu/src/kernels/ArgMax.ts","../../../../tfjs-backend-webgpu/src/kernels/ArgMin.ts","../../../../tfjs-backend-webgpu/src/kernels/Atan2.ts","../../../../tfjs-backend-webgpu/src/pool2d_webgpu.ts","../../../../tfjs-backend-webgpu/src/pool_filtersizeone_webgpu.ts","../../../../tfjs-backend-webgpu/src/reduce_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernel_utils/reduce.ts","../../../../tfjs-backend-webgpu/src/kernels/Max.ts","../../../../tfjs-backend-webgpu/src/kernels/Mean.ts","../../../../tfjs-backend-webgpu/src/kernels/Pool_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/AvgPool.ts","../../../../tfjs-backend-webgpu/src/kernels/BatchMatMul.ts","../../../../tfjs-backend-webgpu/src/slice_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Slice.ts","../../../../tfjs-backend-webgpu/src/kernels/BatchToSpaceND.ts","../../../../tfjs-backend-webgpu/src/kernels/NotEqual.ts","../../../../tfjs-backend-webgpu/src/kernels/Real.ts","../../../../tfjs-backend-webgpu/src/kernels/Cast.ts","../../../../tfjs-backend-webgpu/src/kernel_utils/int.ts","../../../../tfjs-backend-webgpu/src/kernels/Ceil.ts","../../../../tfjs-backend-webgpu/src/clip_vec4_webgpu.ts","../../../../tfjs-backend-webgpu/src/clip_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/ClipByValue.ts","../../../../tfjs-backend-webgpu/src/concat_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Imag.ts","../../../../tfjs-backend-webgpu/src/kernels/Concat_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/Concat.ts","../../../../tfjs-backend-webgpu/src/conv2d_mm_webgpu.ts","../../../../tfjs-backend-webgpu/src/conv2d_naive_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Conv2D_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/Conv2D.ts","../../../../tfjs-backend-webgpu/src/conv_backprop_mm_webgpu.ts","../../../../tfjs-backend-webgpu/src/conv_backprop_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Conv2DBackpropInput.ts","../../../../tfjs-backend-webgpu/src/cum_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Cos.ts","../../../../tfjs-backend-webgpu/src/kernels/Cosh.ts","../../../../tfjs-backend-webgpu/src/crop_and_resize_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/CropAndResize.ts","../../../../tfjs-backend-webgpu/src/kernels/Cum_impl.ts","../../../../tfjs-backend-webgpu/src/kernels/Cumprod.ts","../../../../tfjs-backend-webgpu/src/kernels/Cumsum.ts","../../../../tfjs-backend-webgpu/src/depth_to_space_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/DepthToSpace.ts","../../../../tfjs-backend-webgpu/src/depthwise_conv2d_nchw_shared_webgpu.ts","../../../../tfjs-backend-webgpu/src/depthwise_conv2d_vec4_webgpu.ts","../../../../tfjs-backend-webgpu/src/depthwise_conv2d_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/DepthwiseConv2dNative.ts","../../../../tfjs-backend-webgpu/src/kernels/Multiply.ts","../../../../tfjs-backend-webgpu/src/kernels/Sum.ts","../../../../tfjs-backend-webgpu/src/kernels/Einsum.ts","../../../../tfjs-backend-webgpu/src/kernels/Elu.ts","../../../../tfjs-backend-webgpu/src/kernels/Equal.ts","../../../../tfjs-backend-webgpu/src/kernels/Exp.ts","../../../../tfjs-backend-webgpu/src/kernels/ExpandDims.ts","../../../../tfjs-backend-webgpu/src/kernels/FromPixels.ts","../../../../tfjs-backend-webgpu/src/kernels/Expm1.ts","../../../../tfjs-backend-webgpu/src/flip_left_right_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/FlipLeftRight.ts","../../../../tfjs-backend-webgpu/src/kernels/Floor.ts","../../../../tfjs-backend-webgpu/src/kernels/FloorDiv.ts","../../../../tfjs-backend-webgpu/src/from_pixels_webgpu.ts","../../../../tfjs-backend-webgpu/src/batchnorm_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/FusedBatchNorm.ts","../../../../tfjs-backend-webgpu/src/kernels/FusedConv2D.ts","../../../../tfjs-backend-webgpu/src/kernels/FusedDepthwiseConv2D.ts","../../../../tfjs-backend-webgpu/src/gather_nd_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/GatherNd.ts","../../../../tfjs-backend-webgpu/src/gather_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/GatherV2.ts","../../../../tfjs-backend-webgpu/src/kernels/Greater.ts","../../../../tfjs-backend-webgpu/src/kernels/GreaterEqual.ts","../../../../tfjs-backend-webgpu/src/kernels/IsNaN.ts","../../../../tfjs-backend-webgpu/src/kernels/LeakyRelu.ts","../../../../tfjs-backend-webgpu/src/kernels/Less.ts","../../../../tfjs-backend-webgpu/src/kernels/LessEqual.ts","../../../../tfjs-backend-webgpu/src/kernels/Log.ts","../../../../tfjs-backend-webgpu/src/kernels/LogicalAnd.ts","../../../../tfjs-backend-webgpu/src/kernels/LogicalNot.ts","../../../../tfjs-backend-webgpu/src/kernels/Maximum.ts","../../../../tfjs-backend-webgpu/src/kernels/MaxPool.ts","../../../../tfjs-backend-webgpu/src/kernels/Min.ts","../../../../tfjs-backend-webgpu/src/kernels/Minimum.ts","../../../../tfjs-backend-webgpu/src/mirror_pad_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/MirrorPad.ts","../../../../tfjs-backend-webgpu/src/kernels/Neg.ts","../../../../tfjs-backend-webgpu/src/kernels/NonMaxSuppressionV3.ts","../../../../tfjs-backend-webgpu/src/kernels/NonMaxSuppressionV5.ts","../../../../tfjs-backend-webgpu/src/kernels/ZerosLike.ts","../../../../tfjs-backend-webgpu/src/kernels/OnesLike.ts","../../../../tfjs-backend-webgpu/src/kernels/Pack.ts","../../../../tfjs-backend-webgpu/src/pad_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/PadV2.ts","../../../../tfjs-backend-webgpu/src/kernels/Pow.ts","../../../../tfjs-backend-webgpu/src/kernels/Prelu.ts","../../../../tfjs-backend-webgpu/src/kernels/Prod.ts","../../../../tfjs-backend-webgpu/src/kernels/Range.ts","../../../../tfjs-backend-webgpu/src/kernels/RealDiv.ts","../../../../tfjs-backend-webgpu/src/kernels/Reciprocal.ts","../../../../tfjs-backend-webgpu/src/kernels/Relu.ts","../../../../tfjs-backend-webgpu/src/kernels/Relu6.ts","../../../../tfjs-backend-webgpu/src/resize_bilinear_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/ResizeBilinear.ts","../../../../tfjs-backend-webgpu/src/resize_nearest_neighbor_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/ResizeNearestNeighbor.ts","../../../../tfjs-backend-webgpu/src/rotate_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/RotateWithOffset.ts","../../../../tfjs-backend-webgpu/src/kernels/Rsqrt.ts","../../../../tfjs-backend-webgpu/src/scatter_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/ScatterNd.ts","../../../../tfjs-backend-webgpu/src/select_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Select.ts","../../../../tfjs-backend-webgpu/src/kernels/Sigmoid.ts","../../../../tfjs-backend-webgpu/src/kernels/Sin.ts","../../../../tfjs-backend-webgpu/src/kernels/Sinh.ts","../../../../tfjs-backend-webgpu/src/kernels/Sub.ts","../../../../tfjs-backend-webgpu/src/kernels/Softmax.ts","../../../../tfjs-backend-webgpu/src/kernels/SpaceToBatchND.ts","../../../../tfjs-backend-webgpu/src/tile_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Tile.ts","../../../../tfjs-backend-webgpu/src/kernels/SparseToDense.ts","../../../../tfjs-backend-webgpu/src/kernels/SplitV.ts","../../../../tfjs-backend-webgpu/src/kernels/Sqrt.ts","../../../../tfjs-backend-webgpu/src/kernels/Square.ts","../../../../tfjs-backend-webgpu/src/kernels/SquaredDifference.ts","../../../../tfjs-backend-webgpu/src/strided_slice_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/StridedSlice.ts","../../../../tfjs-backend-webgpu/src/kernels/StringNGrams.ts","../../../../tfjs-backend-webgpu/src/kernels/Tanh.ts","../../../../tfjs-backend-webgpu/src/top_k_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/TopK.ts","../../../../tfjs-backend-webgpu/src/transform_webgpu.ts","../../../../tfjs-backend-webgpu/src/kernels/Transform.ts","../../../../tfjs-backend-webgpu/src/kernels/Unpack.ts","../../../../tfjs-backend-webgpu/src/register_all_kernels.ts"],"sourcesContent":["/******************************************************************************\r\nCopyright (c) Microsoft Corporation.\r\n\r\nPermission to use, copy, modify, and/or distribute this software for any\r\npurpose with or without fee is hereby granted.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\r\nREGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY\r\nAND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\r\nINDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\r\nLOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\r\nOTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\r\nPERFORMANCE OF THIS SOFTWARE.\r\n***************************************************************************** */\r\n/* global Reflect, Promise */\r\n\r\nvar extendStatics = function(d, b) {\r\n    extendStatics = Object.setPrototypeOf ||\r\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\r\n        function (d, b) { for (var p in b) if (Object.prototype.hasOwnProperty.call(b, p)) d[p] = b[p]; };\r\n    return extendStatics(d, b);\r\n};\r\n\r\nexport function __extends(d, b) {\r\n    if (typeof b !== \"function\" && b !== null)\r\n        throw new TypeError(\"Class extends value \" + String(b) + \" is not a constructor or null\");\r\n    extendStatics(d, b);\r\n    function __() { this.constructor = d; }\r\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\r\n}\r\n\r\nexport var __assign = function() {\r\n    __assign = Object.assign || function __assign(t) {\r\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\r\n            s = arguments[i];\r\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\r\n        }\r\n        return t;\r\n    }\r\n    return __assign.apply(this, arguments);\r\n}\r\n\r\nexport function __rest(s, e) {\r\n    var t = {};\r\n    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)\r\n        t[p] = s[p];\r\n    if (s != null && typeof Object.getOwnPropertySymbols === \"function\")\r\n        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {\r\n            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))\r\n                t[p[i]] = s[p[i]];\r\n        }\r\n    return t;\r\n}\r\n\r\nexport function __decorate(decorators, target, key, desc) {\r\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\r\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\r\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\r\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\r\n}\r\n\r\nexport function __param(paramIndex, decorator) {\r\n    return function (target, key) { decorator(target, key, paramIndex); }\r\n}\r\n\r\nexport function __metadata(metadataKey, metadataValue) {\r\n    if (typeof Reflect === \"object\" && typeof Reflect.metadata === \"function\") return Reflect.metadata(metadataKey, metadataValue);\r\n}\r\n\r\nexport function __awaiter(thisArg, _arguments, P, generator) {\r\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\r\n    return new (P || (P = Promise))(function (resolve, reject) {\r\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\r\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\r\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\r\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\r\n    });\r\n}\r\n\r\nexport function __generator(thisArg, body) {\r\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\r\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\r\n    function verb(n) { return function (v) { return step([n, v]); }; }\r\n    function step(op) {\r\n        if (f) throw new TypeError(\"Generator is already executing.\");\r\n        while (_) try {\r\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\r\n            if (y = 0, t) op = [op[0] & 2, t.value];\r\n            switch (op[0]) {\r\n                case 0: case 1: t = op; break;\r\n                case 4: _.label++; return { value: op[1], done: false };\r\n                case 5: _.label++; y = op[1]; op = [0]; continue;\r\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\r\n                default:\r\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\r\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\r\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\r\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\r\n                    if (t[2]) _.ops.pop();\r\n                    _.trys.pop(); continue;\r\n            }\r\n            op = body.call(thisArg, _);\r\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\r\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\r\n    }\r\n}\r\n\r\nexport var __createBinding = Object.create ? (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    var desc = Object.getOwnPropertyDescriptor(m, k);\r\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\r\n        desc = { enumerable: true, get: function() { return m[k]; } };\r\n    }\r\n    Object.defineProperty(o, k2, desc);\r\n}) : (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    o[k2] = m[k];\r\n});\r\n\r\nexport function __exportStar(m, o) {\r\n    for (var p in m) if (p !== \"default\" && !Object.prototype.hasOwnProperty.call(o, p)) __createBinding(o, m, p);\r\n}\r\n\r\nexport function __values(o) {\r\n    var s = typeof Symbol === \"function\" && Symbol.iterator, m = s && o[s], i = 0;\r\n    if (m) return m.call(o);\r\n    if (o && typeof o.length === \"number\") return {\r\n        next: function () {\r\n            if (o && i >= o.length) o = void 0;\r\n            return { value: o && o[i++], done: !o };\r\n        }\r\n    };\r\n    throw new TypeError(s ? \"Object is not iterable.\" : \"Symbol.iterator is not defined.\");\r\n}\r\n\r\nexport function __read(o, n) {\r\n    var m = typeof Symbol === \"function\" && o[Symbol.iterator];\r\n    if (!m) return o;\r\n    var i = m.call(o), r, ar = [], e;\r\n    try {\r\n        while ((n === void 0 || n-- > 0) && !(r = i.next()).done) ar.push(r.value);\r\n    }\r\n    catch (error) { e = { error: error }; }\r\n    finally {\r\n        try {\r\n            if (r && !r.done && (m = i[\"return\"])) m.call(i);\r\n        }\r\n        finally { if (e) throw e.error; }\r\n    }\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nexport function __spread() {\r\n    for (var ar = [], i = 0; i < arguments.length; i++)\r\n        ar = ar.concat(__read(arguments[i]));\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nexport function __spreadArrays() {\r\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\r\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\r\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\r\n            r[k] = a[j];\r\n    return r;\r\n}\r\n\r\nexport function __spreadArray(to, from, pack) {\r\n    if (pack || arguments.length === 2) for (var i = 0, l = from.length, ar; i < l; i++) {\r\n        if (ar || !(i in from)) {\r\n            if (!ar) ar = Array.prototype.slice.call(from, 0, i);\r\n            ar[i] = from[i];\r\n        }\r\n    }\r\n    return to.concat(ar || Array.prototype.slice.call(from));\r\n}\r\n\r\nexport function __await(v) {\r\n    return this instanceof __await ? (this.v = v, this) : new __await(v);\r\n}\r\n\r\nexport function __asyncGenerator(thisArg, _arguments, generator) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var g = generator.apply(thisArg, _arguments || []), i, q = [];\r\n    return i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i;\r\n    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }\r\n    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }\r\n    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }\r\n    function fulfill(value) { resume(\"next\", value); }\r\n    function reject(value) { resume(\"throw\", value); }\r\n    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }\r\n}\r\n\r\nexport function __asyncDelegator(o) {\r\n    var i, p;\r\n    return i = {}, verb(\"next\"), verb(\"throw\", function (e) { throw e; }), verb(\"return\"), i[Symbol.iterator] = function () { return this; }, i;\r\n    function verb(n, f) { i[n] = o[n] ? function (v) { return (p = !p) ? { value: __await(o[n](v)), done: n === \"return\" } : f ? f(v) : v; } : f; }\r\n}\r\n\r\nexport function __asyncValues(o) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var m = o[Symbol.asyncIterator], i;\r\n    return m ? m.call(o) : (o = typeof __values === \"function\" ? __values(o) : o[Symbol.iterator](), i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i);\r\n    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }\r\n    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }\r\n}\r\n\r\nexport function __makeTemplateObject(cooked, raw) {\r\n    if (Object.defineProperty) { Object.defineProperty(cooked, \"raw\", { value: raw }); } else { cooked.raw = raw; }\r\n    return cooked;\r\n};\r\n\r\nvar __setModuleDefault = Object.create ? (function(o, v) {\r\n    Object.defineProperty(o, \"default\", { enumerable: true, value: v });\r\n}) : function(o, v) {\r\n    o[\"default\"] = v;\r\n};\r\n\r\nexport function __importStar(mod) {\r\n    if (mod && mod.__esModule) return mod;\r\n    var result = {};\r\n    if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\r\n    __setModuleDefault(result, mod);\r\n    return result;\r\n}\r\n\r\nexport function __importDefault(mod) {\r\n    return (mod && mod.__esModule) ? mod : { default: mod };\r\n}\r\n\r\nexport function __classPrivateFieldGet(receiver, state, kind, f) {\r\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\r\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\r\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\r\n}\r\n\r\nexport function __classPrivateFieldSet(receiver, state, value, kind, f) {\r\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\r\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\r\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\r\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\r\n}\r\n\r\nexport function __classPrivateFieldIn(state, receiver) {\r\n    if (receiver === null || (typeof receiver !== \"object\" && typeof receiver !== \"function\")) throw new TypeError(\"Cannot use 'in' operator on non-object\");\r\n    return typeof state === \"function\" ? receiver === state : state.has(receiver);\r\n}\r\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {env} from '@tensorflow/tfjs-core';\n\nconst ENV = env();\n\n/** The batched dispatching calls size in the device queue. */\nENV.registerFlag('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE', () => 15);\n\n/**\n * Whether we forward execution to the CPU backend if tensors are small and\n * reside on the CPU.\n */\nENV.registerFlag('WEBGPU_CPU_FORWARD', () => true);\n\n/**\n * This flag is used to test different types of matmul programs.\n *\n * See MatMulProgramType in webgpu_util.ts for a list of available values.\n */\nENV.registerFlag('WEBGPU_MATMUL_PROGRAM_TYPE', () => -1);\n\n/**\n * Whether to use conv2dTranspose_naive which directly implement the\n * conv2dTranspose logic rather than using a matmul to simulate.\n */\nENV.registerFlag('WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE', () => false);\n\n/**\n * Whether we use low power GPU. Otherwise, a high performance GPU will be\n * requested.\n */\nENV.registerFlag('WEBGPU_USE_LOW_POWER_GPU', () => false);\n\n/**\n * Threshold for input tensor size that determines whether WebGPU backend will\n * delegate computation to CPU.\n *\n * Default value is 1000.\n */\nENV.registerFlag('WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD', () => 1000);\n\n/**\n * Whether to use a dummy canvas to make profiling tools like PIX work with\n * TFJS webgpu backend.\n */\nENV.registerFlag('WEBGPU_USE_PROFILE_TOOL', () => false);\n\n/**\n * Whether to use import API.\n */\nENV.registerFlag('WEBGPU_IMPORT_EXTERNAL_TEXTURE', () => true);\n\n/**\n * Whether to use conv2dNaive for debugging.\n */\nENV.registerFlag('WEBGPU_USE_NAIVE_CONV2D_DEBUG', () => false);\n","/**\n * @license\n * Copyright 2022 Google LLC.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// TODO: Remove it once webgpu/types is successfully upgraded.\n// https://github.com/tensorflow/tfjs/issues/6869\nexport interface GPUAdapterInfo {\n  vendor: string;\n  architecture: string;\n}\n\nexport class AdapterInfo {\n  private vendor: string;\n\n  constructor(adapterInfo: GPUAdapterInfo) {\n    if (adapterInfo) {\n      this.vendor = adapterInfo.vendor;\n    }\n  }\n\n  isIntel(): boolean {\n    return this.vendor === 'intel';\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nexport class BufferManager {\n  private numUsedBuffers = 0;\n  private numFreeBuffers = 0;\n  private freeBuffers: Map<string, GPUBuffer[]> = new Map();\n  private usedBuffers: Map<string, GPUBuffer[]> = new Map();\n\n  public numBytesUsed = 0;\n  public numBytesAllocated = 0;\n\n  constructor(private device: GPUDevice) {}\n\n  acquireUploadBuffer(size: number, usage: GPUBufferUsageFlags) {\n    return this.acquireBuffer(size, usage, true);\n  }\n\n  acquireBuffer(\n      size: number, usage: GPUBufferUsageFlags, mappedAtCreation = false) {\n    const key = getBufferKey(size, usage);\n    if (!this.freeBuffers.has(key)) {\n      this.freeBuffers.set(key, []);\n    }\n\n    if (!this.usedBuffers.has(key)) {\n      this.usedBuffers.set(key, []);\n    }\n\n    this.numBytesUsed += size;\n    this.numUsedBuffers++;\n\n    if (this.freeBuffers.get(key).length > 0) {\n      this.numFreeBuffers--;\n\n      const newBuffer = this.freeBuffers.get(key).shift();\n      this.usedBuffers.get(key).push(newBuffer);\n      return newBuffer;\n    }\n\n    this.numBytesAllocated += size;\n    const newBuffer = this.device.createBuffer({size, usage, mappedAtCreation});\n    this.usedBuffers.get(key).push(newBuffer);\n\n    return newBuffer;\n  }\n\n  releaseBuffer(buffer: GPUBuffer, size: number, usage: GPUBufferUsageFlags) {\n    if (this.freeBuffers.size === 0) {\n      return;\n    }\n\n    const key = getBufferKey(size, usage);\n    if (!this.freeBuffers.has(key)) {\n      this.freeBuffers.set(key, []);\n    }\n\n    this.freeBuffers.get(key).push(buffer);\n    this.numFreeBuffers++;\n    this.numUsedBuffers--;\n\n    const bufferList = this.usedBuffers.get(key);\n    const bufferIndex = bufferList.indexOf(buffer);\n    if (bufferIndex < 0) {\n      throw new Error(\n          'Cannot release a buffer that was never provided by this ' +\n          'buffer manager');\n    }\n    bufferList.splice(bufferIndex, 1);\n    this.numBytesUsed -= size;\n  }\n\n  releaseUploadBuffer(\n      buffer: GPUBuffer, size: number, usage: GPUBufferUsageFlags) {\n    buffer.mapAsync(GPUMapMode.WRITE)\n        .then(\n            () => {\n              this.releaseBuffer(buffer, size, usage);\n            },\n            (err) => {\n                // Do nothing;\n            });\n  }\n\n  getNumUsedBuffers(): number {\n    return this.numUsedBuffers;\n  }\n\n  getNumFreeBuffers(): number {\n    return this.numFreeBuffers;\n  }\n\n  dispose() {\n    this.freeBuffers.forEach((buffers, key) => {\n      buffers.forEach(buffer => {\n        buffer.destroy();\n      });\n    });\n\n    this.usedBuffers.forEach((buffers, key) => {\n      buffers.forEach(buffer => {\n        buffer.destroy();\n      });\n    });\n\n    this.freeBuffers = new Map();\n    this.usedBuffers = new Map();\n    this.numUsedBuffers = 0;\n    this.numFreeBuffers = 0;\n    this.numBytesUsed = 0;\n    this.numBytesAllocated = 0;\n  }\n}\n\nfunction getBufferKey(size: number, usage: GPUBufferUsageFlags) {\n  return `${size}_${usage}`;\n}\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nexport class TextureManager {\n  private numUsedTextures = 0;\n  private numFreeTextures = 0;\n  private freeTextures: Map<string, GPUTexture[]> = new Map();\n  private usedTextures: Map<string, GPUTexture[]> = new Map();\n\n  public numBytesUsed = 0;\n  public numBytesAllocated = 0;\n\n  constructor(private device: GPUDevice) {}\n\n  acquireTexture(\n      width: number, height: number, format: GPUTextureFormat,\n      usage: GPUTextureUsageFlags) {\n    const bytesPerElement = getBytesPerElement(format);\n    const byteSize = width * height * bytesPerElement;\n    const key = getTextureKey(width, height, format, usage);\n    if (!this.freeTextures.has(key)) {\n      this.freeTextures.set(key, []);\n    }\n\n    if (!this.usedTextures.has(key)) {\n      this.usedTextures.set(key, []);\n    }\n\n    this.numBytesUsed += byteSize;\n    this.numUsedTextures++;\n\n    if (this.freeTextures.get(key).length > 0) {\n      this.numFreeTextures--;\n\n      const newTexture = this.freeTextures.get(key).shift();\n      this.usedTextures.get(key).push(newTexture);\n      return newTexture;\n    }\n\n    this.numBytesAllocated += byteSize;\n\n    const newTexture = this.device.createTexture({\n      size: [width, height],\n      format,\n      usage,\n    });\n    this.usedTextures.get(key).push(newTexture);\n\n    return newTexture;\n  }\n\n  releaseTexture(\n      texture: GPUTexture, width: number, height: number,\n      format: GPUTextureFormat, usage: GPUTextureUsageFlags) {\n    if (this.freeTextures.size === 0) {\n      return;\n    }\n\n    const key = getTextureKey(width, height, format, usage);\n    if (!this.freeTextures.has(key)) {\n      this.freeTextures.set(key, []);\n    }\n\n    this.freeTextures.get(key).push(texture);\n    this.numFreeTextures++;\n    this.numUsedTextures--;\n\n    const textureList = this.usedTextures.get(key);\n    const textureIndex = textureList.indexOf(texture);\n    if (textureIndex < 0) {\n      throw new Error(\n          'Cannot release a texture that was never provided by this ' +\n          'texture manager');\n    }\n    textureList.splice(textureIndex, 1);\n    const bytesPerElement = getBytesPerElement(format);\n    const byteSize = width * height * bytesPerElement;\n    this.numBytesUsed -= byteSize;\n  }\n\n  getNumUsedTextures(): number {\n    return this.numUsedTextures;\n  }\n\n  getNumFreeTextures(): number {\n    return this.numFreeTextures;\n  }\n\n  dispose() {\n    this.freeTextures.forEach((textures, key) => {\n      textures.forEach(texture => {\n        texture.destroy();\n      });\n    });\n\n    this.usedTextures.forEach((textures, key) => {\n      textures.forEach(texture => {\n        texture.destroy();\n      });\n    });\n\n    this.freeTextures = new Map();\n    this.usedTextures = new Map();\n    this.numUsedTextures = 0;\n    this.numFreeTextures = 0;\n    this.numBytesUsed = 0;\n    this.numBytesAllocated = 0;\n  }\n}\n\nfunction getTextureKey(\n    width: number, height: number, format: GPUTextureFormat,\n    usage: GPUTextureUsageFlags) {\n  return `${width}_${height}_${format}_${usage}`;\n}\n\nfunction getBytesPerElement(format: GPUTextureFormat) {\n  if (format === 'rgba8unorm') {\n    return 16;\n  } else {\n    throw new Error(`${format} is not supported!`);\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// Generates GLSL that computes strides.\nexport function symbolicallyComputeStrides(\n    indicesArr: number[], variableName: string): string[] {\n  if (Math.max(...indicesArr) > 3) {\n    throw new Error('Cannot symbolically compute strides for rank > 4 tensor.');\n  }\n\n  const numCoords = indicesArr.length;\n  const shape = indicesArr.map(d => `${variableName}[${d}]`);\n  const strides = new Array(numCoords - 1);\n  strides[numCoords - 2] = shape[numCoords - 1];\n  for (let i = numCoords - 3; i >= 0; --i) {\n    strides[i] = `(${strides[i + 1]} * ${shape[i + 1]})`;\n  }\n\n  return strides;\n}\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DataType, Rank, ShapeMap, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {symbolicallyComputeStrides} from './shader_util';\n\nexport interface WebGPUProgram {\n  // Whether to use atomic built-in functions.\n  atomic?: boolean;\n  // dispatch specifies geometry of thread groups - derived from dispatchLayout.\n  dispatch: [number, number, number];\n  // dispatchLayout enumerates how tensor dimensions are distributed among\n  // dispatch x,y,z dimensions.\n  dispatchLayout: {x: number[], y?: number[], z?: number[]};\n  isFromPixels?: boolean;\n  isVec4?: boolean;\n  outputShape: number[];\n  // The unique key to distinguish different shader source code.\n  shaderKey: string;\n  // Whether to use output size for bounds checking.\n  size?: boolean;\n  uniforms?: string;\n  variableNames: string[];\n  // Describe each variable's type and must have one-one mapping with\n  // variableNames. If not set, all variables type will be either f32 or\n  // vec4<f32> based on isVec4 member.\n  variableTypes?: string[];\n  // workGroupSize.x * workGroupSize.y * workGroupSize.z = the number of threads\n  // in a thread group. Individual dimensions determines thread layout within\n  // the group.\n  workGroupSize: [number, number, number];\n  // Size of register cache in one dimension (assumes square cache).\n  // Each thread writes to workPerThread * workPerThread locations in the output\n  // buffer.\n  workPerThread?: number;\n  getUserCode: () => string;\n}\n\nexport const compileProgram =\n    (device: GPUDevice, program: WebGPUProgram, inputsData: InputInfo[],\n     output: TensorInfo): GPUComputePipeline => {\n      const outputData = {dtype: output.dtype, shape: output.shape};\n      const source = makeShader(inputsData, outputData, program);\n      const module = device.createShaderModule(\n          {code: source, label: program.constructor.name});\n      const pipeline = device.createComputePipeline({\n        compute: {module, entryPoint: '_start'},\n        label: program.constructor.name,\n        layout: 'auto'\n      });\n\n      return pipeline;\n    };\n\nexport function getCoordsDataType(rank: number): string {\n  if (rank <= 1) {\n    return 'i32';\n  } else if (rank === 2) {\n    return `vec2<i32>`;\n  } else if (rank === 3) {\n    return `vec3<i32>`;\n  } else if (rank === 4) {\n    return `vec4<i32>`;\n  } else if (rank === 5) {\n    return `vec5`;\n  } else if (rank === 6) {\n    return `vec6`;\n  } else {\n    throw Error(`GPU for rank ${rank} is not yet supported`);\n  }\n}\n\nexport function getCoordsXYZ(index: number): string {\n  if (index === 0) {\n    return 'x';\n  } else if (index === 1) {\n    return 'y';\n  } else if (index === 2) {\n    return 'z';\n  } else if (index === 3) {\n    return 'w';\n  } else if (index === 4) {\n    return 'u';\n  } else if (index === 5) {\n    return 'v';\n  } else {\n    throw Error(`Index ${index} is not yet supported`);\n  }\n}\n\nexport function getMainHeaderString(): string;\nexport function getMainHeaderString(index: string): string;\nexport function getMainHeaderString(...params: string[]): string {\n  let snippet: string;\n  switch (params.length) {\n    case 0:\n      snippet = `\n        ${getWorkGroupSizeString()}\n        fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,\n                  @builtin(global_invocation_id) GlobalId : vec3<u32>,\n                  @builtin(num_workgroups) NumWorkgroups : vec3<u32>) {\n          localId = LocalId;\n          globalId = GlobalId;\n          numWorkgroups = NumWorkgroups;\n          main();\n        }\n\n        fn main()\n      `;\n      break;\n    case 1:\n      snippet = `\n        ${getWorkGroupSizeString()}\n        fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,\n                  @builtin(global_invocation_id) GlobalId : vec3<u32>,\n                  @builtin(num_workgroups) NumWorkgroups : vec3<u32>) {\n          localId = LocalId;\n          globalId = GlobalId;\n          numWorkgroups = NumWorkgroups;\n          main(getGlobalIndex());\n        }\n\n        fn main(${params[0]} : i32)\n      `;\n      break;\n    default:\n      throw Error('Unreachable');\n  }\n  return snippet;\n}\n\nexport function getWorkGroupSizeString(): string {\n  return `\n  @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)\n`;\n}\n\nfunction makeShader(\n    inputInfo: InputInfo[], outputData: {dtype: DataType, shape: number[]},\n    program: WebGPUProgram): string {\n  const prefixSnippets: string[] = [];\n  prefixSnippets.push(`\n      const workGroupSizeX = ${program.workGroupSize[0]}u;\n      const workGroupSizeY = ${program.workGroupSize[1]}u;\n      const workGroupSizeZ = ${program.workGroupSize[2]}u;\n\n      var<private> localId: vec3<u32>;\n      var<private> globalId: vec3<u32>;\n      var<private> numWorkgroups: vec3<u32>;\n\n      // Only used when the y/z dimension of workgroup size is 1.\n      fn getGlobalIndex() -> i32 {\n        ${\n      isFlatDispatch(program) ?\n          `  return i32(globalId.x);` :\n          `  let localInvocationIndex = localId.z * workGroupSizeX * workGroupSizeY +\n                   localId.y * workGroupSizeX + localId.x;\n               let workGroupID = (globalId - localId)/vec3<u32>(\n                   workGroupSizeX, workGroupSizeY, workGroupSizeZ);\n\n               return i32((workGroupID.z * numWorkgroups.x * numWorkgroups.y +\n                   workGroupID.y * numWorkgroups.x + workGroupID.x) *\n                   (workGroupSizeX * workGroupSizeY * workGroupSizeZ) +\n                   localInvocationIndex);\n        `}\n      }\n    `);\n\n  if (program.isFromPixels) {\n    prefixSnippets.push(`\n        struct Uniform {\n          size            : i32,\n          numChannels     : i32,\n          outShapeStrides : vec2<i32>,\n        };\n\n        @group(0) @binding(0) var<storage, read_write> result: array<${\n        mapToWgslTypes(outputData.dtype, program.isVec4)}>;\n        @group(0) @binding(2) var<uniform> uniforms: Uniform;\n      `);\n    return [\n      commonSnippet,\n      prefixSnippets.join('\\n'),\n      getCoordsFromIndexSnippet(outputData.shape),\n      program.getUserCode(),\n    ].join('\\n');\n  }\n\n  let uniformDeclaration = 'struct Uniforms { NAN : f32, ';\n  program.variableNames.forEach((x, i) => {\n    const perDataType = getCoordsDataType(inputInfo[i].shape.length);\n    uniformDeclaration +=\n        `${x.charAt(0).toLowerCase() + x.slice(1)}Shape : ${perDataType}, `;\n  });\n  const outputDataType = getCoordsDataType(outputData.shape.length);\n  uniformDeclaration += `outShape : ${outputDataType}, `;\n  const stridesLength = outputData.shape.length - 1;\n  const stridesDataType = getCoordsDataType(stridesLength);\n  uniformDeclaration += `\n         outShapeStrides: ${stridesDataType}, `;\n\n  if (program.size) {\n    uniformDeclaration += 'size : i32, ';\n  }\n\n  if (program.uniforms) {\n    uniformDeclaration += program.uniforms;\n  }\n  uniformDeclaration += '};';\n  uniformDeclaration = insertAlignment(uniformDeclaration);\n\n  prefixSnippets.push(uniformDeclaration);\n\n  // Output buffer.\n  if (program.atomic) {\n    prefixSnippets.push(`\n      @group(0) @binding(0) var<storage, read_write> result: array<atomic<i32>>;\n    `);\n  } else {\n    prefixSnippets.push(`\n      @group(0) @binding(0) var<storage, read_write> result: array<${\n        mapToWgslTypes(outputData.dtype, program.isVec4)}>;\n    `);\n  }\n  program.variableNames.forEach((x, i) => {\n    prefixSnippets.push(`\n      @group(0) @binding(${1 + i}) var<storage, read> ${x}: array<${\n        program.variableTypes ?\n            program.variableTypes[i] :\n            mapToWgslTypes(inputInfo[i].dtype, program.isVec4)}>;\n        `);\n  });\n\n  if (uniformDeclaration !== '') {\n    prefixSnippets.push(`\n      @group(0) @binding(${\n        1 + program.variableNames.length}) var<uniform> uniforms: Uniforms;\n      `);\n  }\n\n  const coordsSnippet =\n      getOutputCoordsSnippet(outputData.shape, program.dispatchLayout);\n\n  const sources = [\n    commonSnippet, prefixSnippets.join('\\n'),\n    getCoordsFromIndexSnippet(outputData.shape), coordsSnippet,\n    getOutputIndexFromCoordsSnippet(outputData.shape.length)\n  ];\n  if (!program.atomic) {\n    sources.push(\n        setOutputSnippet(outputData.shape, outputData.dtype, program.isVec4));\n  }\n\n  const inputSnippet =\n      inputInfo\n          .map(\n              (x, i) => getInputSnippet(\n                  x, outputData.shape,\n                  program.variableTypes ?\n                      (program.variableTypes[i] === 'vec4<f32>') :\n                      program.isVec4,\n                  program.dispatchLayout.x.length === outputData.shape.length))\n          .join('\\n');\n  sources.push(inputSnippet);\n\n  sources.push(program.getUserCode());\n  const source = sources.join('\\n');\n  return source;\n}\n\nexport function makeShaderKey<R extends Rank>(\n    program: WebGPUProgram, shapes: Array<ShapeMap[R]>, inputsData: InputInfo[],\n    output: TensorInfo): string {\n  let key = program.shaderKey;\n  if (program.isFromPixels) {\n    return key;\n  }\n\n  const types = inputsData.map(d => d.dtype).concat(output.dtype);\n  const broadcastDims =\n      inputsData.map(d => backend_util.getBroadcastDims(d.shape, output.shape));\n  const inputShapesEqualsOutShape =\n      inputsData.map(d => util.arraysEqual(d.shape, output.shape)).join('_');\n  const broadcastDimsKey = broadcastDims.map(d => d.join('_')).join(';');\n\n  const flatDispatchString = isFlatDispatch(program) ? 'flatDispatch' : '';\n\n  key += '_' + (program.workGroupSize ? program.workGroupSize.join(',') : '') +\n      shapes.map(shape => shape.length).join(',') + types.join(',') +\n      program.variableNames.join(',') + broadcastDimsKey +\n      inputShapesEqualsOutShape + flatDispatchString;\n\n  return key;\n}\n\nconst commonSnippet = `\n  struct vec5 {x: i32, y: i32, z: i32, w: i32, u: i32};\n  struct vec6 {x: i32, y: i32, z: i32, w: i32, u: i32, v: i32};\n\n  // Checks whether coordinates lie within the bounds of the shape.\n  fn coordsInBounds2D(coord : vec2<i32>, shape : vec2<i32>) -> bool {\n    return all(coord >= vec2<i32>(0)) && all(coord < shape);\n  }\n  fn coordsInBounds3D(coord : vec3<i32>, shape : vec3<i32>) -> bool {\n    return all(coord >= vec3<i32>(0)) && all(coord < shape);\n  }\n  fn coordsInBounds4D(coord : vec4<i32>, shape : vec4<i32>) -> bool {\n    return all(coord >= vec4<i32>(0)) && all(coord < shape);\n  }\n\n  fn getIndexFromCoords1D(coord : i32, shape : i32) -> i32 {\n    return coord;\n  }\n  fn getIndexFromCoords2D(coords : vec2<i32>, shape : vec2<i32>) -> i32 {\n    return dot(coords, vec2<i32>(shape.y, 1));\n  }\n  fn getIndexFromCoords3D(coords : vec3<i32>, shape : vec3<i32>) -> i32 {\n    return dot(coords, vec3<i32>(shape.y * shape.z, shape.z, 1));\n  }\n  fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {\n    return dot(coords, vec4<i32>(\n        shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));\n  }\n  fn getIndexFromCoords5D(coords : vec5, shape : vec5) -> i32 {\n    let shapeStrides: vec5 = vec5(shape.y * shape.z * shape.w * shape.u, shape.z * shape.w * shape.u, shape.w * shape.u, shape.u, 1);\n    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u;\n  }\n  fn getIndexFromCoords6D(coords : vec6, shape : vec6) -> i32 {\n    let shapeStrides: vec6 = vec6(shape.y * shape.z * shape.w * shape.u * shape.v, shape.z * shape.w * shape.u * shape.v, shape.w * shape.u * shape.v, shape.u * shape.v, shape.v, 1);\n    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u + coords.v*shapeStrides.v;\n  }\n\n  fn idiv(a: i32, b: i32, sign: f32) -> i32 {\n    var res: i32 = a / b;\n    let modulo: i32 = a % b;\n    if (sign < 0. && modulo != 0) {\n      res = res - 1;\n    }\n    return res;\n  }\n\n  // NaN defination in IEEE 754-1985 is :\n  //   - sign = either 0 or 1.\n  //   - biased exponent = all 1 bits.\n  //   - fraction = anything except all 0 bits (since all 0 bits represents infinity).\n  // https://en.wikipedia.org/wiki/IEEE_754-1985#Representation_of_non-numbers\n  fn isnan(val: f32) -> bool {\n    let floatToUint: u32 = bitcast<u32>(val);\n    return (floatToUint & 0x7fffffffu) > 0x7f800000u;\n  }\n  fn isnanVec4(val : vec4<f32>) -> vec4<bool> {\n    return vec4<bool>(isnan(val[0]), isnan(val[1]), isnan(val[2]), isnan(val[3]));\n  }\n`;\n\ntype InputInfo = {\n  dtype: DataType; shape: number[]; name: string;\n};\nexport type WGSLDataType = 'f32'|'i32'|'vec4<f32>'|'vec4<i32>'|'vec4<bool>';\n\n/**\n * Derives logical coordinates from a flat index. Performs integer division\n * with each stride and decrements the index until the index equals the final\n * dimension coordinate.\n */\nfunction getCoordsFromIndexSnippet(shape: number[]): string {\n  const rank = shape.length;\n\n  if (rank <= 1) {\n    return `fn getCoordsFromIndex(index : i32) -> i32 { return index; }`;\n  }\n\n  const strides = util.computeStrides(shape);\n  const dtype = getCoordsDataType(rank);\n\n  const coords: string[] = [];\n  for (let i = 0; i < rank; i++) {\n    coords.push(`d${i}`);\n  }\n\n  if (strides.length === 1) {\n    return `    fn getCoordsFromIndex(index : i32) -> vec2<i32> {\n      let d0 = index / uniforms.outShapeStrides; let d1 = index - d0 * uniforms.outShapeStrides;\n      return vec2<i32>(d0, d1);\n    }`;\n  }\n  let snippet;\n  snippet = 'var index2 = index;' +\n      strides\n          .map((_, i) => {\n            const line1 =\n                `let ${coords[i]} = index2 / uniforms.outShapeStrides.${\n                    getCoordsXYZ(i)}`;\n            const line2 = i === strides.length - 1 ?\n                `let ${coords[i + 1]} = index2 - ${\n                    coords[i]} * uniforms.outShapeStrides.${getCoordsXYZ(i)}` :\n                `index2 = index2 - ${coords[i]} * uniforms.outShapeStrides.${\n                    getCoordsXYZ(i)}`;\n            return `${line1}; ${line2};`;\n          })\n          .join('');\n\n  return `\n    fn getCoordsFromIndex(index : i32) -> ${dtype} {\n      ${snippet}\n      return ${dtype}(${coords.join(',')});\n    }\n  `;\n}\n\nfunction getInputAtCoordsSnippet(\n    inputInfo: InputInfo, isVec4: boolean): string {\n  const texName = inputInfo.name;\n  const rank = inputInfo.shape.length;\n  const type = getCoordsDataType(rank);\n  const funcName = 'get' + texName.charAt(0).toUpperCase() + texName.slice(1);\n  const dims = ['d0', 'd1', 'd2', 'd3', 'd4', 'd5'].slice(0, rank);\n  const inputs = dims.map(d => `${d} : i32`).join(', ');\n\n  if (rank < 1) {\n    if (isVec4) {\n      return `\n        fn ${funcName}() -> vec4<f32> {\n          return vec4<f32>(${texName}[0]);\n        }\n      `;\n    }\n\n    return `\n      fn ${funcName}() ->f32 {\n        return f32(${texName}[0]);\n      }\n    `;\n  }\n\n  const shapeStr =\n      `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;\n  let rankStr = `${rank}D`;\n  if (rank === 0) {\n    rankStr = '1D';\n  }\n\n  if (isVec4) {\n    return `\n      fn ${funcName}(${inputs}) -> vec4<f32> {\n        return vec4<f32>(${texName}[getIndexFromCoords${rankStr}(${type}(${\n        dims.join(',')}),\n          ${shapeStr}) / 4]);\n      }\n      `;\n  }\n\n  return `\n    fn ${funcName}(${inputs}) -> f32 {\n      return f32(${texName}[getIndexFromCoords${rankStr}(${type}(${\n      dims.join(',')}),\n        ${shapeStr})]);\n    }\n   `;\n}\n\nfunction getInputByOutputSnippet(\n    inputInfo: InputInfo, outShape: number[], isVec4: boolean,\n    isFlatDispatchLayout: boolean): string {\n  const texName = inputInfo.name;\n  const texFuncSnippet = texName.charAt(0).toUpperCase() + texName.slice(1);\n\n  const funcName = 'get' + texFuncSnippet + 'ByOutput';\n\n  const inRank = inputInfo.shape.length;\n  const outRank = outShape.length;\n  const type = getCoordsDataType(outRank);\n\n  // If the inShape equals the outShape and the dispatch layout is flat, we can\n  // directly use |gl_GlobalInvocationID.x| as the index and don't need coords\n  // conversion between these two shapes.\n  if (util.arraysEqual(inputInfo.shape, outShape) && isFlatDispatchLayout) {\n    if (isVec4) {\n      return `\n      fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {\n        return vec4<f32>(${texName}[globalIndex]);\n      }\n\n      fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {\n        return vec4<f32>(${texName}[${\n          outRank > 1 ? 'getOutputIndexFromCoords(coords)' : 'coords'} / 4]);\n      }\n      `;\n    } else {\n      return `\n    fn ${funcName}Index(globalIndex : i32) -> f32 {\n      return f32(${texName}[globalIndex]);\n    }\n\n    fn ${funcName}Coords(coords : ${type}) -> f32 {\n      return f32(${texName}[${\n          outRank > 1 ? 'getOutputIndexFromCoords(coords)' : 'coords'}]);\n    }\n    `;\n    }\n  }\n\n  const broadcastDims =\n      backend_util.getBroadcastDims(inputInfo.shape, outShape);\n  const rankDiff = outRank - inRank;\n\n  let coordsSnippet = '';\n\n  if (inRank === 0) {\n    if (isVec4) {\n      return `\n    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {\n      return get${texFuncSnippet}();\n    }\n\n    fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {\n      return get${texFuncSnippet}();\n    }\n  `;\n    }\n    return `\n    fn ${funcName}Index(globalIndex : i32) -> f32{\n      return get${texFuncSnippet}();\n    }\n\n    fn ${funcName}Coords(coords : ${type}) -> f32{\n      return get${texFuncSnippet}();\n    }\n  `;\n  } else {\n    if (outRank < 2 && broadcastDims.length >= 1) {\n      coordsSnippet = 'coords = 0;';\n    } else {\n      coordsSnippet =\n          broadcastDims.map(d => `coords.${getCoordsXYZ(d + rankDiff)} = 0;`)\n              .join('\\n');\n    }\n  }\n\n  let unpackedCoordsSnippet = '';\n  if (outRank < 2 && inRank > 0) {\n    unpackedCoordsSnippet = 'coords';\n  } else {\n    if (outRank > 1) {\n      const coordsType = getCoordsDataType(inRank);\n      const coordsValues =\n          inputInfo.shape.map((s, i) => `coords.${getCoordsXYZ(i + rankDiff)}`)\n              .join(', ');\n      unpackedCoordsSnippet = `${coordsType}(${coordsValues})`;\n    } else {\n      unpackedCoordsSnippet = 'coords';\n    }\n  }\n\n  const shapeStr =\n      `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;\n  const rankStr = `${inRank}D`;\n  if (isVec4) {\n    return `\n    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {\n      var coords = getCoordsFromIndex(globalIndex);\n      ${coordsSnippet}\n      return ${texName}[getIndexFromCoords${rankStr}(${\n        unpackedCoordsSnippet}, ${shapeStr}) / 4];\n    }\n\n    fn ${funcName}Coords(coordsIn : ${type}) -> vec4<f32> {\n      var coords = coordsIn;\n      ${coordsSnippet}\n      return ${texName}[getIndexFromCoords${rankStr}(${\n        unpackedCoordsSnippet}, ${shapeStr}) / 4];\n    }\n  `;\n  }\n\n  return `\n  fn ${funcName}Index(globalIndex : i32) -> f32 {\n    var coords = getCoordsFromIndex(globalIndex);\n    ${coordsSnippet}\n    return f32(${texName}[getIndexFromCoords${rankStr}(${\n      unpackedCoordsSnippet}, ${shapeStr})]);\n  }\n\n  fn ${funcName}Coords(coordsIn : ${type}) -> f32 {\n    var coords = coordsIn;\n    ${coordsSnippet}\n    return f32(${texName}[getIndexFromCoords${rankStr}(${\n      unpackedCoordsSnippet}, ${shapeStr})]);\n  }\n`;\n}\n\nfunction getInputSnippet(\n    inputInfo: InputInfo, outShape: number[], isVec4: boolean,\n    isFlatDispatchLayout: boolean): string {\n  let res = getInputAtCoordsSnippet(inputInfo, isVec4);\n\n  const inShape = inputInfo.shape;\n  if (inShape.length <= outShape.length) {\n    res += getInputByOutputSnippet(\n        inputInfo, outShape, isVec4, isFlatDispatchLayout);\n  }\n\n  return res;\n}\n\n/**\n * Generates getOutputCoords() function that computes output coordinates from\n * dispatch geometry to reduce arithmetic.\n */\nfunction getOutputCoordsSnippet(\n    outShape: number[],\n    dispatchLayout: {x: number[], y?: number[], z?: number[]}): string {\n  const {x, y = [], z = []} = dispatchLayout;\n\n  const outRank = outShape.length;\n  const rank = x.length + y.length + z.length;\n  // getOutputCoords is only meaningful when the output rank is same with\n  // dispatch layout rank.\n  if (rank !== outRank) {\n    return '';\n  }\n\n  if (x.length === outRank) {\n    const dtype = getCoordsDataType(outRank);\n    const snippet = `fn getOutputCoords() -> ${dtype}{\n    let globalIndex = getGlobalIndex();\n    return getCoordsFromIndex(globalIndex);\n  }\n  `;\n    return snippet;\n  }\n\n  let gatherDimensionsStr = '';\n  const dims = [x, y, z];\n\n  for (let i = 0; i < dims.length; i++) {\n    const arr = dims[i];\n\n    if (arr.length === 0) {\n      continue;\n    }\n\n    if (arr.length === 1) {\n      gatherDimensionsStr += `let d${arr[0]} = i32(globalId[${i}]);`;\n    } else {\n      const strides = symbolicallyComputeStrides(arr, 'uniforms.outShape');\n      gatherDimensionsStr += `var index${i} = i32(globalId[${i}]);`;\n      for (let j = 0; j < strides.length; j++) {\n        gatherDimensionsStr += `let d${arr[j]} = index${i} / ${strides[j]};`;\n\n        if (j === strides.length - 1) {\n          gatherDimensionsStr += `let d${arr[j + 1]} = ` +\n              `index${i} - d${arr[j]} * ${strides[j]};`;\n        } else {\n          gatherDimensionsStr +=\n              `index${i} = index${i} - d${arr[j]} * ${strides[j]};`;\n        }\n      }\n    }\n  }\n\n  const dimensions = [];\n  for (let i = 0; i < rank; i++) {\n    dimensions.push(`d${i}`);\n  }\n\n  const dtype = getCoordsDataType(rank);\n  let snippet = `fn getOutputCoords() -> ${dtype} {\n  ${gatherDimensionsStr}\n`;\n  if (dimensions.length === 0) {\n    snippet += `return ${dtype}(0); }`;\n  } else {\n    snippet += `return ${dtype}(${dimensions.join(',')}); }`;\n  }\n\n  return snippet;\n}\n\nfunction getOutputIndexFromCoordsSnippet(outRank: number) {\n  let snippet = '';\n  switch (outRank) {\n    case 0:\n    case 1:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : i32) -> i32 {\n          return coords;\n        }\n        `;\n      break;\n    case 2:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : vec2<i32>) -> i32 {\n          return dot(coords, vec2<i32>(uniforms.outShapeStrides, 1));\n        }\n        `;\n      break;\n    case 3:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : vec3<i32>) -> i32 {\n          return dot(coords, vec3<i32>(uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, 1));\n        }\n        `;\n      break;\n    case 4:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {\n          return dot(coords, vec4<i32>(\n            uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, uniforms.outShapeStrides.z, 1));\n        }\n        `;\n      break;\n    case 5:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : vec5) -> i32 {\n          return coords.x * uniforms.outShapeStrides.x +\n              coords.y * uniforms.outShapeStrides.y +\n              coords.z * uniforms.outShapeStrides.z +\n              coords.w * uniforms.outShapeStrides.w +\n              coords.u;\n        }\n        `;\n      break;\n    case 6:\n      snippet += `\n        fn getOutputIndexFromCoords(coords : vec6) -> i32 {\n          return coords.x * uniforms.outShapeStrides.x +\n              coords.y * uniforms.outShapeStrides.y +\n              coords.z * uniforms.outShapeStrides.z +\n              coords.w * uniforms.outShapeStrides.w +\n              coords.u * uniforms.outShapeStrides.u +\n              coords.v;\n        }\n        `;\n      break;\n    default:\n      util.assert(false, () => `Unsupported ${outRank}D shape`);\n      break;\n  }\n  return snippet;\n}\n\nfunction isFlatDispatch(program: WebGPUProgram): boolean {\n  return program.dispatch[1] === 1 && program.dispatch[2] === 1;\n}\n\nexport function mapToWgslTypes(type: DataType, isVec4: boolean): WGSLDataType|\n    DataType {\n  if (type === 'float32') {\n    return isVec4 ? 'vec4<f32>' : 'f32';\n  } else if (type === 'int32') {\n    return isVec4 ? 'vec4<i32>' : 'i32';\n  } else if (type === 'bool') {\n    // Type 'bool' cannot be used in storage class,\n    // https://www.w3.org/TR/WGSL/#host-shareable-types.\n    return isVec4 ? 'vec4<i32>' : 'i32';\n  }\n\n  return type;\n}\n\nfunction setOutputSnippet(\n    outShape: number[], outBufferType: DataType, isVec4: boolean): string {\n  const outRank = outShape.length;\n  const wgslType = mapToWgslTypes(outBufferType, isVec4);\n  let snippet;\n  if (isVec4) {\n    snippet = `fn setOutputAtIndex(flatIndex : i32, value : vec4<f32>) {\n      result[flatIndex] = ${wgslType}(value);\n    }\n    fn setOutputAtIndexI32(flatIndex : i32, value : vec4<i32>) {\n      result[flatIndex] = ${wgslType}(value);\n    }`;\n  } else {\n    snippet = `fn setOutputAtIndex(flatIndex : i32, value : f32) {\n      result[flatIndex] = ${wgslType}(value);\n    }\n    fn setOutputAtIndexI32(flatIndex : i32, value : i32) {\n      result[flatIndex] = ${wgslType}(value);\n    }`;\n  }\n  if (outRank >= 2) {\n    const dims = ['d0', 'd1', 'd2', 'd3', 'd4', 'd5'].slice(0, outRank);\n    const type = getCoordsDataType(outRank);\n\n    if (isVec4) {\n      snippet += `\n      fn setOutputAtCoords(${\n          dims.map(d => `${d} : i32`).join(', ')}, value : vec4<f32>) {\n        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));\n        setOutputAtIndex(flatIndex / 4, value);\n      }\n      fn setOutputAtCoordsI32(${\n          dims.map(d => `${d} : i32`).join(', ')}, value : vec4<i32>) {\n        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));\n        setOutputAtIndexI32(flatIndex / 4, value);\n      }\n    `;\n    } else {\n      snippet += `\n      fn setOutputAtCoords(${\n          dims.map(d => `${d} : i32`).join(', ')}, value : f32) {\n        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));\n        setOutputAtIndex(flatIndex, value);\n      }\n      fn setOutputAtCoordsI32(${\n          dims.map(d => `${d} : i32`).join(', ')}, value : i32) {\n        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));\n        setOutputAtIndexI32(flatIndex, value);\n      }\n    `;\n    }\n  }\n\n  return snippet;\n}\n\nfunction insertAlignment(uniformShader: string) {\n  // insert alignment when current pattern is vec5 or vec6\n  const curInsertRe = /(\\w+)\\s*:\\s*vec(5|6)/g;\n  uniformShader = uniformShader.replace(curInsertRe, (match) => {\n    return '@align(16) ' + match;\n  });\n\n  // insert alignment when previous pattern is vec5 or vec6\n  const preInsertRe = /vec(5|6)\\s*,\\s*(\\w+)/g;\n  uniformShader = uniformShader.replace(preInsertRe, (_, p1, p2) => {\n    return `vec${p1}, @align(16) ${p2}`;\n  });\n  return uniformShader;\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {DataType} from '@tensorflow/tfjs-core';\n\nconst arrayProduct = (arr: number[]) => {\n  let product = 1;\n  for (let i = 0; i < arr.length; i++) {\n    product *= arr[i];\n  }\n  return product;\n};\n\nexport function tilesFitEvenlyIntoShape(\n    tileSize: number[], shape: number[]): boolean {\n  if (tileSize.length !== shape.length) {\n    throw new Error(\n        `Cannot compute whether rank ${tileSize.length}` +\n        ` tiles fit evenly into rank ${shape.length} shape` +\n        ` - ranks must match.`);\n  }\n  return shape.every(\n      (dim: number, dimIdx: number) => dim % tileSize[dimIdx] === 0);\n}\n\n// Computes dispatch geometry based on layout of output dimensions and\n// workGroupSize.\nexport function computeDispatch(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    workGroupSize: [number, number, number] = [1, 1, 1],\n    elementsPerThread: [number, number, number] =\n        [1, 1, 1]): [number, number, number] {\n  const [dispatchX, dispatchY, dispatchZ] = [\n    Math.ceil(\n        arrayProduct(layout.x.map(d => outputShape[d])) /\n        (workGroupSize[0] * elementsPerThread[0])),\n    layout.y ? Math.ceil(\n                   arrayProduct(layout.y.map(d => outputShape[d])) /\n                   (workGroupSize[1] * elementsPerThread[1])) :\n               1,\n    layout.z ? Math.ceil(\n                   arrayProduct(layout.z.map(d => outputShape[d])) /\n                   (workGroupSize[2] * elementsPerThread[2])) :\n               1\n  ];\n  return [dispatchX, dispatchY, dispatchZ];\n}\n\nexport type WorkGroupInfo = {\n  workGroupSize: [number, number, number],\n  elementsPerThread: [number, number, number],\n};\n\nexport function computeWorkGroupInfoForMatMul(\n    dimAOuter: number, dimInner: number, dimBOuter: number,\n    transposeA = false): WorkGroupInfo {\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the input shapes to improve the EU occupancy.\n  // TODO: WebGPU limits the maximum allowed shared memory size as 16K. To make\n  // sure it doesn't exceed this limitations. Temporarily reduce the work group\n  // size to [8, 8, 1] and the work per thread size is [4, 4, 1]. But we should\n  // revisit it and find the balance between work group size and work per thread\n  // size.\n  const workGroupSize: [number, number, number] = [8, 8, 1];\n  const elementsPerThread: [number, number, number] = [4, 4, 1];\n\n  if (!transposeA) {\n    if (dimAOuter <= 8) {\n      elementsPerThread[1] = 1;\n    }\n\n    if (dimInner <= 16 && dimBOuter <= 16) {\n      workGroupSize[0] = 4;\n    }\n  }\n\n  return {workGroupSize, elementsPerThread};\n}\n\nexport function computeWorkGroupSizeForConv2d(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    isVec4 = false): [number, number, number] {\n  if (isVec4) {\n    return [8, 8, 1];\n  }\n\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the output shape. For example, when one dimension is smaller\n  // than 4, it will be wasteful if we assign a larger size for this dimension,\n  // which results lots of threads doing useless work and reduces parallelism\n  // of hardware threads. But it is always a balance between work group size\n  // and shared memory. If one dimension is too small, such as 1, shared memory\n  // will won't be fully utilized.\n  if (dim0 <= 4) {\n    return [4, 16, 1];\n  }\n  if (dim1 <= 4) {\n    return [16, 4, 1];\n  }\n\n  return [16, 16, 1];\n}\n\nexport function computeWorkPerThreadForConv2d(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    isVec4 = false): [number, number, number] {\n  if (isVec4) {\n    return [4, 4, 1];\n  }\n\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // The following conditions correspond to the values set in\n  // computeWorkGroupSizeForConv2d.\n  if (dim0 <= 4) {\n    return [1, 2, 1];\n  }\n  if (dim1 <= 4) {\n    return [2, 1, 1];\n  }\n\n  return [2, 2, 1];\n}\n\nexport function flatDispatchLayout(shape: number[]) {\n  return {x: shape.map((d, i) => i)};\n}\n\nexport function GPUBytesPerElement(dtype: DataType): number {\n  if (dtype === 'float32' || dtype === 'int32' || dtype === 'bool' ||\n      dtype === 'string') {\n    return 4;\n  } else if (dtype === 'complex64') {\n    return 8;\n  } else {\n    throw new Error(`Unknown dtype ${dtype}`);\n  }\n}\n\nexport function ArrayBufferToTypedArray(data: ArrayBuffer, dtype: DataType) {\n  if (dtype === 'float32') {\n    return new Float32Array(data);\n  } else if (dtype === 'int32') {\n    return new Int32Array(data);\n  } else if (dtype === 'bool' || dtype === 'string') {\n    return Uint8Array.from(new Int32Array(data));\n  } else {\n    throw new Error(`Unknown dtype ${dtype}`);\n  }\n}\n\nexport function isWebGPUSupported(): boolean {\n  return ((typeof window !== 'undefined') ||\n          //@ts-ignore\n          (typeof WorkerGlobalScope !== 'undefined')) &&\n      !!navigator.gpu;\n}\n\nexport enum MatMulProgramType {\n  MatMulReduceProgram,\n  MatMulSplitKProgram,\n  MatMulSmallOutputSizeProgram,\n  MatMulPackedProgram,\n  MatMulMax\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport './flags_webgpu';\n\nimport {backend_util, buffer, DataStorage, DataType, engine, env, GPUData, KernelBackend, Rank, RecursiveArray, ShapeMap, TensorBuffer, TensorInfo, TimingInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {AdapterInfo, GPUAdapterInfo} from './adapter_info';\nimport {BufferManager} from './buffer_manager';\nimport {TextureManager} from './texture_manager';\nimport * as webgpu_program from './webgpu_program';\nimport * as webgpu_util from './webgpu_util';\n\nexport interface WebGPUMemoryInfo extends backend_util.MemoryInfo {\n  numBytesInGPU: number;\n  numBytesAllocatedInGPU: number;\n  unreliable: boolean;\n}\n\nexport type BufferInfo = {\n  size: number,\n  usage: GPUBufferUsageFlags,\n  buffer: GPUBuffer\n};\n\nexport type TextureInfo = {\n  width: number,\n  height: number,\n  format: GPUTextureFormat,\n  usage: GPUTextureUsageFlags,\n  texture: GPUTexture|GPUExternalTexture\n};\n\ntype TensorData = {\n  values: backend_util.BackendValues,\n  dtype: DataType,\n  shape: number[],\n  refCount: number,\n  resourceInfo?: BufferInfo|TextureInfo,\n  // For complex numbers, the real and imaginary parts are stored as their own\n  // individual tensors, with a parent joining the two with the\n  // complexTensorInfos field.\n  complexTensorInfos?: {real: TensorInfo, imag: TensorInfo}\n};\n\ninterface DataId {}\n\nexport type WebGPUKernelInfo = {\n  name: string; query: Promise<number>;\n};\n\nexport type TimerNode = RecursiveArray<WebGPUKernelInfo>|WebGPUKernelInfo;\n\nexport interface WebGPUTimingInfo extends TimingInfo {\n  uploadWaitMs: number;\n  downloadWaitMs: number;\n}\n\ntype ProgramUniform = Array<{type: string; data: number[]}>;\n\n// Empirically determined constant used to determine size threshold for handing\n// off execution to the CPU.\nconst CPU_HANDOFF_SIZE_THRESHOLD =\n    env().getNumber('WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD');\n\n// Reshape dispatch, not to exceed device limits.\nconst reshapeDispatch =\n    (device: GPUDevice,\n     program: webgpu_program.WebGPUProgram): [number, number, number] => {\n      const MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE =\n          device.limits.maxComputeWorkgroupsPerDimension;\n      const layout = program['dispatchLayout'];\n      const dispatch = program['dispatch'];\n      if (dispatch.every((d) => d <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE)) {\n        return dispatch;\n      }\n\n      util.assert(\n          dispatch[0] > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE &&\n              layout.y === undefined && layout.z === undefined,\n          () => 'Dispatch size exceeds WebGPU limits in Y or Z dimension.');\n\n      let dispatchAverage = Math.ceil(Math.sqrt(dispatch[0]));\n      if (dispatchAverage > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE) {\n        dispatchAverage = Math.ceil(Math.cbrt(dispatch[0]));\n        util.assert(\n            dispatchAverage <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE,\n            () => 'Total dispatch size exceeds WebGPU maximum.');\n        return [dispatchAverage, dispatchAverage, dispatchAverage];\n      } else {\n        return [dispatchAverage, dispatchAverage, 1];\n      }\n    };\n\nexport class WebGPUBackend extends KernelBackend {\n  bufferManager: BufferManager;\n  adapterInfo: AdapterInfo;\n  device: GPUDevice;\n  queue: GPUQueue;\n  tensorMap: DataStorage<TensorData>;\n  textureManager: TextureManager;\n\n  private activeTimers: TimerNode[];\n  private currentCommandEncoder: GPUCommandEncoder;\n  private currentComputePass: GPUComputePassEncoder;\n  private commandQueueOwnedIds = new WeakSet<DataId>();\n  private dispatchNumberInEncoder = 0;\n  private disposed = false;\n  private downloadWaitMs = 0;\n  private dummyCanvas: HTMLCanvasElement;\n  private dummyContext: GPUCanvasContext;\n  private tensorDataPendingDisposal: DataId[] = [];\n  private static nextDataId = 0;\n  private pipelineCache: {[key: string]: GPUComputePipeline};\n  private programTimersStack: TimerNode[];\n  private querySet: GPUQuerySet;\n  private stagingPendingDisposal: BufferInfo[] = [];\n  private supportTimeQuery: boolean;\n  private uniformPendingDisposal: BufferInfo[] = [];\n  private uploadWaitMs = 0;\n\n  private nextDataId(): number {\n    return WebGPUBackend.nextDataId++;\n  }\n\n  constructor(device: GPUDevice, adapterInfo?: GPUAdapterInfo) {\n    super();\n    if (!webgpu_util.isWebGPUSupported()) {\n      throw new Error('WebGPU is not supported on this device');\n    }\n    this.pipelineCache = {};\n    this.device = device;\n    this.queue = device.queue;\n    this.currentCommandEncoder = null;\n    this.currentComputePass = null;\n    this.supportTimeQuery = device.features.has('timestamp-query');\n    this.adapterInfo = new AdapterInfo(adapterInfo);\n\n    this.bufferManager = new BufferManager(this.device);\n    this.textureManager = new TextureManager(this.device);\n    this.tensorMap = new DataStorage(this, engine());\n    if (this.supportTimeQuery) {\n      this.querySet = this.device.createQuerySet({\n        type: 'timestamp',\n        count: 2,\n      });\n    }\n\n    // Profiling tools like PIX needs this dummy canvas to\n    // trigger capturing a frame.\n    if (env().getBool('WEBGPU_USE_PROFILE_TOOL')) {\n      this.dummyCanvas = document.createElement('canvas');\n      this.dummyCanvas.width = 1;\n      this.dummyCanvas.height = 1;\n\n      this.dummyContext = this.dummyCanvas.getContext('webgpu');\n      this.dummyContext.configure({\n        device,\n        format: 'bgra8unorm',\n      });\n\n      document.body.appendChild(this.dummyCanvas);\n    }\n  }\n\n  floatPrecision(): 32 {\n    return 32;\n  }\n\n  defaultGpuBufferUsage(): number {\n    return GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC |\n        GPUBufferUsage.COPY_DST;\n  }\n\n  /**\n   * Dispose the memory if the dataId has 0 refCount. Return true if the memory\n   * is released or memory is not managed in this backend, false if memory is\n   * not cleared.\n   * @param dataId\n   * @oaram force Optional, remove the data regardless of refCount\n   */\n  disposeData(dataId: DataId, force = false): boolean {\n    if (this.tensorDataPendingDisposal.indexOf(dataId) >= 0) {\n      return false;\n    }\n    if (!this.tensorMap.has(dataId)) {\n      return true;\n    }\n\n    const tensorData = this.tensorMap.get(dataId);\n    this.decRef(dataId);\n    if (!force && tensorData.refCount > 0) {\n      return false;\n    }\n\n    // complex is never in commandQueueOwnedIds\n    if (this.commandQueueOwnedIds.has(dataId)) {\n      this.tensorDataPendingDisposal.push(dataId);\n      return false;\n    }\n\n    const {complexTensorInfos} = this.tensorMap.get(dataId);\n    if (complexTensorInfos != null) {\n      this.disposeData(complexTensorInfos.real.dataId, force);\n      this.disposeData(complexTensorInfos.imag.dataId, force);\n    }\n\n    this.releaseResource(dataId);\n    this.tensorMap.delete(dataId);\n\n    return true;\n  }\n\n  memory(): WebGPUMemoryInfo {\n    return {\n      numBytesInGPU: this.bufferManager.numBytesUsed,\n      numBytesAllocatedInGPU: this.bufferManager.numBytesAllocated,\n      unreliable: false\n    } as WebGPUMemoryInfo;\n  }\n\n  releaseResource(dataId: DataId) {\n    const tensorData = this.tensorMap.get(dataId);\n    if (!tensorData || !tensorData.resourceInfo) {\n      return;\n    }\n    if ('texture' in tensorData.resourceInfo) {\n      const textureInfo = tensorData.resourceInfo;\n      if (textureInfo.texture instanceof GPUTexture) {\n        this.textureManager.releaseTexture(\n            textureInfo.texture, textureInfo.width, textureInfo.height,\n            textureInfo.format, textureInfo.usage);\n      }\n      textureInfo.texture = null;\n    } else {\n      const bufferInfo = tensorData.resourceInfo;\n      this.bufferManager.releaseBuffer(\n          bufferInfo.buffer, bufferInfo.size, bufferInfo.usage);\n      bufferInfo.buffer = null;\n    }\n    tensorData.resourceInfo = null;\n  }\n\n  /** Return refCount of a `TensorData`. */\n  refCount(dataId: DataId): number {\n    if (this.tensorMap.has(dataId)) {\n      const tensorData = this.tensorMap.get(dataId);\n      return tensorData.refCount;\n    }\n    return 0;\n  }\n\n  /** Increase refCount of a `TensorData`. */\n  incRef(dataId: DataId): void {\n    const tensorData = this.tensorMap.get(dataId);\n    tensorData.refCount++;\n  }\n\n  /** Decrease refCount of a `TensorData`. */\n  decRef(dataId: DataId): void {\n    if (this.tensorMap.has(dataId)) {\n      const tensorData = this.tensorMap.get(dataId);\n      tensorData.refCount--;\n    }\n  }\n\n  write(values: backend_util.BackendValues, shape: number[], dtype: DataType):\n      DataId {\n    if (dtype === 'complex64' && values != null) {\n      throw new Error(\n          `Cannot write to a complex64 dtype. ` +\n          `Please use tf.complex(real, imag).`);\n    }\n    const dataId = {id: this.nextDataId()};\n    this.tensorMap.set(dataId, {dtype, shape, values, refCount: 1});\n    return dataId;\n  }\n\n  move(\n      dataId: DataId, values: backend_util.BackendValues, shape: number[],\n      dtype: DataType, refCount: number): void {\n    if (dtype === 'complex64') {\n      throw new Error(\n          `Cannot write to a complex64 dtype. ` +\n          `Please use tf.complex(real, imag).`);\n    }\n    this.tensorMap.set(dataId, {dtype, shape, values, refCount});\n  }\n\n  submitQueue() {\n    this.ensureComputePassEnded();\n    this.queue.submit([this.currentCommandEncoder.finish()]);\n    this.currentCommandEncoder = null;\n    this.dispatchNumberInEncoder = 0;\n\n    this.commandQueueOwnedIds = new WeakSet<DataId>();\n\n    this.tensorDataPendingDisposal.forEach(d => {\n      this.releaseResource(d);\n      this.tensorMap.delete(d);\n    });\n    this.uniformPendingDisposal.forEach(\n        d => this.bufferManager.releaseBuffer(d.buffer, d.size, d.usage));\n    this.stagingPendingDisposal.forEach(\n        d => this.bufferManager.releaseUploadBuffer(d.buffer, d.size, d.usage));\n\n    this.tensorDataPendingDisposal = [];\n    this.uniformPendingDisposal = [];\n    this.stagingPendingDisposal = [];\n  }\n\n  ensureCommandEncoderReady() {\n    if (!this.currentCommandEncoder) {\n      this.currentCommandEncoder = this.device.createCommandEncoder();\n    }\n  }\n\n  ensureComputePassEnded() {\n    if (this.currentComputePass) {\n      this.currentComputePass.end();\n      this.currentComputePass = null;\n    }\n  }\n\n  getComputePass() {\n    if (!this.currentComputePass) {\n      this.currentComputePass = this.currentCommandEncoder.beginComputePass();\n    }\n    return this.currentComputePass;\n  }\n\n  public async getBufferData(buffer: GPUBuffer, size: number):\n      Promise<backend_util.BackendValues> {\n    const staging = this.bufferManager.acquireBuffer(\n        size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);\n    this.ensureCommandEncoderReady();\n    this.ensureComputePassEnded();\n    this.currentCommandEncoder.copyBufferToBuffer(buffer, 0, staging, 0, size);\n    this.submitQueue();\n\n    await staging.mapAsync(GPUMapMode.READ);\n    const values = staging.getMappedRange().slice(0);\n\n    staging.unmap();\n    if (staging != null) {\n      this.bufferManager.releaseBuffer(\n          staging, size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);\n    }\n\n    // Need to get texture from swapChain to enable profiling tool\n    // to capture a frame\n    if (env().getBool('WEBGPU_USE_PROFILE_TOOL')) {\n      util.assert(\n          this.dummyContext !== undefined,\n          () => `Fail to get context for profiling tool`);\n      this.dummyContext.getCurrentTexture();\n    }\n\n    return values as backend_util.BackendValues;\n  }\n\n  private convertAndCacheOnCPU(dataId: DataId, data: backend_util.TypedArray):\n      backend_util.TypedArray {\n    const tensorData = this.tensorMap.get(dataId);\n    this.releaseResource(dataId);\n    tensorData.values = data;\n    return tensorData.values;\n  }\n\n  // TODO: Remove once this is fixed:\n  // https://github.com/tensorflow/tfjs/issues/1595\n  readSync(dataId: object): backend_util.BackendValues {\n    const tensorData = this.tensorMap.get(dataId);\n    const {values} = tensorData;\n\n    if (values == null) {\n      throw new Error(\n          'WebGPU readSync is only available for CPU-resident tensors.');\n    }\n\n    return values;\n  }\n\n  async read(dataId: object): Promise<backend_util.BackendValues> {\n    if (!this.tensorMap.has(dataId)) {\n      throw new Error(`Tensor ${dataId} was not registered!`);\n    }\n    const tensorData = this.tensorMap.get(dataId);\n\n    const {values} = tensorData;\n\n    if (values != null) {\n      // TODO(xing.xu@intel.com): Merge backend_util.BackendValues and\n      // backend_util.TypedArray.\n      return this.convertAndCacheOnCPU(\n                 dataId, values as backend_util.TypedArray) as\n          backend_util.BackendValues;\n    }\n\n    // Download the values from the GPU.\n    let vals: backend_util.BackendValues;\n    if (tensorData.dtype === 'complex64') {\n      const ps = await Promise.all([\n        this.read(tensorData.complexTensorInfos.real.dataId),\n        this.read(tensorData.complexTensorInfos.imag.dataId)\n      ]);\n\n      const realValues = ps[0];\n      const imagValues = ps[1];\n      vals = backend_util.mergeRealAndImagArrays(\n          realValues as Float32Array, imagValues as Float32Array);\n    } else {\n      const bufferInfo = tensorData.resourceInfo as BufferInfo;\n      const data = await this.getBufferData(bufferInfo.buffer, bufferInfo.size);\n      vals = webgpu_util.ArrayBufferToTypedArray(\n          data as ArrayBuffer, tensorData.dtype);\n    }\n    this.convertAndCacheOnCPU(dataId, vals);\n    return vals;\n  }\n\n  /**\n   * Read tensor to a new GPUBuffer.\n   * @param dataId The source tensor.\n   */\n  readToGPU(dataId: DataId): GPUData {\n    const srcTensorData = this.tensorMap.get(dataId);\n    const {values, dtype, shape, resourceInfo} = srcTensorData;\n\n    if (dtype === 'complex64') {\n      throw new Error('Does not support reading buffer for complex64 dtype.');\n    }\n\n    if (resourceInfo == null) {\n      if (values != null) {\n        throw new Error('Data is not on GPU but on CPU.');\n      } else {\n        throw new Error('There is no data on GPU or CPU.');\n      }\n    }\n\n    const size = (resourceInfo as BufferInfo).size;\n    const buffer = this.bufferManager.acquireBuffer(size, resourceInfo.usage);\n    this.ensureCommandEncoderReady();\n    this.ensureComputePassEnded();\n    this.currentCommandEncoder.copyBufferToBuffer(\n        (resourceInfo as BufferInfo).buffer, 0, buffer, 0, size);\n    this.submitQueue();\n\n    const tensorInfo = this.makeTensorInfo(shape, dtype);\n    // Make engine track this tensor, so that we can dispose it later.\n    const tensorRef = engine().makeTensorFromTensorInfo(tensorInfo);\n\n    const tensorData = this.tensorMap.get(tensorInfo.dataId);\n    tensorData\n        .resourceInfo = {size, usage: this.defaultGpuBufferUsage(), buffer};\n\n    return {tensorRef, buffer, bufSize: size};\n  }\n\n  bufferSync<R extends Rank, D extends DataType>(t: TensorInfo):\n      TensorBuffer<R, D> {\n    const data = this.readSync(t.dataId);\n    if (t.dtype === 'string') {\n      try {\n        // Decode the bytes into string.\n        const strings = (data as Uint8Array[]).map(d => util.decodeString(d));\n        return buffer(t.shape as ShapeMap[R], t.dtype, strings) as\n            TensorBuffer<R, D>;\n      } catch {\n        throw new Error('Failed to decode encoded string bytes into utf-8');\n      }\n    }\n    return buffer(t.shape as ShapeMap[R], t.dtype, data as TypedArray) as\n        TensorBuffer<R, D>;\n  }\n\n  async time(f: () => void): Promise<WebGPUTimingInfo> {\n    if (!this.supportTimeQuery) {\n      console.warn(\n          `This device doesn't support timestamp-query extension. ` +\n          `Start Chrome browser with flag ` +\n          `--disable-dawn-features=disallow_unsafe_apis then try again. ` +\n          `Otherwise, zero will be shown for the kernel time when profiling ` +\n          `mode is enabled. Using performance.now is not workable for webgpu ` +\n          `since it doesn't support synchronous data read from GPU.`);\n    }\n    const oldActiveTimers = this.activeTimers;\n    const newActiveTimers: TimerNode[] = [];\n\n    let outerMostTime = false;\n    if (this.programTimersStack == null) {\n      this.programTimersStack = newActiveTimers;\n      outerMostTime = true;\n    } else {\n      this.activeTimers.push(newActiveTimers);\n    }\n    this.activeTimers = newActiveTimers;\n\n    f();\n\n    const flattenedActiveTimerQueries =\n        util.flatten(this.activeTimers.map((d: WebGPUKernelInfo) => d.query))\n            .filter(d => d != null);\n    const flattenedActiveTimerNames =\n        util.flatten(this.activeTimers.map((d: WebGPUKernelInfo) => d.name))\n            .filter(d => d != null);\n\n    this.activeTimers = oldActiveTimers;\n\n    if (outerMostTime) {\n      this.programTimersStack = null;\n    }\n    const res: WebGPUTimingInfo = {\n      uploadWaitMs: this.uploadWaitMs,\n      downloadWaitMs: this.downloadWaitMs,\n      kernelMs: null,\n      wallMs: null\n    };\n\n    const kernelMs = await Promise.all(flattenedActiveTimerQueries);\n    res['kernelMs'] = util.sum(kernelMs);\n    res['getExtraProfileInfo'] = () =>\n        kernelMs.map((d, i) => ({name: flattenedActiveTimerNames[i], ms: d}))\n            .map(d => `${d.name}: ${d.ms}`)\n            .join(', ');\n    this.uploadWaitMs = 0;\n    this.downloadWaitMs = 0;\n    return res;\n  }\n\n  makeTensorInfo(\n      shape: number[], dtype: DataType,\n      values?: backend_util.BackendValues|string[]): TensorInfo {\n    if (dtype === 'string' && values != null && values.length > 0 &&\n        util.isString(values[0])) {\n      values = (values as {} as string[]).map(d => util.encodeString(d));\n    }\n    const dataId =\n        this.write(values as backend_util.BackendValues, shape, dtype);\n    return {dataId, shape, dtype};\n  }\n\n  private tensorToBinding(tensor?: TensorInfo): GPUBindingResource {\n    if (!tensor) {\n      return null;\n    }\n\n    const tensorData = this.tensorMap.get(tensor.dataId);\n    if ('texture' in tensorData.resourceInfo) {\n      const info = tensorData.resourceInfo;\n      if (info.texture instanceof GPUExternalTexture) {\n        return info.texture;\n      } else {\n        return info.texture.createView();\n      }\n    }\n    const bufferInfo = tensorData.resourceInfo;\n    return {offset: 0, size: bufferInfo.size, buffer: bufferInfo.buffer};\n  }\n\n  async getQueryTime(query: GPUQuerySet): Promise<number> {\n    if (this.supportTimeQuery) {\n      return this.getTimeFromQuerySet(query);\n    } else {\n      return 0;\n    }\n  }\n\n  uploadToGPU(dataId: DataId): void {\n    const tensorData = this.tensorMap.get(dataId);\n    // Already on the GPU.\n    if (tensorData.resourceInfo) {\n      return;\n    }\n\n    const size = webgpu_util.GPUBytesPerElement(tensorData.dtype) *\n        util.sizeFromShape(tensorData.shape);\n    const buffer =\n        this.bufferManager.acquireBuffer(size, this.defaultGpuBufferUsage());\n\n    tensorData\n        .resourceInfo = {size, usage: this.defaultGpuBufferUsage(), buffer};\n    if (tensorData.values) {\n      const stagingBuffer = this.bufferManager.acquireUploadBuffer(\n          size, GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC);\n      const arrayBuffer = stagingBuffer.getMappedRange();\n      if (tensorData.dtype === 'int32' || tensorData.dtype === 'bool') {\n        new Int32Array(arrayBuffer).set(tensorData.values as TypedArray);\n      } else {\n        new Float32Array(arrayBuffer).set(tensorData.values as Float32Array);\n      }\n      stagingBuffer.unmap();\n      this.ensureCommandEncoderReady();\n      this.ensureComputePassEnded();\n      this.currentCommandEncoder.copyBufferToBuffer(\n          stagingBuffer, 0, buffer, 0, size);\n\n      const stagingInfo = {\n        size,\n        usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC,\n        buffer: stagingBuffer\n      };\n      this.stagingPendingDisposal.push(stagingInfo);\n      // TODO: WebGPU doesn't support read data synchronously from GPU to CPU.\n      // So it will report error when switching backend from WebGPU to others.\n      // There are two situations: 1) swithcing the backend after running a\n      // model; 2) swithcing the backend within the model. Temporarilly keep the\n      // values on CPU to solve the first issue.\n      // tensorData.values = null;\n    }\n  }\n\n  private makeUniforms(programUniform: ProgramUniform): GPUBindingResource {\n    let currentOffset = 0;\n    let preLength = 0;\n    const offsets: number[] = [];\n    programUniform.forEach((d) => {\n      if (d.data.length === 0) {\n        d.data = [1];\n      }\n      // https://www.w3.org/TR/WGSL/#alignof\n      let baseAlignment: number;\n      switch (d.data.length) {\n        case 1:\n          baseAlignment = 4;\n          break;\n        case 2:\n          baseAlignment = 8;\n          break;\n        case 3:\n          baseAlignment = 16;\n          break;\n        case 4:\n          baseAlignment = 16;\n          break;\n        case 5:\n          baseAlignment = 16;\n          break;\n        case 6:\n          baseAlignment = 16;\n          break;\n        default:\n          util.assert(false, () => `Unsupported ${d.data.length}D shape`);\n      }\n\n      if (preLength === 5 || preLength === 6) {\n        baseAlignment = 16;\n      }\n      currentOffset = Math.ceil(currentOffset / baseAlignment) * baseAlignment;\n      preLength = d.data.length;\n      offsets.push(currentOffset);\n      currentOffset += d.data.length * 4;\n    });\n\n    const arrayBuffer = new ArrayBuffer(currentOffset);\n    programUniform.forEach((d, i) => {\n      const offset = offsets[i];\n      if (d.type === 'int32') {\n        new Int32Array(arrayBuffer, offset, d.data.length).set(d.data);\n      } else if (d.type === 'uint32') {\n        new Uint32Array(arrayBuffer, offset, d.data.length).set(d.data);\n      } else {\n        new Float32Array(arrayBuffer, offset, d.data.length).set(d.data);\n      }\n    });\n\n    const uniformBuffer = this.bufferManager.acquireBuffer(\n        currentOffset, GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM);\n    this.queue.writeBuffer(uniformBuffer, 0, arrayBuffer, 0, currentOffset);\n\n    const uniformInfo = {\n      size: currentOffset,\n      usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM,\n      buffer: uniformBuffer\n    };\n    this.uniformPendingDisposal.push(uniformInfo);\n\n    return {offset: 0, size: currentOffset, buffer: uniformBuffer};\n  }\n\n  public runWebGPUProgram(\n      program: webgpu_program.WebGPUProgram, inputs: TensorInfo[],\n      outputDtype: DataType, programDefinedUniform?: ProgramUniform,\n      output?: TensorInfo): TensorInfo {\n    if (!output) {\n      output = this.makeTensorInfo(program.outputShape, outputDtype);\n    }\n    if (util.sizeFromShape(output.shape) === 0) {\n      // Short-circuit the computation since the result is empty (has 0 in its\n      // shape).\n      this.tensorMap.get(output.dataId).values =\n          util.getTypedArrayFromDType(output.dtype as 'float32', 0);\n      return output;\n    }\n    this.uploadToGPU(output.dataId);\n    program.dispatch = reshapeDispatch(this.device, program);\n\n    // There are five kinds of uniforms: NAN, shapes, shape strides, program\n    // size, program defined uniforms.\n    let programUniform: ProgramUniform = [];\n    let bufferShapes: number[][] = [];\n    if (!program.isFromPixels) {\n      programUniform.push({type: 'float32', data: [NaN]});\n      bufferShapes = inputs.concat(output).map(d => d.shape);\n      const uniformsType = 'int32';\n      bufferShapes.map(d => {\n        programUniform.push({type: uniformsType, data: d});\n      });\n      const strides = util.computeStrides(output.shape);\n      programUniform.push({type: uniformsType, data: strides});\n      if (program.size) {\n        const size = util.sizeFromShape(program.outputShape);\n        programUniform.push(\n            {type: uniformsType, data: [program.isVec4 ? size / 4 : size]});\n      }\n    }\n\n    const inputsData = inputs.map((input: TensorInfo, i: number) => {\n      if (input.dtype === 'complex64') {\n        throw new Error(\n            `GPGPUProgram does not support complex64 input. For complex64 ` +\n            `dtypes, please separate the program into real and imaginary ` +\n            `parts.`);\n      }\n      this.uploadToGPU(input.dataId);\n\n      return {\n        // Returning dtype from tensorMap because it reflects dtype\n        // of underlying buffer, rather than abstract dtype.\n        dtype: this.tensorMap.get(input.dataId).dtype,\n        shape: input.shape,\n        name: program.variableNames[i]\n      };\n    });\n\n    const key =\n        webgpu_program.makeShaderKey(program, bufferShapes, inputsData, output);\n\n    let pipeline;\n    if (key in this.pipelineCache) {\n      pipeline = this.pipelineCache[key];\n    } else {\n      pipeline = webgpu_program.compileProgram(\n          this.device, program, inputsData, output);\n      this.pipelineCache[key] = pipeline;\n    }\n\n    if (programDefinedUniform) {\n      programUniform = [...programUniform, ...programDefinedUniform];\n    }\n    const bindings = [\n      this.tensorToBinding(output), ...inputs.map(t => this.tensorToBinding(t)),\n      this.makeUniforms(programUniform)\n    ];\n\n    const bindGroup = this.device.createBindGroup({\n      layout: pipeline.getBindGroupLayout(0),\n      entries: bindings.map((b, i) => ({binding: i, resource: b})),\n    });\n\n    this.ensureCommandEncoderReady();\n    const pass = this.getComputePass();\n    const shouldTimeProgram = this.activeTimers != null;\n    if (shouldTimeProgram) {\n      if (this.supportTimeQuery) {\n        // tslint:disable-next-line:no-any\n        (pass as any).writeTimestamp(this.querySet, 0);\n      }\n    }\n    pass.setPipeline(pipeline);\n    pass.setBindGroup(0, bindGroup);\n    pass.dispatchWorkgroups(\n        program.dispatch[0], program.dispatch[1], program.dispatch[2]);\n    if (shouldTimeProgram) {\n      if (this.supportTimeQuery) {\n        // tslint:disable-next-line:no-any\n        (pass as any).writeTimestamp(this.querySet, 1);\n      }\n    }\n    this.dispatchNumberInEncoder++;\n\n    inputs.forEach(input => {\n      this.commandQueueOwnedIds.add(input.dataId);\n    });\n    this.commandQueueOwnedIds.add(output.dataId);\n\n    if (env().get('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE') as\n        number <= this.dispatchNumberInEncoder) {\n      this.submitQueue();\n    }\n\n    if (shouldTimeProgram) {\n      this.activeTimers.push({\n        name: program.constructor.name,\n        query: this.getQueryTime(this.querySet)\n      });\n    }\n    return output;\n  }\n\n  async getTimeFromQuerySet(querySet: GPUQuerySet) {\n    const queryBuffer = this.bufferManager.acquireBuffer(\n        16, GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);\n    const dst = this.bufferManager.acquireBuffer(\n        16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);\n\n    this.ensureCommandEncoderReady();\n    this.ensureComputePassEnded();\n    this.currentCommandEncoder.resolveQuerySet(querySet, 0, 2, queryBuffer, 0);\n    this.currentCommandEncoder.copyBufferToBuffer(queryBuffer, 0, dst, 0, 16);\n    this.submitQueue();\n    await dst.mapAsync(GPUMapMode.READ);\n    const arrayBuf = new BigUint64Array(dst.getMappedRange());\n    const timeElapsedNanos = Number((arrayBuf[1] - arrayBuf[0]));\n    dst.unmap();\n    this.bufferManager.releaseBuffer(\n        dst, 16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);\n    this.bufferManager.releaseBuffer(\n        queryBuffer, 16,\n        GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);\n    // Return milliseconds.\n    return timeElapsedNanos / 1000000;\n  }\n\n  shouldExecuteOnCPU(\n      inputs: TensorInfo[],\n      sizeThreshold = CPU_HANDOFF_SIZE_THRESHOLD): boolean {\n    return env().getBool('WEBGPU_CPU_FORWARD') &&\n        inputs.every(\n            input => this.tensorMap.get(input.dataId).resourceInfo == null &&\n                util.sizeFromShape(input.shape) < sizeThreshold);\n  }\n\n  numDataIds() {\n    return this.tensorMap.numDataIds() - this.tensorDataPendingDisposal.length;\n  }\n\n  dispose() {\n    if (this.disposed) {\n      return;\n    }\n    this.bufferManager.dispose();\n    this.textureManager.dispose();\n    this.disposed = true;\n  }\n}\n","/**\n * @license\n * Copyright 2022 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport './flags_webgpu';\n\nimport {env, registerBackend} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from './backend_webgpu';\nimport {isWebGPUSupported} from './webgpu_util';\n\nif (isWebGPUSupported()) {\n  registerBackend('webgpu', async () => {\n    // Remove it once we figure out how to correctly read the tensor data\n    // before the tensor is disposed in profiling mode.\n    env().set('CHECK_COMPUTATION_FOR_ERRORS', false);\n\n    const gpuDescriptor: GPURequestAdapterOptions = {\n      powerPreference: env().get('WEBGPU_USE_LOW_POWER_GPU') ?\n          'low-power' :\n          'high-performance'\n    };\n\n    const adapter = await navigator.gpu.requestAdapter(gpuDescriptor);\n    const adapterLimits = adapter.limits;\n    const deviceDescriptor: GPUDeviceDescriptor = {};\n    const supportTimeQuery = adapter.features.has('timestamp-query');\n    deviceDescriptor.requiredLimits = {\n      'maxComputeWorkgroupStorageSize':\n          adapterLimits.maxComputeWorkgroupStorageSize,\n      'maxComputeWorkgroupsPerDimension':\n          adapterLimits.maxComputeWorkgroupsPerDimension,\n      'maxStorageBufferBindingSize': adapterLimits.maxStorageBufferBindingSize,\n    };\n\n    if (supportTimeQuery) {\n      deviceDescriptor.requiredFeatures = ['timestamp-query'];\n    }\n    const device: GPUDevice = await adapter.requestDevice(deviceDescriptor);\n    // tslint:disable-next-line:no-any\n    const adapterInfo = await (adapter as any).requestAdapterInfo();\n    return new WebGPUBackend(device, adapterInfo);\n  }, 3 /*priority*/);\n}\n\n// Export webgpu utilities\nexport * from './webgpu';\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nexport enum BinaryOpType {\n  MUL,\n  ADD,\n  ATAN2,\n  SUB,\n  DIV,\n  EQUAL,\n  GREATER,\n  GREATER_EQUAL,\n  LESS,\n  LESS_EQUAL,\n  LOGICAL_AND,\n  NOT_EQUAL,\n  SQUARED_DIFFERENCE,\n  INT_DIV,\n  POW,\n  PRELU,\n  MAX,\n  MIN,\n  COMPLEX_MULTIPLY_REAL,\n  COMPLEX_MULTIPLY_IMAG\n}\n\nconst CHECK_NAN_SNIPPET = `\n  if (isnan(a)) { return a; }\n  if (isnan(b)) { return b; }\n  `;\n\nconst CHECK_NAN_SNIPPET_VEC4_INNER = `\n  if (isNaN.r) {\n    resultTemp.r = valueForNaN;\n  }\n  if (isNaN.g) {\n    resultTemp.g = valueForNaN;\n  }\n  if (isNaN.b) {\n    resultTemp.b = valueForNaN;\n  }\n  if (isNaN.a) {\n    resultTemp.a = valueForNaN;\n  }\n  `;\n\nconst CHECK_NAN_SNIPPET_VEC4 = `\n  let isNaN = isnanVec4(a) | isnanVec4(b);\n  ${CHECK_NAN_SNIPPET_VEC4_INNER}\n  `;\n\nconst ADD = 'return a + b;';\n// (Ar + Ai)(Br + Bi) =\n// ArBr + ArBi + AiBr + AiBi = ArBr - AB + ArBi + AiBr\n// Yr = ArBr - AB\n// Yi = ArBi + AiBr\nconst COMPLEX_MULTIPLY_REAL = 'return areal * breal - aimag * bimag;';\nconst COMPLEX_MULTIPLY_IMAG = 'return areal * bimag + aimag * breal;';\nconst DIV = 'return a / b;';\nconst MUL = 'return a * b;';\nconst SQUARED_DIFFERENCE = 'return (a - b) * (a - b);';\nconst SUB = 'return a - b;';\nconst EQUAL = 'return f32(a == b);';\nconst EQUAL_VEC4 = 'return vec4<f32>(a == b);';\nconst GREATER = 'return f32(a > b);';\nconst GREATER_VEC4 = 'return vec4<f32>(a > b);';\nconst GREATER_EQUAL = 'return f32(a >= b);';\nconst GREATER_EQUAL_VEC4 = 'return vec4<f32>(a >= b);';\nconst LESS = 'return f32(a < b);';\nconst LESS_VEC4 = 'return vec4<f32>(a < b);';\nconst LESS_EQUAL = 'return f32(a <= b);';\nconst LESS_EQUAL_VEC4 = 'return vec4<f32>(a <= b);';\nconst LOGICAL_AND = 'return f32(f32(a) >= 1.0 && f32(b) >= 1.0);';\nconst LOGICAL_AND_VEC4 = `return (vec4<f32>(a >= vec4<f32>(1.0)) *\n  vec4<f32>(b >= vec4<f32>(1.0)));`;\nconst INT_DIV = `\n  let s = sign(a) * sign(b);\n  let ia = i32(round(a));\n  let ib = i32(round(b));\n  return f32(idiv(ia, ib, s));\n  `;\n\nconst INT_DIV_VEC4 = `\n  let ia = vec4<i32>(round(a));\n  let ib = vec4<i32>(round(b));\n  let cond = ib != vec4<i32>(0);\n  var resultTemp = vec4<i32>(0);\n  let s = sign(a) * sign(b);\n\n  // Windows (D3D) wants guaranteed non-zero int division at compile-time.\n  if (cond[0]) {\n    resultTemp[0] = idiv(ia[0], ib[0], s[0]);\n  }\n  if (cond[1]) {\n    resultTemp[1] = idiv(ia[1], ib[1], s[1]);\n  }\n  if (cond[2]) {\n    resultTemp[2] = idiv(ia[2], ib[2], s[2]);\n  }\n  if (cond[3]) {\n    resultTemp[3] = idiv(ia[3], ib[3], s[3]);\n  }\n  return vec4<f32>(resultTemp);\n  `;\n\nconst NOT_EQUAL = `\n  if (isnan(a) || isnan(b)) {\n    return 1.0;\n  }\n  return f32(a != b);\n`;\nconst NOT_EQUAL_VEC4 = `\n  var resultTemp = vec4<f32>(a != b);\n  let valueForNaN = 1.0;\n  ${CHECK_NAN_SNIPPET_VEC4}\n\n  return resultTemp;\n`;\nconst POW = `\n  if(a < 0.0 && floor(b) < b) {\n    return uniforms.NAN;\n  }\n  if (b == 0.0) {\n    return 1.0;\n  }\n  if (round(abs(b) % 2.0) != 1.0) {\n    return pow(abs(a), b);\n  }\n  return sign(a) * pow(abs(a), b);\n  `;\nconst POW_VEC4 = `\n  let isModRound1Bool = vec4<i32>(round(abs(b) % vec4<f32>(2.0))) == vec4<i32>(1);\n  let isModRound1 = vec4<f32>(isModRound1Bool);\n  let multiplier = sign(a) * isModRound1 + (vec4<f32>(1.0) - isModRound1);\n  var resultTemp = multiplier * pow(abs(a), b);\n\n  // Ensure that a^0 = 1, including 0^0 = 1 as this correspond to TF and JS\n  let isExpZero = b == vec4<f32>(0.0);\n  if (isExpZero.r) {\n    resultTemp.r = 1.0;\n  }\n  if (isExpZero.g) {\n    resultTemp.g = 1.0;\n  }\n  if (isExpZero.b) {\n    resultTemp.b = 1.0;\n  }\n  if (isExpZero.a) {\n    resultTemp.a = 1.0;\n  }\n  let isNaN = (a < vec4<f32>(0.0)) & (floor(b) < b);\n  let valueForNaN = uniforms.NAN;\n  ${CHECK_NAN_SNIPPET_VEC4_INNER}\n  return resultTemp;\n  `;\n\nconst PRELU = `if (a < 0.0) { return b * a; }  return a;`;\nconst PRELU_VEC4 = `\n  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));\n  return (aLessThanZero * (b * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);\n  `;\n\nfunction getBinaryWithNanString(\n    op: string, useVec4: boolean, valueForNaN = 'uniforms.NAN') {\n  const checkNanSnippet = useVec4 ? CHECK_NAN_SNIPPET_VEC4 : CHECK_NAN_SNIPPET;\n  return useVec4 ? `\n    let valueForNaN = ${valueForNaN};\n    var resultTemp = vec4<f32>(${op}(a, b));\n    ` + checkNanSnippet +\n          `\n    return resultTemp;\n  ` :\n                   checkNanSnippet + `\n    return ${op}(a, b);\n  `;\n}\n\nexport function getBinaryOpString(\n    type: BinaryOpType, useVec4?: boolean): string {\n  switch (type) {\n    case BinaryOpType.MUL:\n      return MUL;\n    case BinaryOpType.ADD:\n      return ADD;\n    case BinaryOpType.ATAN2:\n      return getBinaryWithNanString('atan2', useVec4);\n    case BinaryOpType.SUB:\n      return SUB;\n    case BinaryOpType.DIV:\n      return DIV;\n    case BinaryOpType.EQUAL:\n      return useVec4 ? EQUAL_VEC4 : EQUAL;\n    case BinaryOpType.GREATER:\n      return useVec4 ? GREATER_VEC4 : GREATER;\n    case BinaryOpType.GREATER_EQUAL:\n      return useVec4 ? GREATER_EQUAL_VEC4 : GREATER_EQUAL;\n    case BinaryOpType.LESS:\n      return useVec4 ? LESS_VEC4 : LESS;\n    case BinaryOpType.LESS_EQUAL:\n      return useVec4 ? LESS_EQUAL_VEC4 : LESS_EQUAL;\n    case BinaryOpType.LOGICAL_AND:\n      return useVec4 ? LOGICAL_AND_VEC4 : LOGICAL_AND;\n    case BinaryOpType.NOT_EQUAL:\n      return useVec4 ? NOT_EQUAL_VEC4 : NOT_EQUAL;\n    case BinaryOpType.SQUARED_DIFFERENCE:\n      return SQUARED_DIFFERENCE;\n    case BinaryOpType.INT_DIV:\n      return useVec4 ? INT_DIV_VEC4 : INT_DIV;\n    case BinaryOpType.PRELU:\n      return useVec4 ? PRELU_VEC4 : PRELU;\n    case BinaryOpType.MAX:\n      return getBinaryWithNanString('max', useVec4);\n    case BinaryOpType.MIN:\n      return getBinaryWithNanString('min', useVec4);\n    case BinaryOpType.POW:\n      return useVec4 ? POW_VEC4 : POW;\n    case BinaryOpType.COMPLEX_MULTIPLY_REAL:\n      return COMPLEX_MULTIPLY_REAL;\n    case BinaryOpType.COMPLEX_MULTIPLY_IMAG:\n      return COMPLEX_MULTIPLY_IMAG;\n    default:\n      throw new Error(`BinaryType ${type} is not implemented!`);\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nexport enum UnaryOpType {\n  ABS,\n  CEIL,\n  COS,\n  COSH,\n  ELU,\n  EXP,\n  EXPM1,\n  FLOOR,\n  IS_NAN,\n  LINEAR,\n  LOG,\n  LOGICAL_NOT,\n  NEG,\n  RELU,\n  RELU6,\n  LEAKYRELU,\n  RECIPROCAL,\n  RSQRT,\n  SIN,\n  SINH,\n  SIGMOID,\n  SQRT,\n  SQUARE,\n  TANH,\n  TO_INT\n}\n\nconst ABS = `return abs(a);`;\nconst CEIL = `return ceil(a);`;\nconst COS = `return cos(a);`;\nconst COSH = `\n  let e2x = exp(-a);\n  return (e2x + 1.0 / e2x) / 2.0;\n`;\nconst EXPM1 = `return exp(a) - 1.0;`;\nconst ELU = `if (a >= 0.0) { return a; }  return (exp(a) - 1.0);`;\nconst ELU_VEC4 = `\n  var resFloat = exp(a) - vec4<f32>(1.0);\n  if (a.r >= 0.0) {\n    resFloat.r = a.r;\n  }\n  if (a.g >= 0.0) {\n    resFloat.g = a.g;\n  }\n  if (a.b >= 0.0) {\n    resFloat.b = a.b;\n  }\n  if (a.a >= 0.0) {\n    resFloat.a = a.a;\n  }\n  return resFloat;\n`;\nconst EXP = `return exp(a);`;\nconst FLOOR = `return floor(a);`;\nconst IS_NAN = `return f32(isnan(a));`;\nconst LINEAR = `return a;`;\nconst LOG = `if (a < 0.0) { return uniforms.NAN; }\n  return log(a);`;\nconst LOGICAL_NOT = `return f32(!(a >= 1.0));`;\nconst NEG = `return -a;`;\nconst LEAKYRELU = `if (a < 0.0) { return uniforms.alpha * a; } return a;`;\nconst LEAKYRELU_VEC4 = `\n  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));\n  return (aLessThanZero * (uniforms.alpha * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);\n`;\nconst RECIPROCAL = `return 1.0 / a;`;\nconst RELU = `return select(a, 0.0, a < 0.0);`;\nconst RELU6 = 'return clamp(a, 0.0, 6.0);';\nconst RELU6_VEC4 =\n    'return clamp(a, vec4<f32>(0.0, 0.0, 0.0, 0.0), vec4<f32>(6.0, 6.0, 6.0, 6.0));';\nconst RELU_VEC4 = `\n  return select(a, vec4<f32>(0.0), a < vec4<f32>(0.0));\n`;\nconst RSQRT = `return 1.0/sqrt(a);`;\nconst SIGMOID = `return 1.0 / (1.0 + exp(-1.0 * a));`;\nconst SIN = `return sin(a);`;\nconst SINH = `\n  let e2x = exp(a);\n  return (e2x - 1.0 / e2x) / 2.0;\n`;\nconst SQRT = `return sqrt(a);`;\nconst SQUARE = `return a * a;`;\nconst TANH = `\n  let e2x = exp(-2.0 * abs(a));\n  return sign(a) * (1.0 - e2x) / (1.0 + e2x);\n`;\nconst TO_INT = `return f32(i32((a)));`;\n\nexport function getUnaryOpString(type: UnaryOpType, useVec4?: boolean): string {\n  switch (type) {\n    case UnaryOpType.ABS:\n      return ABS;\n    case UnaryOpType.COS:\n      return COS;\n    case UnaryOpType.COSH:\n      return COSH;\n    case UnaryOpType.CEIL:\n      return CEIL;\n    case UnaryOpType.ELU:\n      return useVec4 ? ELU_VEC4 : ELU;\n    case UnaryOpType.EXP:\n      return EXP;\n    case UnaryOpType.EXPM1:\n      return EXPM1;\n    case UnaryOpType.FLOOR:\n      return FLOOR;\n    case UnaryOpType.IS_NAN:\n      return IS_NAN;\n    case UnaryOpType.LINEAR:\n      return LINEAR;\n    case UnaryOpType.LOG:\n      return LOG;\n    case UnaryOpType.LOGICAL_NOT:\n      return LOGICAL_NOT;\n    case UnaryOpType.NEG:\n      return NEG;\n    case UnaryOpType.LEAKYRELU:\n      return useVec4 ? LEAKYRELU_VEC4 : LEAKYRELU;\n    case UnaryOpType.RECIPROCAL:\n      return RECIPROCAL;\n    case UnaryOpType.RELU:\n      return useVec4 ? RELU_VEC4 : RELU;\n    case UnaryOpType.RELU6:\n      return useVec4 ? RELU6_VEC4 : RELU6;\n    case UnaryOpType.RSQRT:\n      return RSQRT;\n    case UnaryOpType.SIGMOID:\n      return SIGMOID;\n    case UnaryOpType.SIN:\n      return SIN;\n    case UnaryOpType.SINH:\n      return SINH;\n    case UnaryOpType.SQRT:\n      return SQRT;\n    case UnaryOpType.SQUARE:\n      return SQUARE;\n    case UnaryOpType.TANH:\n      return TANH;\n    case UnaryOpType.TO_INT:\n      return TO_INT;\n\n    default:\n      throw new Error(`BinaryType ${type} is not implemented!`);\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType, getBinaryOpString} from './binary_op_util';\nimport {getUnaryOpString, UnaryOpType} from './unary_op_util';\n\nexport const typeSnippet = (component: number) => {\n  switch (component) {\n    case 1:\n      return 'f32';\n    case 2:\n      return 'vec2<f32>';\n    case 3:\n      return 'vec3<f32>';\n    case 4:\n      return 'vec4<f32>';\n    default:\n      throw new Error(`${component}-component is not supported.`);\n  }\n};\n\nexport function activationFnSnippet(\n    activation: backend_util.Activation, hasPreluActivationWeights = false,\n    packed = false, coordsLength = 3): string {\n  if (activation === null) {\n    return '';\n  }\n\n  let activationOpSnippet = '';\n  if (activation === 'linear') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.LINEAR);\n  } else if (activation === 'relu') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.RELU, packed);\n  } else if (activation === 'elu') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.ELU, packed);\n  } else if (activation === 'relu6') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.RELU6, packed);\n  } else if (activation === 'prelu') {\n    activationOpSnippet = getBinaryOpString(BinaryOpType.PRELU, packed);\n  } else if (activation === 'sigmoid') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.SIGMOID, packed);\n  } else if (activation === 'leakyrelu') {\n    activationOpSnippet = getUnaryOpString(UnaryOpType.LEAKYRELU, packed);\n  } else {\n    throw new Error(`Activation ${\n        activation} has not been implemented for the WebGPU backend.`);\n  }\n  const elementSize = packed ? 4 : 1;\n  const dataType = typeSnippet(elementSize);\n  let activationFnSnippet = '';\n  if (hasPreluActivationWeights) {\n    activationFnSnippet = `\n      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${\n        dataType} {\n        let b = getPreluActivationWeightsByOutputCoords(coords);\n        ${activationOpSnippet}\n      }`;\n  } else {\n    activationFnSnippet = `\n      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${\n        dataType} {\n        ${activationOpSnippet}\n      }`;\n  }\n  return activationFnSnippet;\n}\n\nexport function biasActivationSnippet(\n    hasBias: boolean, activation: backend_util.Activation): string {\n  return `\n      ${hasBias ? 'value = value + getBiasByOutputCoords(coords);' : ''}\n      ${activation ? 'value = activation(value, coords);' : ''}\n      `;\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\nimport {activationFnSnippet, biasActivationSnippet, typeSnippet} from './activation_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, computeWorkGroupInfoForMatMul} from './webgpu_util';\n\nexport function matMulReadFnSource(\n    batchAEqualOne: boolean, batchBEqualOne: boolean, transposeA: boolean,\n    transposeB: boolean, fitAOuter = false, fitBOuter = false, fitInner = false,\n    component = 1) {\n  util.assert(\n      transposeA && component === 1 || !transposeA,\n      () => `transposeA ${transposeA} is not compatible with component size ${\n          component}`);\n  const sampleA = `\n      let batch = ${batchAEqualOne ? '0' : 'batchIn'};\n      ${\n      transposeA ? `value = getA(batch, col, row);` :\n                   `value = getA(batch, row, col);`}\n\n    `;\n  const sampleB = transposeB ? `value = getB(batch, col, row);` :\n                               `value = getB(batch, row, col);`;\n\n  return `\n  fn mm_readA(batchIn: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {\n    var value = ${typeSnippet(component)}(0.0);\n    let col = colIn * ${component};\n    ${\n      fitAOuter && fitInner ?\n          sampleA :\n          `\n    ${\n              transposeA ?\n                  `if(row < uniforms.dimAOuter && col < uniforms.dimInner)` :\n                  `if(row < uniforms.aShape[1] && col < uniforms.aShape[2])`}\n    {\n      ${sampleA}\n    }\n    `}\n    return value;\n  }\n\n  fn mm_readB(batchIn: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {\n    let col = colIn * ${component};\n    let batch = ${batchBEqualOne ? '0' : 'batchIn'};\n    var value = ${typeSnippet(component)}(0.0);\n    ${sampleB}\n    return value;\n  }\n  `;\n}\n\nexport function matMulReadWriteFnSource(\n    hasBias: boolean, activation: backend_util.Activation,\n    batchAEqualOne: boolean, batchBEqualOne: boolean, transposeA: boolean,\n    transposeB: boolean, fitAOuter = false, fitBOuter = false, fitInner = false,\n    component = 1) {\n  return `\n  ${\n      matMulReadFnSource(\n          batchAEqualOne, batchBEqualOne, transposeA, transposeB, fitAOuter,\n          fitBOuter, fitInner, component)}\n  fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${\n      typeSnippet(component)}) {\n    let col = colIn * ${component};\n    ${\n      fitAOuter && fitBOuter ?\n          '' :\n          'if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)'}\n    {\n      var value = valueIn;\n      let coords = vec3<i32>(batch, row, col);\n      ${biasActivationSnippet(hasBias, activation)}\n      setOutputAtCoords(coords[0], coords[1], coords[2], value);\n    }\n  }\n  `;\n}\n\nconst writeDataToSubAVec4Snippet = (transpose: boolean) => {\n  if (transpose) {\n    return `\n        mm_Asub[inputRow][inputCol] = mm_readA(batch,\n          kStart + inputRow,\n          globalRowStart / InnerElementSize + inputCol);\n        `;\n\n  } else {\n    return `\n        mm_Asub[inputRow][inputCol] = mm_readA(batch,\n          globalRow + innerRow,\n          kStart / InnerElementSize + inputCol);\n        `;\n  }\n};\n\nconst calculateResultSnippet =\n    (transposeA: boolean, innerElementSize: number) => {\n      if (transposeA) {\n        return `\n        let ACached0 = mm_Asub[k * InnerElementSize][localRow];\n        let ACached1 = mm_Asub[k * InnerElementSize + 1][localRow];\n        let ACached2 = mm_Asub[k * InnerElementSize + 2][localRow];\n        ${\n            innerElementSize === 3 ?\n                '' :\n                'let ACached3 = mm_Asub[k * InnerElementSize + 3][localRow];'}\n        for (var i = 0; i < RowPerThread; i = i + 1) {\n          acc[i] = BCached0 * ACached0[i] + acc[i];\n          acc[i] = BCached1 * ACached1[i] + acc[i];\n          acc[i] = BCached2 * ACached2[i] + acc[i];\n          ${\n            innerElementSize === 3 ?\n                '' :\n                'acc[i] = BCached3 * ACached3[i] + acc[i];'}\n        }`;\n      } else {\n        return `\n        for (var i = 0; i < RowPerThread; i = i + 1) {\n          let ACached = mm_Asub[tileRow + i][k];\n          acc[i] = BCached0 * ACached.x + acc[i];\n          acc[i] = BCached1 * ACached.y + acc[i];\n          acc[i] = BCached2 * ACached.z + acc[i];\n          ${\n            innerElementSize === 3 ? '' :\n                                     'acc[i] = BCached3 * ACached.w + acc[i];'}\n        }`;\n      }\n    };\n\nexport function makeMatMulPackedVec4Source(\n    workPerThread: number[], workGroupSize: [number, number, number],\n    transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32,\n    isVectorA = false): string {\n  const tileAOuter = workGroupSize[1] * workPerThread[1];\n  const tileBOuter = workGroupSize[0] * workPerThread[0];\n  const tileAWidth = transposeA ? tileAOuter : tileInner;\n  const tileAHight = transposeA ? tileInner : tileAOuter;\n  const innerElementSize = tileAWidth / workGroupSize[0];\n  const rowPerThreadB = tileInner / workGroupSize[1];\n  util.assert(\n      ((transposeA && innerElementSize === 4 && workPerThread[1] === 4) ||\n       (!transposeA && (innerElementSize === 3 || innerElementSize === 4))) &&\n          tileAWidth % workGroupSize[0] === 0 &&\n          tileInner % workGroupSize[1] === 0 && workPerThread[0] === 4,\n      () => `If transposeA ${transposeA} is true, innerElementSize ${\n          innerElementSize} and workPerThread[1] ${workPerThread[1]} must be 4.\n          Otherwise, innerElementSize ${innerElementSize} must be 3 or 4.\n      tileAWidth ${tileAWidth} must be divisible by workGroupSize[0]${\n          workGroupSize[0]}. tileInner ${\n          tileInner} must be divisible by workGroupSize[1] ${\n          workGroupSize[1]}. ColPerThread ${workPerThread[0]} must be 4.`);\n  return `\n  var<workgroup> mm_Asub : array<array<vec${innerElementSize}<f32>, ${\n      tileAWidth / innerElementSize}>, ${tileAHight}>;\n  var<workgroup> mm_Bsub : array<array<vec4<f32>, ${\n      tileBOuter / workPerThread[0]}>, ${tileInner}>;\n\n  const RowPerThread = ${workPerThread[1]};\n  const ColPerThread = ${workPerThread[0]};\n  const InnerElementSize = ${innerElementSize};\n  const TileInner = ${tileInner};\n\n  @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)\n  fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,\n            @builtin(global_invocation_id) GlobalId : vec3<u32>,\n            @builtin(num_workgroups) NumWorkgroups: vec3<u32>,\n            @builtin(workgroup_id) workgroupId: vec3<u32>) {\n    localId = LocalId;\n    globalId = GlobalId;\n    numWorkgroups = NumWorkgroups;\n\n    let localRow = i32(localId.y);\n    let tileRow = ${isVectorA ? '0' : 'localRow * RowPerThread'};\n    let tileCol = i32(localId.x);\n\n    let globalRow = ${isVectorA ? '0' : 'i32(globalId.y) * RowPerThread'};\n    let globalCol = i32(globalId.x);\n    let batch = ${splitK ? '0' : 'i32(globalId.z)'};\n    let globalRowStart = i32(workgroupId.y) * ${tileAOuter};\n\n    let numTiles = ${\n      splitK ? `${Math.ceil(splitedDimInner / tileInner)}` :\n               '(uniforms.dimInner - 1) / TileInner + 1'};\n    var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : '0'};\n\n    var acc: array<vec4<f32>, RowPerThread>;\n\n    // Loop over shared dimension.\n    let tileRowB = localRow * ${rowPerThreadB};\n    for (var t = 0; t < numTiles; t = t + 1) {\n        // Load one tile of A into local memory.\n        for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n            let inputRow = tileRow + innerRow;\n            let inputCol = tileCol;\n            ${writeDataToSubAVec4Snippet(transposeA)}\n        }\n\n        // Load one tile of B into local memory.\n        for (var innerRow = 0; innerRow < ${\n      rowPerThreadB}; innerRow = innerRow + 1) {\n            let inputRow = tileRowB + innerRow;\n            let inputCol = tileCol;\n            mm_Bsub[inputRow][inputCol] = mm_readB(batch, kStart + inputRow, globalCol);\n        }\n        kStart = kStart + TileInner;\n        workgroupBarrier();\n\n        // Compute acc values for a single thread.\n        for (var k = 0; k < TileInner / InnerElementSize; k = k + 1) {\n            let BCached0 = mm_Bsub[k * InnerElementSize][tileCol];\n            let BCached1 = mm_Bsub[k * InnerElementSize + 1][tileCol];\n            let BCached2 = mm_Bsub[k * InnerElementSize + 2][tileCol];\n            ${\n      innerElementSize === 3 ?\n          '' :\n          'let BCached3 = mm_Bsub[k * InnerElementSize + 3][tileCol];'}\n\n            ${calculateResultSnippet(transposeA, innerElementSize)}\n        }\n\n        workgroupBarrier();\n    }\n\n    for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n        mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);\n    }\n  }`;\n}\n\nconst writeDataToSubASnippet = (transpose: boolean) => {\n  if (transpose) {\n    return `\n        mm_Asub[inputRow][inputCol] = mm_readA(batch,\n          kStart + inputRow,\n          globalRowStart + inputCol);\n        `;\n\n  } else {\n    return `\n        mm_Asub[inputRow][inputCol] = mm_readA(batch,\n          globalRowStart + inputRow,\n          kStart + inputCol);\n        `;\n  }\n};\n\nconst readDataFromSubASnippet = (transposeA: boolean) => {\n  return transposeA ? 'let ACached = mm_Asub[k][tileRow + innerRow];' :\n\n                      'let ACached = mm_Asub[tileRow + innerRow][k];';\n};\n\n// sequentialAccessByThreads means sequential data in memory is accessed by\n// threads, instead of a single thread (default behavior).\nexport function makeMatMulPackedSource(\n    workPerThread: number[], workGroupSize: [number, number, number],\n    transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32,\n    sequentialAccessByThreads = false): string {\n  const tileAOuter = workPerThread[1] * workGroupSize[1];\n  const tileBOuter = workPerThread[0] * workGroupSize[0];\n  const tileAWidth = transposeA ? tileAOuter : tileInner;\n  const tileAHight = transposeA ? tileInner : tileAOuter;\n  util.assert(\n      tileAHight % workGroupSize[1] === 0 &&\n          tileAWidth % workGroupSize[0] === 0 &&\n          tileInner % workGroupSize[1] === 0,\n      () => `tileAHight ${tileAHight} must be divisible by workGroupSize[1]${\n          workGroupSize[1]}, tileAWidth ${\n          tileAWidth} must be divisible by workGroupSize[0]${\n          workGroupSize[0]}, tileInner ${\n          tileInner} must be divisible by workGroupSize[1]${workGroupSize[1]}`);\n  const rowPerThreadA = tileAHight / workGroupSize[1];\n  const colPerThreadA = tileAWidth / workGroupSize[0];\n  const rowPerThreadB = tileInner / workGroupSize[1];\n  const matmulSnippet = sequentialAccessByThreads ?\n      `\n      let localRow = i32(localId.y);\n      let localCol = i32(localId.x);\n      let globalRowStart = i32(workgroupId.y) * ${tileAOuter};\n      let globalColStart = i32(workgroupId.x) * ${tileBOuter};\n\n      // Loop over shared dimension.\n      for (var t = 0; t < numTiles; t = t + 1) {\n        // Load one tile of A into local memory.\n        for (var inputRow = localRow; inputRow < ${\n          tileAHight}; inputRow = inputRow + ${workGroupSize[1]}) {\n          for (var inputCol = localCol; inputCol < ${\n          tileAWidth}; inputCol = inputCol + ${workGroupSize[0]}) {\n            ${writeDataToSubASnippet(transposeA)}\n          }\n        }\n        // Load one tile of B into local memory.\n        for (var inputRow = localRow; inputRow < ${\n          tileInner}; inputRow = inputRow + ${workGroupSize[1]}) {\n              for (var inputCol = localCol; inputCol < ${\n          tileBOuter}; inputCol = inputCol + ${workGroupSize[0]}) {\n            mm_Bsub[inputRow][inputCol] = mm_readB(batch,\n              kStart + inputRow,\n              globalColStart + inputCol);\n          }\n        }\n        kStart = kStart + TileInner;\n        workgroupBarrier();\n\n        // Compute acc values for a single thread.\n        var BCached : array<f32, ColPerThread>;\n        for (var k = 0; k < TileInner; k = k + 1) {\n          for (var inner = 0; inner < ColPerThread; inner = inner + 1) {\n            BCached[inner] = mm_Bsub[k][localCol + inner * ${workGroupSize[0]}];\n          }\n          for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n            let ACached = ${\n          transposeA ?\n              `mm_Asub[k][localRow + innerRow * ${workGroupSize[1]}];` :\n              `mm_Asub[localRow + innerRow * ${workGroupSize[1]}][k];`}\n            for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n              acc[innerRow][innerCol] = acc[innerRow][innerCol] +\n                  ACached * BCached[innerCol];\n            }\n          }\n        }\n        workgroupBarrier();\n      }\n      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n        let gRow = globalRowStart + localRow + innerRow * ${workGroupSize[1]};\n        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n          let gCol = globalColStart + localCol + innerCol * ${workGroupSize[0]};\n          mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);\n        }\n      }\n      ` :\n      `\n  let tileRow = i32(localId.y) * RowPerThread;\n  let tileCol = i32(localId.x) * ColPerThread;\n\n  let globalRow = i32(globalId.y) * RowPerThread;\n  let globalCol = i32(globalId.x) * ColPerThread;\n  let globalRowStart = i32(workgroupId.y) * ${tileAOuter};\n\n  let tileRowA = i32(localId.y) * ${rowPerThreadA};\n  let tileColA = i32(localId.x) * ${colPerThreadA};\n  let tileRowB = i32(localId.y) * ${rowPerThreadB};\n  // Loop over shared dimension.\n  for (var t = 0; t < numTiles; t = t + 1) {\n    // Load one tile of A into local memory.\n    for (var innerRow = 0; innerRow < ${\n          rowPerThreadA}; innerRow = innerRow + 1) {\n      for (var innerCol = 0; innerCol < ${\n          colPerThreadA}; innerCol = innerCol + 1) {\n        let inputRow = tileRowA + innerRow;\n        let inputCol = tileColA + innerCol;\n        ${writeDataToSubASnippet(transposeA)}\n      }\n    }\n\n    // Load one tile of B into local memory.\n    for (var innerRow = 0; innerRow < ${\n          rowPerThreadB}; innerRow = innerRow + 1) {\n      for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n        let inputRow = tileRowB + innerRow;\n        let inputCol = tileCol + innerCol;\n        mm_Bsub[inputRow][inputCol] = mm_readB(batch,\n          kStart + inputRow,\n          globalCol + innerCol);\n      }\n    }\n    kStart = kStart + TileInner;\n    workgroupBarrier();\n\n    // Compute acc values for a single thread.\n    var BCached : array<f32, ColPerThread>;\n    for (var k = 0; k < TileInner; k = k + 1) {\n      for (var inner = 0; inner < ColPerThread; inner = inner + 1) {\n        BCached[inner] = mm_Bsub[k][tileCol + inner];\n      }\n\n      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n        ${readDataFromSubASnippet(transposeA)}\n        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n          acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];\n        }\n      }\n    }\n\n    workgroupBarrier();\n  }\n\n  for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n    for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n      mm_write(batch, globalRow + innerRow, globalCol + innerCol,\n          acc[innerRow][innerCol]);\n    }\n  }\n  `;\n\n  return `\n    var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHight}>;\n    var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;\n    const RowPerThread = ${workPerThread[1]};\n    const ColPerThread = ${workPerThread[0]};\n    const TileInner = ${tileInner};\n\n    @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)\n    fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,\n              @builtin(global_invocation_id) GlobalId : vec3<u32>,\n              @builtin(num_workgroups) NumWorkgroups: vec3<u32>,\n              @builtin(workgroup_id) workgroupId: vec3<u32>) {\n      localId = LocalId;\n      globalId = GlobalId;\n      numWorkgroups = NumWorkgroups;\n\n      let batch = ${splitK ? '0' : 'i32(globalId.z)'};\n      let numTiles = ${\n      splitK ? `${Math.ceil(splitedDimInner / tileInner)}` :\n               '(uniforms.dimInner - 1) / TileInner + 1'};\n      var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : '0'};\n\n      var acc : array<array<f32, ColPerThread>, RowPerThread>;\n\n      // Without this initialization strange values show up in acc.\n      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {\n        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {\n          acc[innerRow][innerCol] = 0.0;\n        }\n      }\n      ${matmulSnippet}\n    }\n  `;\n}\n\nconst readVectorASnippet = (transpose: boolean) => {\n  return transpose ? `\n      mm_readA(batch, colA, globalRow),\n      mm_readA(batch, colA + 1, globalRow),\n      mm_readA(batch, colA + 2, globalRow),\n      mm_readA(batch, colA + 3, globalRow)\n  ` :\n                     `\n      mm_readA(batch, globalRow, colA),\n      mm_readA(batch, globalRow, colA + 1),\n      mm_readA(batch, globalRow, colA + 2),\n      mm_readA(batch, globalRow, colA + 3)\n  `;\n};\n\nexport function makeVectorMatrixProductSource(\n    workGroupSize: [number, number, number], transposeA = false): string {\n  util.assert(\n      workGroupSize[1] === 1 && workGroupSize[2] === 1,\n      () => `A linear work group size is required. But got ${workGroupSize}.`);\n  return `\n    const TileSize = ${workGroupSize[0] * 4};\n    var<workgroup> mm_Asub : array<vec4<f32>, ${workGroupSize[0]}>;\n\n    ${main()} {\n      let tileCol = i32(localId.x);\n      let globalCol = i32(globalId.x);\n      let globalRow = i32(globalId.y);\n\n      let numTiles = (uniforms.dimInner - 1) / TileSize + 1;\n      let batch = i32(globalId.z);\n      // Without this initialization strange values show up in acc.\n      var acc = 0.0;\n\n      // Loop over shared dimension.\n      for (var t = 0; t < numTiles; t = t + 1) {\n        // Load one tile of A into local memory.\n        let colA = t * TileSize + tileCol * 4;\n        mm_Asub[tileCol] = vec4<f32>(${readVectorASnippet(transposeA)});\n        workgroupBarrier();\n\n        // Compute acc values for a single thread.\n        for (var k = 0; k < TileSize / 4; k = k + 1) {\n          let rowB = t * TileSize + k * 4;\n          let BCached = vec4<f32>(mm_readB(batch, rowB, globalCol),\n                              mm_readB(batch, rowB + 1, globalCol),\n                              mm_readB(batch, rowB + 2, globalCol),\n                              mm_readB(batch, rowB + 3, globalCol));\n\n          let ACached = mm_Asub[k];\n          acc = acc + dot(ACached, BCached);\n        }\n\n        workgroupBarrier();\n      }\n\n      mm_write(batch, globalRow, globalCol, acc);\n    }\n  `;\n}\n\nexport class MatMulPackedProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['A', 'B'];\n  uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;\n  workGroupSize: [number, number, number];\n  elementsPerThread: [number, number, number];\n  transposeA: boolean;\n  transposeB: boolean;\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivationWeights: boolean;\n  batchAEqualOne: boolean;\n  batchBEqualOne: boolean;\n  fitAOuter: boolean;\n  fitBOuter: boolean;\n  fitInner: boolean;\n  tileInner: number;\n  isVectorA: boolean;\n  isVec4: boolean;\n  private sequentialAccessByThreads: boolean;\n\n  constructor(\n      aShape: [number, number, number], outputShape: [number, number, number],\n      batchAEqualOne: boolean, batchBEqualOne: boolean, transposeA = false,\n      transposeB = false, bias: TensorInfo = null,\n      activation: backend_util.Activation = null,\n      preluActivationWeights: TensorInfo = null,\n      sequentialAccessByThreads = false) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = {x: [2], y: [1], z: [0]};\n    const dimInner = transposeA ? aShape[1] : aShape[2];\n    this.isVec4 = ((dimInner % 4 === 0 && !transposeA) ||\n                   (outputShape[1] % 4 === 0 && transposeA)) &&\n        outputShape[2] % 4 === 0 && !transposeB;\n    this.isVectorA = outputShape[1] === 1 && !transposeA;\n\n    if (!this.isVec4 && this.isVectorA) {\n      // For makeVectorMatrixProductSource\n      this.elementsPerThread = [1, 1, 1];\n      this.workGroupSize = [32, 1, 1];\n    } else {\n      const workGroupInfo = computeWorkGroupInfoForMatMul(\n          outputShape[1], dimInner, outputShape[2], transposeA);\n      this.workGroupSize = workGroupInfo.workGroupSize;\n      this.elementsPerThread = workGroupInfo.elementsPerThread;\n    }\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        this.elementsPerThread);\n\n    const addBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n\n    if (hasPreluActivationWeights) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.sequentialAccessByThreads = sequentialAccessByThreads;\n    this.transposeA = transposeA;\n    this.transposeB = transposeB;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivationWeights = hasPreluActivationWeights;\n    this.batchAEqualOne = batchAEqualOne;\n    this.batchBEqualOne = batchBEqualOne;\n    [this.fitAOuter, this.fitBOuter, this.fitInner] =\n        this.getShapeFit(outputShape[1], outputShape[2], dimInner);\n    this.shaderKey = `matMulPacked_${this.elementsPerThread}_${transposeA}_${\n        transposeB}_${this.activation}_${this.fitAOuter}_${this.fitBOuter}_${\n        this.fitInner}_${this.isVec4}_${this.isVectorA}_${\n        this.batchAEqualOne}_${this.batchBEqualOne}_${\n        this.sequentialAccessByThreads}`;\n  }\n\n  getShapeFit(dimAOuter: number, dimBOuter: number, dimInner: number):\n      boolean[] {\n    const tileAOuter = this.workGroupSize[1] * this.elementsPerThread[1];\n    const tileBOuter = this.workGroupSize[0] * this.elementsPerThread[0];\n\n    if (!this.isVec4 && this.isVectorA) {\n      // For makeVectorMatrixProductSource\n      this.tileInner = this.workGroupSize[0] * 4;\n    } else {\n      this.tileInner = tileBOuter;\n    }\n\n    const fitAOuter = dimAOuter % tileAOuter === 0;\n    const fitBOuter = dimBOuter % tileBOuter === 0;\n    const fitInner = dimInner % this.tileInner === 0;\n    return [fitAOuter, fitBOuter, fitInner];\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${\n        activationFnSnippet(\n            this.activation, this.hasPreluActivationWeights, this.isVec4)}\n      ${\n        matMulReadWriteFnSource(\n            this.addBias, this.activation, this.batchAEqualOne,\n            this.batchBEqualOne,\n            false /* transposeA is implemented in makeMatMulPackedSource */,\n            this.transposeB, this.fitAOuter, this.fitBOuter, this.fitInner,\n            this.isVec4 ? 4 : 1)}\n      ${\n        this.isVec4 ?\n            makeMatMulPackedVec4Source(\n                this.elementsPerThread, this.workGroupSize, this.transposeA,\n                this.tileInner, false, null, this.isVectorA) :\n            (this.isVectorA ? makeVectorMatrixProductSource(\n                                  this.workGroupSize, this.transposeA) :\n                              makeMatMulPackedSource(\n                                  this.elementsPerThread, this.workGroupSize,\n                                  this.transposeA, this.tileInner, false, null,\n                                  this.sequentialAccessByThreads))}\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet} from './activation_util';\nimport {matMulReadWriteFnSource} from './matmul_packed_webgpu';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch} from './webgpu_util';\n\nexport function makeMatMulReduceSource(): string {\n  return `\n    var<workgroup> sumValues : array<f32, workGroupSizeX>;\n    ${main()} {\n      let coords = getOutputCoords();\n      let batch = coords[0];\n      let row = coords[1];\n      let col = coords[2];\n      var sum = 0.0;\n      let Length = uniforms.dimInner;\n      for (var k = i32(localId.x); k < Length; k = k + i32(workGroupSizeX)) {\n        let dataA = mm_readA(batch, row, k);\n        let dataB = mm_readB(batch, k, col);\n        sum = sum + dataA * dataB;\n      }\n      sumValues[localId.x] = sum;\n      workgroupBarrier();\n\n      for(var currentSize = workGroupSizeX / 2u; currentSize > 1u;\n          currentSize = currentSize / 2u) {\n        if (localId.x < currentSize)\n        {\n          sumValues[localId.x] = sumValues[localId.x] + sumValues[localId.x + currentSize];\n        }\n        workgroupBarrier();\n      }\n\n      if (localId.x == 0u) {\n        sum = sumValues[0] + sumValues[1];\n        mm_write(batch, row, col, sum);\n      }\n    }\n  `;\n}\n\nexport class MatMulReduceProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['A', 'B'];\n  uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;\n  workGroupSize: [number, number, number] = [256, 1, 1];\n  transposeA: boolean;\n  transposeB: boolean;\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivationWeights: boolean;\n  batchAEqualOne: boolean;\n  batchBEqualOne: boolean;\n\n  constructor(\n      outputShape: [number, number, number], batchAEqualOne: boolean,\n      batchBEqualOne: boolean, transposeA = false, transposeB = false,\n      bias: TensorInfo = null, activation: backend_util.Activation = null,\n      preluActivationWeights: TensorInfo = null) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = {x: [], y: [1, 2], z: [0]};\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    const addBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n\n    if (hasPreluActivationWeights) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.transposeA = transposeA;\n    this.transposeB = transposeB;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivationWeights = hasPreluActivationWeights;\n    this.batchAEqualOne = batchAEqualOne;\n    this.batchBEqualOne = batchBEqualOne;\n    this.shaderKey = `matMulReduce_${this.activation}_${transposeA}_${\n        transposeB}_${this.batchAEqualOne}_${this.batchBEqualOne}`;\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}\n      ${\n        matMulReadWriteFnSource(\n            this.addBias, this.activation, this.batchAEqualOne,\n            this.batchBEqualOne, this.transposeA, this.transposeB)}\n      ${makeMatMulReduceSource()}\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo} from '@tensorflow/tfjs-core';\nimport {activationFnSnippet} from './activation_util';\nimport {matMulReadWriteFnSource} from './matmul_packed_webgpu';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\n\nexport function makeMatMulSmallOutputSizeSource(\n    workGroupSize: [number, number, number]): string {\n  const tileAOuter = workGroupSize[1];\n  const tileBOuter = workGroupSize[0];\n  const tileInner = tileAOuter > tileBOuter ? tileAOuter : tileBOuter;\n  return `\n  var<workgroup> mm_Asub : array<array<f32, ${tileInner}>, ${tileAOuter}>;\n  var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;\n\n  // If the output size is small for matrix multiplication, avoid to use vec4\n  // and handle some elements per thread to optimally utilize the ALU.\n  // Read data from global memory to registers firstly, then store them into\n  // shared memory, so it is instruction-Level parallelism for arithmetic\n  // operations and others handle IO operations between barrier api, makes ALU\n  // and load/store units work simultaneously, could improves the performance.\n  ${main()} {\n    let tileRow = i32(localId.y);\n    let tileCol = i32(localId.x);\n    let globalRow = i32(globalId.y);\n    let globalCol = i32(globalId.x);\n    let batch = i32(globalId.z);\n\n    // uniforms.dimInner should be greater than 0.\n    let numTiles = (uniforms.dimInner - 1) / ${tileInner} + 1;\n    var acc = 0.0;\n\n    var globalColA = tileCol;\n    var globalRowB = 0;\n    var regA = mm_readA(batch, globalRow, globalColA);\n    var regB0 = mm_readB(batch, globalRowB + 2 * tileRow, globalCol);\n    var regB1 = mm_readB(batch, globalRowB + 2 * tileRow + 1, globalCol);\n    globalColA = globalColA + ${tileInner};\n    globalRowB = globalRowB + ${tileInner};\n\n    for (var t = 0; t < numTiles; t = t + 1) {\n      mm_Asub[tileRow][tileCol] = regA;\n      mm_Bsub[2 * tileRow][tileCol] = regB0;\n      mm_Bsub[2 * tileRow + 1][tileCol] = regB1;\n\n      workgroupBarrier();\n\n      regA = mm_readA(batch, globalRow, globalColA);\n      regB0 = mm_readB(batch, globalRowB + 2 * tileRow, globalCol);\n      regB1 = mm_readB(batch, globalRowB + 2 * tileRow + 1, globalCol);\n      globalColA = globalColA + ${tileInner};\n      globalRowB = globalRowB + ${tileInner};\n\n      for (var k = 0; k < ${tileInner}; k = k + 1) {\n        acc = acc + mm_Asub[tileRow][k] * mm_Bsub[k][tileCol];\n      }\n      workgroupBarrier();\n    }\n\n    mm_write(batch, globalRow, globalCol, acc);\n  }\n  `;\n}\n\nexport class MatMulSmallOutputSizeProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['A', 'B'];\n  uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;\n  workGroupSize: [number, number, number] = [16, 8, 1];\n  transposeA: boolean;\n  transposeB: boolean;\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivationWeights: boolean;\n  batchAEqualOne: boolean;\n  batchBEqualOne: boolean;\n\n  constructor(\n      aShape: [number, number, number], bShape: [number, number, number],\n      outputShape: [number, number, number], transposeA = false,\n      transposeB = false, bias: TensorInfo = null,\n      activation: backend_util.Activation = null,\n      preluActivationWeights: TensorInfo = null) {\n    this.outputShape = outputShape;\n\n    this.dispatchLayout = {x: [2], y: [1], z: [0]};\n    this.dispatch = [\n      Math.ceil(outputShape[2] / this.workGroupSize[0]),\n      Math.ceil(outputShape[1] / this.workGroupSize[1]), outputShape[0]\n    ];\n\n    const addBias = bias != null;\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    if (hasPreluActivationWeights) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.transposeA = transposeA;\n    this.transposeB = transposeB;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivationWeights = hasPreluActivationWeights;\n    this.batchAEqualOne = aShape[0] === 1;\n    this.batchBEqualOne = bShape[0] === 1;\n    this.shaderKey = `matMulSmallOutputSize_${this.activation}_${transposeA}_${\n        transposeB}_${this.batchAEqualOne}_${this.batchBEqualOne}`;\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}\n      ${\n        matMulReadWriteFnSource(\n            this.addBias, this.activation, this.batchAEqualOne,\n            this.batchBEqualOne, this.transposeA, this.transposeB)}\n      ${makeMatMulSmallOutputSizeSource(this.workGroupSize)}\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet, biasActivationSnippet, typeSnippet} from './activation_util';\nimport {makeMatMulPackedSource, makeMatMulPackedVec4Source, matMulReadFnSource} from './matmul_packed_webgpu';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class MatMulSplitKProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['A', 'B'];\n  uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;\n  workGroupSize: [number, number, number] = [8, 8, 1];\n  elementsPerThread: [number, number, number];\n  transposeA: boolean;\n  transposeB: boolean;\n  atomic = true;\n  batchAEqualOne: boolean;\n  batchBEqualOne: boolean;\n  isVec4 = false;\n  splitedDimInner = 128;\n\n  constructor(\n      outputShape: [number, number, number], dimInner: number,\n      batchAEqualOne: boolean, batchBEqualOne: boolean, transposeA = false,\n      transposeB = false) {\n    util.assert(\n        outputShape[0] === 1,\n        () => 'MatMulSplitKProgram only supports batch = 1.');\n    this.outputShape = outputShape;\n    this.dispatchLayout = {x: [2], y: [1], z: [0, 3]};\n    this.isVec4 = (transposeA && this.outputShape[1] % 4 === 0 ||\n                   !transposeA && dimInner % 4 === 0) &&\n        this.outputShape[2] % 4 === 0;\n    this.elementsPerThread = [4, 4, this.splitedDimInner];\n\n    if (!this.isVec4) {\n      if (this.outputShape[1] < 16) {\n        this.elementsPerThread[1] = 1;\n      }\n      if (this.outputShape[2] < 16) {\n        this.elementsPerThread[0] = 1;\n      }\n    }\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout,\n        [\n          this.outputShape[0], this.outputShape[1], this.outputShape[2],\n          dimInner\n        ],\n        this.workGroupSize, this.elementsPerThread);\n\n    this.transposeA = transposeA;\n    this.transposeB = transposeB;\n    this.batchAEqualOne = batchAEqualOne;\n    this.batchBEqualOne = batchBEqualOne;\n    this.shaderKey =\n        `matMulSplitK_${transposeA}_${transposeB}_${batchAEqualOne}_${\n            batchBEqualOne}_${this.elementsPerThread}_${this.isVec4}`;\n  }\n\n  getUserCode(): string {\n    // atomicAdd only supports uint/int type. For float, we use\n    // atomicCompareExchangeWeak to simulate.\n    const atomicAddSnippet = (component: number) => {\n      return `\n      for (var i = 0; i < ${component}; i = i + 1)\n      {\n        var oldValue = atomicLoad(&(result[flatIndex + i]));\n        var exchanged = false;\n        for (; !exchanged;) {\n          let newValueF32 = bitcast<f32>(oldValue) + ${\n          component > 1 ? 'value[i]' : 'value'};\n          let newValue = bitcast<i32>(newValueF32);\n          let res = atomicCompareExchangeWeak(&(result[flatIndex + i]), oldValue, newValue);\n          oldValue = res.old_value;\n          exchanged = res.exchanged;\n        }\n      }\n      `;\n    };\n\n    const component = this.isVec4 ? 4 : 1;\n    const userCode = `\n      ${\n        matMulReadFnSource(\n            this.batchAEqualOne, this.batchBEqualOne, false, this.transposeB,\n            false, false, false, component)}\n      fn mm_write(batch: i32, row : i32, colIn : i32, value : ${\n        typeSnippet(component)}) {\n        let col = colIn * ${component};\n        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter) {\n          let coords = vec3<i32>(batch, row, col);\n          let flatIndex = getOutputIndexFromCoords(coords);\n          // The problem is that we should initialize output to zero before using.\n          // Otherwise, the original value will be added to the result.\n          ${atomicAddSnippet(component)}\n        }\n      }\n      ${\n        this.isVec4 ? makeMatMulPackedVec4Source(\n                          this.elementsPerThread, this.workGroupSize,\n                          this.transposeA, 32, true, this.splitedDimInner) :\n                      makeMatMulPackedSource(\n                          this.elementsPerThread, this.workGroupSize,\n                          this.transposeA, 32, true, this.splitedDimInner)}\n    `;\n    return userCode;\n  }\n}\n\nexport class BiasActivationProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  uniforms = '';\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n  private addBias: boolean;\n  private activation: backend_util.Activation;\n  private hasPreluActivationWeights: boolean;\n\n  constructor(\n      outputShape: number[], bias: TensorInfo = null,\n      activation: backend_util.Activation = null,\n      preluActivationWeights: TensorInfo = null) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.addBias = bias != null;\n    this.hasPreluActivationWeights = preluActivationWeights != null;\n    this.activation = activation;\n    if (this.addBias) {\n      this.variableNames.push('bias');\n    }\n\n    if (this.hasPreluActivationWeights) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.shaderKey = `biasActivation_${activation}`;\n  }\n\n  getUserCode(): string {\n    return `\n    ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}\n    ${main('index')} {\n      if (index < uniforms.size) {\n        let coords = getCoordsFromIndex(index);\n        var value = getXByOutputIndex(index);\n        ${biasActivationSnippet(this.addBias, this.activation)}\n        setOutputAtIndex(index, value);\n      }\n    }\n    `;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class FillProgram implements WebGPUProgram {\n  variableNames: string[] = [];\n  outputShape: number[] = [];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  uniforms = 'value : f32,';\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(shape: number[]) {\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = 'fill';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n    ${main('index')} {\n      if (index < uniforms.size) {\n        setOutputAtIndex(index, uniforms.value);\n      }\n    }\n  `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Fill, FillAttrs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {FillProgram} from '../fill_webgpu';\n\nexport function fill(args: {backend: WebGPUBackend, attrs: FillAttrs}):\n    TensorInfo {\n  const {backend, attrs} = args;\n  const {shape, value} = attrs;\n  let {dtype} = attrs;\n\n  dtype = dtype || util.inferDtype(value);\n\n  if (dtype === 'string') {\n    // String type should be handled in CPU memory.\n    const values = util.getArrayFromDType(dtype, util.sizeFromShape(shape));\n    values.fill(value as string);\n    return backend.makeTensorInfo(shape, dtype, values);\n  } else {\n    const program = new FillProgram(shape);\n    const uniformData = [{type: 'float32', data: [value as number]}];\n    return backend.runWebGPUProgram(program, [], dtype, uniformData);\n  }\n}\n\nexport const fillConfig: KernelConfig = {\n  kernelName: Fill,\n  backendName: 'webgpu',\n  kernelFunc: fill as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Reshape, ReshapeAttrs, ReshapeInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nexport function reshape(\n    args: {inputs: ReshapeInputs, backend: WebGPUBackend, attrs: ReshapeAttrs}):\n    TensorInfo {\n  const {inputs, attrs} = args;\n  const {x} = inputs;\n  const {shape} = attrs;\n\n  const xSize = util.sizeFromShape(x.shape);\n  const $shape = util.inferFromImplicitShape(shape, xSize);\n  const $xSize = util.sizeFromShape($shape);\n\n  util.assert(\n      xSize === $xSize,\n      () => `The new shape (${$shape}) has ${$xSize} elements and the old ` +\n          `shape (${x.shape}) has ${xSize} elements. The new shape and old ` +\n          `shape must have the same number of elements.`);\n\n  // Backend needs to track refCount for the dataId for reshape op\n  args.backend.incRef(x.dataId);\n  return {dataId: x.dataId, shape: $shape, dtype: x.dtype};\n}\n\nexport const reshapeConfig: KernelConfig = {\n  kernelName: Reshape,\n  backendName: 'webgpu',\n  kernelFunc: reshape as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, broadcast_util, env, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {MatMulPackedProgram} from '../matmul_packed_webgpu';\nimport {MatMulReduceProgram} from '../matmul_reduce_webgpu';\nimport {MatMulSmallOutputSizeProgram} from '../matmul_small_output_size_webgpu';\nimport {BiasActivationProgram, MatMulSplitKProgram} from '../matmul_splitK_webgpu';\nimport {WebGPUProgram} from '../webgpu_program';\nimport {MatMulProgramType} from '../webgpu_util';\n\nimport {fill} from './Fill';\nimport {reshape} from './Reshape';\n\ntype BatchMatMulConfig = {\n  a: TensorInfo,\n  b: TensorInfo,\n  transposeA: boolean,\n  transposeB: boolean,\n  backend: WebGPUBackend,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: BatchMatMulConfig): TensorInfo {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape: [number, number, number] = transposeA ?\n      [batchDimA, innerShapeA, outerShapeA] :\n      [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape: [number, number, number] = transposeB ?\n      [batchDimB, outerShapeB, innerShapeB] :\n      [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n  const intermediates: TensorInfo[] = [a3d, b3d];\n\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const batchAEqualOne = batchDimA === 1;\n  const batchBEqualOne = batchDimB === 1;\n\n  const inputs: TensorInfo[] = [a3d, b3d];\n  const dimensions = [\n    {type: 'int32', data: [outerShapeA]}, {type: 'int32', data: [outerShapeB]},\n    {type: 'int32', data: [innerShapeA]}\n  ];\n\n  let program: WebGPUProgram;\n  let out: TensorInfo;\n  const outputShape: [number, number, number] =\n      [batchDim, outerShapeA, outerShapeB];\n  let matmulProgramType = env().get('WEBGPU_MATMUL_PROGRAM_TYPE') as number;\n  if (matmulProgramType < 0) {\n    if (outerShapeA * outerShapeB <= 128) {\n      matmulProgramType = MatMulProgramType.MatMulReduceProgram;\n    } else if (\n        // These boundaries are based on bodypix-ResNet50-image-0.5.\n        // TODO: Relax or tight these boundaries when we have a complete matmul\n        // test coverage.\n        batchDim === 1 && outerShapeA <= 128 && outerShapeB <= 48 &&\n        innerShapeB >= 2000) {\n      matmulProgramType = MatMulProgramType.MatMulSplitKProgram;\n    } else if (\n        // When the output size is absolutely small or relatively small, we may\n        // use MatMulSmallOutputSizeProgram to get better performance.\n        // Absolutely small size means that the output size is smaller than [16,\n        // 512]. Relatively small size means that one demension size of the\n        // output is smaller than 16, and the output size is also more than or\n        // equal two times smaller than each of the two input sizes. For\n        // example, if input sizes are [12, 2048] and [2048, 1024], the output\n        // size is [12, 1024], which is relatively small compared to input\n        // sizes.\n        (outerShapeA <= 16 &&\n         (outerShapeB <= 512 || innerShapeB >= 2 * outerShapeB)) ||\n        (outerShapeB <= 16 &&\n         (outerShapeA <= 512 || innerShapeA >= 2 * outerShapeA))) {\n      matmulProgramType = MatMulProgramType.MatMulSmallOutputSizeProgram;\n    } else {\n      matmulProgramType = MatMulProgramType.MatMulPackedProgram;\n    }\n  }\n\n  switch (matmulProgramType) {\n    case MatMulProgramType.MatMulReduceProgram:\n      program = new MatMulReduceProgram(\n          outputShape, batchAEqualOne, batchBEqualOne, transposeA, transposeB,\n          bias, activation, preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulSplitKProgram: {\n      // The output buffer must be initailzed to zero before using since we\n      // use atomicAdd in MatMulSplitKProgram.\n      out = fill(\n          {backend, attrs: {shape: outputShape, value: 0, dtype: a.dtype}});\n      program = new MatMulSplitKProgram(\n          outputShape, innerShapeB, batchAEqualOne, batchBEqualOne, transposeA,\n          transposeB);\n      if (bias || activation) {\n        out =\n            backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n        const biasActivationProgram = new BiasActivationProgram(\n            out.shape, bias, activation, preluActivationWeights);\n        let uniformData = null;\n        const activationInputs: TensorInfo[] = [out];\n        if (bias) {\n          activationInputs.push(bias);\n        }\n        if (preluActivationWeights) {\n          activationInputs.push(preluActivationWeights);\n        }\n        if (activation === 'leakyrelu') {\n          uniformData = [{type: 'float32', data: [leakyreluAlpha]}];\n          biasActivationProgram.uniforms += ' alpha : f32,';\n        }\n        const outActivated = backend.runWebGPUProgram(\n            biasActivationProgram, activationInputs, out.dtype, uniformData);\n        intermediates.push(out);\n        const outReshaped = reshape(\n            {inputs: {x: outActivated}, backend, attrs: {shape: outShape}});\n        intermediates.push(outActivated);\n        for (const i of intermediates) {\n          backend.disposeData(i.dataId);\n        }\n        return outReshaped;\n      }\n      break;\n    }\n    case MatMulProgramType.MatMulSmallOutputSizeProgram:\n      program = new MatMulSmallOutputSizeProgram(\n          a3dShape, b3dShape, outputShape, transposeA, transposeB, bias,\n          activation, preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulPackedProgram:\n      // Experiments show that sequential access is more friendly for Intel\n      // GPUs.\n      const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n      program = new MatMulPackedProgram(\n          a3dShape, outputShape, batchAEqualOne, batchBEqualOne, transposeA,\n          transposeB, bias, activation, preluActivationWeights,\n          sequentialAccessByThreads);\n      break;\n    default:\n      throw new Error(`Unsupported MatMulProgramType ${matmulProgramType}.`);\n  }\n\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (preluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({type: 'float32', data: [leakyreluAlpha]});\n    program.uniforms += ' alpha : f32,';\n  }\n  out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n  const outReshaped =\n      reshape({inputs: {x: out}, backend, attrs: {shape: outShape}});\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return outReshaped;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {batchMatMulImpl} from './BatchMatMul_impl';\n\nexport function _fusedMatMul(args: {\n  inputs: _FusedMatMulInputs,\n  attrs: _FusedMatMulAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b, bias, preluActivationWeights} = inputs;\n  const {transposeA, transposeB, activation, leakyreluAlpha} = attrs;\n\n  return batchMatMulImpl({\n    a,\n    b,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    preluActivationWeights,\n    leakyreluAlpha,\n    activation\n  });\n}\n\nexport const _fusedMatMulConfig: KernelConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'webgpu',\n  kernelFunc: _fusedMatMul as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {BinaryOpType, getBinaryOpString} from './binary_op_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class BinaryOpComplexProgram implements WebGPUProgram {\n  variableNames = ['AReal', 'AImag', 'BReal', 'BImag'];\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [128, 1, 1];\n  op: BinaryOpType;\n  size = true;\n\n  constructor(op: BinaryOpType, aShape: number[], bShape: number[]) {\n    this.outputShape = backend_util.assertAndGetBroadcastShape(aShape, bShape);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = `binaryOpComplex_${op}`;\n    this.op = op;\n  }\n\n  getUserCode(): string {\n    const opStr = getBinaryOpString(this.op, false);\n    const userCode = `\n      fn binaryOpComplex(\n          areal : f32, aimag : f32, breal : f32, bimag : f32) -> f32 {\n        ${opStr}\n      }\n\n      ${main('index')} {\n        if(index < uniforms.size) {\n          let areal = getARealByOutputIndex(index);\n          let aimag = getAImagByOutputIndex(index);\n          let breal = getBRealByOutputIndex(index);\n          let bimag = getBImagByOutputIndex(index);\n          setOutputAtIndex(index, binaryOpComplex(areal, aimag, breal, bimag));\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, util} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType, getBinaryOpString} from './binary_op_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class BinaryOpProgram implements WebGPUProgram {\n  dispatch: [number, number, number];\n  dispatchLayout: {x: number[]};\n  isVec4: boolean;\n  op: BinaryOpType;\n  outputShape: number[];\n  shaderKey: string;\n  size = true;\n  variableNames = ['A', 'B'];\n  workGroupSize: [number, number, number];\n  workPerThread: number;\n\n  private lastDimensionSize: number;\n  private useSharedMemoryWithA: boolean;\n  private useSharedMemoryWithB: boolean;\n  private type: string;\n\n  constructor(op: BinaryOpType, aShape: number[], bShape: number[]) {\n    this.outputShape = backend_util.assertAndGetBroadcastShape(aShape, bShape);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.op = op;\n\n    this.useSharedMemoryWithA =\n        aShape.length <= 1 && bShape.length > 1 && aShape[0] < 128;\n    this.useSharedMemoryWithB =\n        bShape.length <= 1 && aShape.length > 1 && bShape[0] < 128;\n\n    if (this.useSharedMemoryWithA || this.useSharedMemoryWithB) {\n      this.isVec4 = false;\n      // lastDimensionSize is used as sharedBuf array size, so can not be\n      // used as uniform.\n      this.lastDimensionSize =\n          this.useSharedMemoryWithB ? bShape[0] : aShape[0];\n      this.shaderKey = `binary_${this.type}_${op}_${this.lastDimensionSize}_${\n          this.useSharedMemoryWithB}`;\n      this.type = 'shared';\n      // This is an experimental value when using shared memory.\n      // Note that the maximum of workgroup X dimension is 256.\n      this.workGroupSize = [256, 1, 1];\n      this.workPerThread = 1;\n    } else {\n      if (util.arraysEqual(aShape, bShape) &&\n          util.sizeFromShape(aShape) % 4 === 0) {\n        this.isVec4 = true;\n        this.type = 'vec4';\n        this.workPerThread = 4;\n      } else {\n        this.isVec4 = false;\n        this.type = 'plain';\n        this.workPerThread = 1;\n      }\n      this.shaderKey = `binary_${this.type}_${op}`;\n      // TODO(jiajia.qin@intel.com): Heuristically select a good work group\n      // size.\n      this.workGroupSize = [128, 1, 1];\n    }\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n  }\n\n  getUserCode(): string {\n    let userCode;\n    const dType = this.isVec4 ? 'vec4<f32>' : 'f32';\n    const opFnStr = `\n    fn binaryOperation(a : ${dType}, b : ${dType}) -> ${dType} {\n      ${getBinaryOpString(this.op, this.isVec4)}\n    };\n    `;\n\n    if (this.type === 'shared') {\n      const sharedIndexSnippet = this.lastDimensionSize > 1 ?\n          `coords[${this.outputShape.length - 1}]` :\n          '0';\n      const accessDataSnippet = this.useSharedMemoryWithB ?\n          `let a = getAByOutputIndex(index);\n          let b = sharedBuf[${sharedIndexSnippet}];` :\n          `let a = sharedBuf[${sharedIndexSnippet}];\n          let b = getBByOutputIndex(index);`;\n      userCode = `\n        ${opFnStr}\n        var<workgroup> sharedBuf : array<f32, ${this.lastDimensionSize}>;\n        ${main('index')} {\n          // Fill in the shared memory buffer.\n          let localIndex = i32(localId.x);\n          if(localIndex < ${this.lastDimensionSize}) {\n            sharedBuf[localIndex] = f32(${\n          this.useSharedMemoryWithB ? 'B' : 'A'}[localIndex]);\n          }\n          workgroupBarrier();\n\n          if(index < uniforms.size) {\n            let coords = getCoordsFromIndex(index);\n            ${accessDataSnippet}\n            setOutputAtIndex(index, binaryOperation(a, b));\n          }\n        }\n        `;\n    } else {\n      userCode = `\n       ${opFnStr}\n       ${main('index')} {\n         if (index < uniforms.size) {\n           let a = getAByOutputIndex(index);\n           let b = getBByOutputIndex(index);\n           setOutputAtIndex(index, binaryOperation(a, b));\n         }\n       }\n       `;\n    }\n\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Identity, IdentityInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\nimport {WebGPUBackend} from '../backend_webgpu';\n\nexport function identity(\n    args: {inputs: IdentityInputs, backend: WebGPUBackend}): TensorInfo {\n  const {inputs} = args;\n  const {x} = inputs;\n\n  args.backend.incRef(x.dataId);\n  return {dataId: x.dataId, shape: x.shape, dtype: x.dtype};\n}\n\nexport const identityConfig: KernelConfig = {\n  kernelName: Identity,\n  backendName: 'webgpu',\n  kernelFunc: identity as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Complex, ComplexInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {identity} from './Identity';\n\n/**\n * Complex tensors share data with their real and imaginary components. Complex\n * tensors' reference to the components is tracked by refCount on the individual\n * component. The refCounts are increased by the identity call.\n *\n * When a complex tensor is disposed, it will reduce the refCount on the\n * components by calling disposeData on each.\n */\nexport function complex(args: {inputs: ComplexInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {real, imag} = inputs;\n\n  const complexInfo = backend.makeTensorInfo(real.shape, 'complex64');\n  const complex = backend.tensorMap.get(complexInfo.dataId);\n\n  const realTensorInfo = identity({inputs: {x: real}, backend});\n\n  const imagTensorInfo = identity({inputs: {x: imag}, backend});\n\n  complex.complexTensorInfos = {real: realTensorInfo, imag: imagTensorInfo};\n\n  return complexInfo;\n}\n\nexport const complexConfig: KernelConfig = {\n  kernelName: Complex,\n  backendName: 'webgpu',\n  kernelFunc: complex as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getUnaryOpString, UnaryOpType} from './unary_op_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class UnaryOpProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['A'];\n  workGroupSize: [number, number, number];\n  op: UnaryOpType;\n  uniforms?: string;\n  size = true;\n\n  constructor(outputShape: number[], op: UnaryOpType) {\n    // TODO(jiajia.qin@intel.com): Heuristically select a good work group size.\n    const workGroupSizeX = 128;\n    this.workGroupSize = [workGroupSizeX, 1, 1];\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.op = op;\n    this.shaderKey = `unary_${op}`;\n  }\n\n  getUserCode(): string {\n    return `\n      fn unaryOperation(a : f32) -> f32 {\n        ${getUnaryOpString(this.op, false)}\n      }\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let a = getAByOutputIndex(index);\n          setOutputAtIndex(index, unaryOperation(a));\n        }\n      }\n      `;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BinaryInputs, DataType, KernelFunc, TensorInfo, TypedArray, UnaryInputs, upcastType} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {BinaryOpComplexProgram} from '../binary_op_complex_webgpu';\nimport {BinaryOpType} from '../binary_op_util';\nimport {BinaryOpProgram} from '../binary_op_webgpu';\nimport {complex} from '../kernels/Complex';\nimport {UnaryOpType} from '../unary_op_util';\nimport {UnaryOpProgram} from '../unary_op_webgpu';\n\nimport {SimpleBinaryKernelImplCPU, SimpleUnaryKernelImplCPU} from './shared';\n\ntype UnaryKernelFuncConfig = {\n  opType: UnaryOpType,\n  cpuKernelImpl?: SimpleUnaryKernelImplCPU,\n  dtype?: DataType\n};\n\n/**\n * Template that creates a `KernelFunc` for unary ops.\n * @param opType Op type to create `UnaryOpProgram`.\n * @param cpuKernelImpl Optional. Shared functionality from tfjs-backend-cpu, it\n *     will be involved when necessary.\n * @param dtype Optional. If set, the result has this dtype. Otherwise, the\n *     result has the same dtype as the first input. This is mainly used in\n *     comparison kernels, such as Equal, Less, Greater, etc.\n */\nexport function unaryKernelFunc(\n    {opType, cpuKernelImpl, dtype}: UnaryKernelFuncConfig): KernelFunc {\n  return ({inputs, backend}) => {\n    const {x} = inputs as UnaryInputs;\n    const webgpuBackend = backend as WebGPUBackend;\n\n    const $dtype = dtype || x.dtype;\n    if (webgpuBackend.shouldExecuteOnCPU([x]) && cpuKernelImpl != null) {\n      const xData = webgpuBackend.tensorMap.get(x.dataId);\n      const outValues = cpuKernelImpl(xData.values as TypedArray, $dtype);\n      return webgpuBackend.makeTensorInfo(x.shape, $dtype, outValues);\n    }\n\n    const program: UnaryOpProgram = new UnaryOpProgram(x.shape, opType);\n    return webgpuBackend.runWebGPUProgram(program, [x], $dtype);\n  };\n}\n\ntype BinaryKernelFuncConfig = {\n  opType: BinaryOpType,\n  cpuKernelImpl?: SimpleBinaryKernelImplCPU,\n  supportsComplex?: boolean,\n  dtype?: DataType\n};\n\n/**\n * Template that creates a `KernelFunc` for binary ops.\n * @param opType Op type to create `BinaryOpProgram`.\n * @param cpuKernelImpl Optional. Shared functionality from tfjs-backend-cpu, it\n *     will be involved when necessary.\n * @param dtype Optional. If set, the result has this dtype. Otherwise, the\n *     result has the same dtype as the first input. This is mainly used in\n *     comparison kernels, such as Equal, Less, Greater, etc.\n */\nexport function binaryKernelFunc(\n    {opType, cpuKernelImpl, supportsComplex = false, dtype}:\n        BinaryKernelFuncConfig): KernelFunc {\n  return ({inputs, backend}) => {\n    const {a, b} = inputs as BinaryInputs;\n    const webgpuBackend = backend as WebGPUBackend;\n\n    if (supportsComplex && a.dtype === 'complex64') {\n      const aData = webgpuBackend.tensorMap.get(a.dataId);\n      const bData = webgpuBackend.tensorMap.get(b.dataId);\n      let real: TensorInfo, imag: TensorInfo;\n      if (opType !== BinaryOpType.MUL) {\n        [real, imag] = [\n          [aData.complexTensorInfos.real, bData.complexTensorInfos.real],\n          [aData.complexTensorInfos.imag, bData.complexTensorInfos.imag]\n        ].map(complexParts => {\n          const [aPart, bPart] = complexParts;\n\n          const aHandle = {\n            dataId: aPart.dataId,\n            dtype: aPart.dtype,\n            shape: a.shape\n          };\n          const bHandle = {\n            dataId: bPart.dataId,\n            dtype: bPart.dtype,\n            shape: b.shape\n          };\n\n          const program = new BinaryOpProgram(opType, a.shape, b.shape);\n          return webgpuBackend.runWebGPUProgram(\n              program, [aHandle, bHandle],\n              upcastType(aPart.dtype, bPart.dtype));\n        });\n      } else {\n        const realProgram = new BinaryOpComplexProgram(\n            BinaryOpType.COMPLEX_MULTIPLY_REAL, a.shape, b.shape);\n        const imagProgram = new BinaryOpComplexProgram(\n            BinaryOpType.COMPLEX_MULTIPLY_IMAG, a.shape, b.shape);\n\n        const inputs = [\n          {\n            dataId: aData.complexTensorInfos.real.dataId,\n            dtype: aData.complexTensorInfos.real.dtype,\n            shape: a.shape\n          },\n          {\n            dataId: aData.complexTensorInfos.imag.dataId,\n            dtype: aData.complexTensorInfos.imag.dtype,\n            shape: a.shape\n          },\n          {\n            dataId: bData.complexTensorInfos.real.dataId,\n            dtype: bData.complexTensorInfos.real.dtype,\n            shape: b.shape\n          },\n          {\n            dataId: bData.complexTensorInfos.imag.dataId,\n            dtype: bData.complexTensorInfos.imag.dtype,\n            shape: b.shape\n          }\n        ];\n\n        real = webgpuBackend.runWebGPUProgram(realProgram, inputs, 'float32');\n        imag = webgpuBackend.runWebGPUProgram(imagProgram, inputs, 'float32');\n      }\n\n      const complexOutput =\n          complex({inputs: {real, imag}, backend: webgpuBackend});\n\n      webgpuBackend.disposeData(real.dataId);\n      webgpuBackend.disposeData(imag.dataId);\n\n      // TODO: Implement CPU forwarding for complex inputs.\n\n      return complexOutput;\n    }\n\n    const $dtype = dtype || upcastType(a.dtype, b.dtype);\n    if ((a.dtype === 'string' || b.dtype === 'string' ||\n         webgpuBackend.shouldExecuteOnCPU([a, b])) &&\n        cpuKernelImpl != null) {\n      const aData = webgpuBackend.tensorMap.get(a.dataId).values as TypedArray;\n      const bData = webgpuBackend.tensorMap.get(b.dataId).values as TypedArray;\n      const decodedAVals = a.dtype === 'string' ?\n          // tslint:disable-next-line: no-any\n          backend_util.fromUint8ToStringArray(aData as any as Uint8Array[]) :\n          aData;\n      const decodedBVals = a.dtype === 'string' ?\n          // tslint:disable-next-line: no-any\n          backend_util.fromUint8ToStringArray(bData as any as Uint8Array[]) :\n          bData;\n      const [outValues, outShape] =\n          cpuKernelImpl(a.shape, b.shape, decodedAVals, decodedBVals, $dtype);\n\n      return webgpuBackend.makeTensorInfo(outShape, $dtype, outValues);\n    }\n    const program = new BinaryOpProgram(opType, a.shape, b.shape);\n    return webgpuBackend.runWebGPUProgram(program, [a, b], $dtype);\n  };\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DataType, DataValues, NumericDataType, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {SimpleBinaryKernelImpl, SimpleBinaryOperation} from './binary_types';\n\n/**\n * Template that creates implementation for binary ops. Supports broadcast.\n */\nexport function createSimpleBinaryKernelImpl(op: SimpleBinaryOperation):\n    SimpleBinaryKernelImpl {\n  return (aShape: number[], bShape: number[], aVals: DataValues,\n          bVals: DataValues, dtype: DataType): [TypedArray, number[]] => {\n    const newShape = backend_util.assertAndGetBroadcastShape(aShape, bShape);\n\n    const resultRank = newShape.length;\n    const resultStrides = util.computeStrides(newShape);\n    const resultSize = util.sizeFromShape(newShape);\n\n    const result =\n        util.getTypedArrayFromDType(dtype as NumericDataType, resultSize);\n\n    const aRank = aShape.length;\n    const bRank = bShape.length;\n\n    const aStrides = util.computeStrides(aShape);\n    const bStrides = util.computeStrides(bShape);\n\n    const aBroadcastDims = backend_util.getBroadcastDims(aShape, newShape);\n    const bBroadcastDims = backend_util.getBroadcastDims(bShape, newShape);\n\n    if (aBroadcastDims.length + bBroadcastDims.length === 0) {\n      for (let i = 0; i < result.length; ++i) {\n        result[i] = op(aVals[i % aVals.length], bVals[i % bVals.length]);\n      }\n    } else {\n      for (let i = 0; i < result.length; ++i) {\n        const loc = util.indexToLoc(i, resultRank, resultStrides);\n\n        const aLoc = loc.slice(-aRank);\n        aBroadcastDims.forEach(d => aLoc[d] = 0);\n        const aIndex = util.locToIndex(aLoc, aRank, aStrides);\n\n        const bLoc = loc.slice(-bRank);\n        bBroadcastDims.forEach(d => bLoc[d] = 0);\n        const bIndex = util.locToIndex(bLoc, bRank, bStrides);\n\n        result[i] = op(aVals[aIndex], bVals[bIndex]);\n      }\n    }\n\n    return [result, newShape];\n  };\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Add, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc, createComplexBinaryKernelImpl} from '../utils/binary_utils';\n\nexport const addImpl =\n    createSimpleBinaryKernelImpl(((a: number, b: number) => a + b));\nexport const addComplexImpl =\n    createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {\n      return {real: aReal + bReal, imag: aImag + bImag};\n    }));\n\nexport const add = binaryKernelFunc(Add, addImpl, addComplexImpl);\n\nexport const addConfig: KernelConfig = {\n  kernelName: Add,\n  backendName: 'cpu',\n  kernelFunc: add\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {NumericDataType, util} from '@tensorflow/tfjs-core';\n\nimport {SimpleUnaryImpl, SimpleUnaryOperation} from './unary_types';\n\n/**\n * Template that creates implementation for unary op.\n */\nexport function createSimpleUnaryImpl(op: SimpleUnaryOperation):\n    SimpleUnaryImpl {\n  return (values, dtype, attrs) => {\n    const newValues =\n        util.getTypedArrayFromDType(dtype as NumericDataType, values.length);\n    for (let i = 0; i < values.length; ++i) {\n      newValues[i] = op(values[i], attrs);\n    }\n    return newValues;\n  };\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Ceil, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const ceilImpl = createSimpleUnaryImpl((xi) => Math.ceil(xi));\nexport const ceil = unaryKernelFuncFromImpl(Ceil, ceilImpl);\n\nexport const ceilConfig: KernelConfig = {\n  kernelName: Ceil,\n  backendName: 'cpu',\n  kernelFunc: ceil,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Equal, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const equalImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => (a === b) ? 1 : 0);\nexport const equal =\n    binaryKernelFunc(Equal, equalImpl, null /* complexImpl */, 'bool');\n\nexport const equalConfig: KernelConfig = {\n  kernelName: Equal,\n  backendName: 'cpu',\n  kernelFunc: equal\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Exp, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const expImpl = createSimpleUnaryImpl((xi) => Math.exp(xi));\nexport const exp = unaryKernelFuncFromImpl(Exp, expImpl, 'float32');\n\nexport const expConfig: KernelConfig = {\n  kernelName: Exp,\n  backendName: 'cpu',\n  kernelFunc: exp,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Expm1, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const expm1Impl = createSimpleUnaryImpl((xi) => Math.expm1(xi));\nexport const expm1 = unaryKernelFuncFromImpl(Expm1, expm1Impl);\n\nexport const expm1Config: KernelConfig = {\n  kernelName: Expm1,\n  backendName: 'cpu',\n  kernelFunc: expm1,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Floor, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const floorImpl = createSimpleUnaryImpl((xi) => Math.floor(xi));\nexport const floor = unaryKernelFuncFromImpl(Floor, floorImpl);\n\nexport const floorConfig: KernelConfig = {\n  kernelName: Floor,\n  backendName: 'cpu',\n  kernelFunc: floor,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Greater, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const greaterImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => (a > b) ? 1 : 0);\nexport const greater =\n    binaryKernelFunc(Greater, greaterImpl, null /* complexImpl */, 'bool');\n\nexport const greaterConfig: KernelConfig = {\n  kernelName: Greater,\n  backendName: 'cpu',\n  kernelFunc: greater\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {GreaterEqual, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const greaterEqualImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => (a >= b) ? 1 : 0);\nexport const greaterEqual = binaryKernelFunc(\n    GreaterEqual, greaterEqualImpl, null /* complexImpl */, 'bool');\n\nexport const greaterEqualConfig: KernelConfig = {\n  kernelName: GreaterEqual,\n  backendName: 'cpu',\n  kernelFunc: greaterEqual\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Less} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const lessImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => (a < b) ? 1 : 0);\nexport const less =\n    binaryKernelFunc(Less, lessImpl, null /* complexImpl */, 'bool');\n\nexport const lessConfig: KernelConfig = {\n  kernelName: Less,\n  backendName: 'cpu',\n  kernelFunc: less\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LessEqual} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const lessEqualImpl =\n    createSimpleBinaryKernelImpl((a: number, b: number) => (a <= b) ? 1 : 0);\nexport const lessEqual =\n    binaryKernelFunc(LessEqual, lessEqualImpl, null /* complexImpl */, 'bool');\n\nexport const lessEqualConfig: KernelConfig = {\n  kernelName: LessEqual,\n  backendName: 'cpu',\n  kernelFunc: lessEqual\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Log} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const logImpl = createSimpleUnaryImpl((xi) => Math.log(xi));\nexport const log = unaryKernelFuncFromImpl(Log, logImpl);\n\nexport const logConfig: KernelConfig = {\n  kernelName: Log,\n  backendName: 'cpu',\n  kernelFunc: log,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Maximum} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const maximumImpl = createSimpleBinaryKernelImpl(\n    ((aValue, bValue) => Math.max(aValue as number, bValue as number)));\nexport const maximum = binaryKernelFunc(Maximum, maximumImpl);\n\nexport const maximumConfig: KernelConfig = {\n  kernelName: Maximum,\n  backendName: 'cpu',\n  kernelFunc: maximum\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Minimum} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const minimumImpl = createSimpleBinaryKernelImpl(\n    ((aValue, bValue) => Math.min(aValue as number, bValue as number)));\nexport const minimum = binaryKernelFunc(Minimum, minimumImpl);\n\nexport const minimumConfig: KernelConfig = {\n  kernelName: Minimum,\n  backendName: 'cpu',\n  kernelFunc: minimum\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Multiply} from '@tensorflow/tfjs-core';\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc, createComplexBinaryKernelImpl} from '../utils/binary_utils';\n\nexport const multiplyImpl = createSimpleBinaryKernelImpl(\n    ((aValue: number, bValue: number) => aValue * bValue));\nexport const multiplyComplexImpl =\n    createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {\n      return {\n        real: aReal * bReal - aImag * bImag,\n        imag: aReal * bImag + aImag * bReal\n      };\n    }));\n\nexport const multiply =\n    binaryKernelFunc(Multiply, multiplyImpl, multiplyComplexImpl);\n\nexport const multiplyConfig: KernelConfig = {\n  kernelName: Multiply,\n  backendName: 'cpu',\n  kernelFunc: multiply\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, NotEqual} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc} from '../utils/binary_utils';\n\nexport const notEqualImpl =\n    createSimpleBinaryKernelImpl(((a, b) => (a !== b) ? 1 : 0));\nexport const notEqual =\n    binaryKernelFunc(NotEqual, notEqualImpl, null /* complexOp */, 'bool');\n\nexport const notEqualConfig: KernelConfig = {\n  kernelName: NotEqual,\n  backendName: 'cpu',\n  kernelFunc: notEqual\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, broadcastTo, DataType, reshape, tidy, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport RowPartitionType = backend_util.RowPartitionType;\n// Based on\n// https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc\nclass RaggedTensorToTensorOp {\n  private readonly rowPartitionTypes: RowPartitionType[];\n  private readonly raggedRank: number;\n  constructor(\n      private shape: TypedArray, private shapeShape: number[],\n      private values: TypedArray, private valuesShape: number[],\n      private valuesDType: DataType, private defaultValue: TypedArray,\n      private defaultValueShape: number[],\n      private readonly rowPartitionValues: TypedArray[],\n      private readonly rowPartitionValuesShapes: number[][],\n      rowPartitionTypeStrings: string[]) {\n    this.rowPartitionTypes =\n        backend_util.getRowPartitionTypesHelper(rowPartitionTypeStrings);\n    this.raggedRank = backend_util.getRaggedRank(this.rowPartitionTypes);\n  }\n\n  private getRowPartitionTypeByDimension(dimension: number) {\n    if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {\n      return this.rowPartitionTypes[dimension + 1];\n    } else {\n      return this.rowPartitionTypes[dimension];\n    }\n  }\n\n  // Returns the relationship between dimension and dimension + 1.\n  private getRowPartitionTensor(dimension: number) {\n    if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {\n      return this.rowPartitionValues[dimension + 1];\n    } else {\n      return this.rowPartitionValues[dimension];\n    }\n  }\n\n  private getMaxWidth(dimension: number) {\n    const rowPartitionTensor = this.getRowPartitionTensor(dimension - 1);\n    switch (this.getRowPartitionTypeByDimension(dimension - 1)) {\n      case RowPartitionType.VALUE_ROWIDS:\n        return RaggedTensorToTensorOp.getMaxWidthValueRowID(rowPartitionTensor);\n      case RowPartitionType.ROW_SPLITS:\n        return RaggedTensorToTensorOp.getMaxWidthRowSplit(rowPartitionTensor);\n      default:\n        throw new Error(`Cannot handle partition type ${\n            RowPartitionType[this.getRowPartitionTypeByDimension(\n                dimension - 1)]}`);\n    }\n  }\n\n  static getMaxWidthRowSplit(rowSplit: TypedArray) {\n    const tensorLength = rowSplit.length;\n    if (tensorLength === 0 || tensorLength === 1) {\n      return 0;\n    }\n    let maxWidth = 0;\n    for (let i = 0; i < tensorLength - 1; ++i) {\n      const currentWidth = rowSplit[i + 1] - rowSplit[i];\n      if (currentWidth > maxWidth) {\n        maxWidth = currentWidth;\n      }\n    }\n    return maxWidth;\n  }\n\n  static getMaxWidthValueRowID(valueRowIds: TypedArray) {\n    const indexLength = valueRowIds.length;\n    if (indexLength === 0) {\n      return 0;\n    }\n    let firstEqualIndex = 0;\n    let firstEqualIndexValue = valueRowIds[0];\n    let maxWidth = 0;\n    for (let i = 1; i < indexLength; ++i) {\n      const value = valueRowIds[i];\n      if (value !== firstEqualIndexValue) {\n        firstEqualIndexValue = value;\n        maxWidth = Math.max(i - firstEqualIndex, maxWidth);\n        firstEqualIndex = i;\n      }\n    }\n    return Math.max(indexLength - firstEqualIndex, maxWidth);\n  }\n\n  private tensorShapeFromTensor(\n      t: TypedArray, tShape: number[], isPartial = true) {\n    if (tShape.length === 0) {\n      if (t[0] === -1) {\n        return [];\n      }\n      throw new Error(\n          `The only valid scalar shape tensor is the fully unknown shape specified as -1.`);\n    }\n    // MakePartialShape/MakeShapeHelper.\n    return makeShape(t, isPartial);\n  }\n\n  private calculateOutputSize(firstDim: number) {\n    const valueShape = this.valuesShape;\n    const defaultValueShape = this.defaultValueShape;\n\n    backend_util.validateDefaultValueShape(defaultValueShape, valueShape);\n\n    const shape = this.tensorShapeFromTensor(this.shape, this.shapeShape);\n    const outputShape = backend_util.combineRaggedTensorToTensorShapes(\n        this.raggedRank, shape, valueShape);\n\n    const result = outputShape;\n\n    if (result[0] < 0) {\n      result[0] = firstDim;\n    }\n    for (let i = 1; i <= this.raggedRank; ++i) {\n      if (result[i] < 0) {\n        result[i] = this.getMaxWidth(i);\n      }\n    }\n\n    return result;\n  }\n\n  /**\n   * The outputIndex represents the index in the output tensor\n   * where the first element of a particular dimension would be written.\n   * If it is -1, it indicates that the index is out of scope.\n   * Example, given firstDimension = 10, firstDimensionOutput = 6,\n   * and outputIndexMultiplier = 100:\n   * result = [0 100 200 300 400 500 -1 -1 -1 -1]\n   * If firstDimensionOutput = 11 instead, then:\n   * result = [0 100 200 300 400 500 600 700 800 900]\n   */\n  private calculateFirstParentOutputIndex(\n      firstDimension: number, outputIndexMultiplier: number,\n      firstDimensionOutput: number) {\n    const minDimension = Math.min(firstDimension, firstDimensionOutput);\n    const result: number[] = [];\n    let currentOutputIndex = 0;\n    for (let i = 0; i < minDimension;\n         ++i, currentOutputIndex += outputIndexMultiplier) {\n      result.push(currentOutputIndex);\n    }\n    for (let i = minDimension; i < firstDimension; ++i) {\n      result.push(-1);\n    }\n    util.assert(\n        result.length === firstDimension,\n        () => 'Final length of result must be equal to firstDimension.');\n\n    return result;\n  }\n\n  private calculateOutputIndexRowSplit(\n      rowSplit: TypedArray, parentOutputIndex: number[],\n      outputIndexMultiplier: number, outputSize: number) {\n    const rowSplitSize = rowSplit.length;\n    const result: number[] = [];\n    for (let i = 0; i < rowSplitSize - 1; ++i) {\n      const rowLength = rowSplit[i + 1] - rowSplit[i];\n      let realLength = Math.min(outputSize, rowLength);\n      let parentOutputIndexCurrent = parentOutputIndex[i];\n\n      if (parentOutputIndexCurrent === -1) {\n        realLength = 0;\n      }\n      for (let j = 0; j < realLength; ++j) {\n        result.push(parentOutputIndexCurrent);\n        parentOutputIndexCurrent += outputIndexMultiplier;\n      }\n      for (let j = 0; j < rowLength - realLength; ++j) {\n        result.push(-1);\n      }\n    }\n    if (rowSplitSize > 0 && result.length !== rowSplit[rowSplitSize - 1]) {\n      throw new Error('Invalid row split size.');\n    }\n\n    return result;\n  }\n\n  // Calculate the output index of the first element of a list.\n  // The parentOutputIndex is the same computation for the previous list.\n  // -1 indicates an element or list that is out of range.\n  // The outputIndexMultiplier is the number of output indices one moves\n  // forward for each column.\n  // E.g., given:\n  // valueRowIds:[0 1 2 2 2 3 5 5 6]\n  // parentOutputIndex:[1000 1100 2000 2100 -1 3000 4000]\n  // outputIndexMultiplier: 10\n  // outputSize: 2\n  // You get:\n  // result = [1000 1100 2000 2010 -1 2100 -1 -1 3000]\n  // result[0] = parentOutputIndex[valueRowIds[0]]\n  // result[1] = parentOutputIndex[valueRowIds[1]]\n  // result[2] = parentOutputIndex[valueRowIds[2]]\n  // result[3] = parentOutputIndex[valueRowIds[2] + 10]\n  // result[4] = -1 because it is the third element the size is 2.\n  // result[5] = parentOutputIndex[valueRowIds[3]]\n  // result[6] = -1 because parentOutputIndex[valueRowIds[6]] == -1\n  // result[7] = -1 because parentOutputIndex[valueRowIds[6]] == -1\n  // result[8] = parentOutputIndex[valueRowIds[7]]\n  private calculateOutputIndexValueRowID(\n      valueRowIds: TypedArray, parentOutputIndex: number[],\n      outputIndexMultiplier: number, outputSize: number) {\n    const indexSize = valueRowIds.length;\n    const result: number[] = [];\n    if (indexSize === 0) {\n      return [];\n    }\n\n    let currentOutputColumn = 0;\n    let currentValueRowId = valueRowIds[0];\n\n    if (currentValueRowId >= parentOutputIndex.length) {\n      throw new Error(\n          `Got currentValueRowId=${currentValueRowId}, which is not less than ${\n              parentOutputIndex.length}`);\n    }\n\n    let currentOutputIndex = parentOutputIndex[currentValueRowId];\n    result.push(currentOutputIndex);\n    for (let i = 1; i < indexSize; ++i) {\n      const nextValueRowId = valueRowIds[i];\n      if (nextValueRowId === currentValueRowId) {\n        if (currentOutputIndex >= 0) {\n          ++currentOutputColumn;\n          if (currentOutputColumn < outputSize) {\n            currentOutputIndex += outputIndexMultiplier;\n          } else {\n            currentOutputIndex = -1;\n          }\n        }\n      } else {\n        currentOutputColumn = 0;\n        currentValueRowId = nextValueRowId;\n\n        if (nextValueRowId >= parentOutputIndex.length) {\n          throw new Error(\n              `Got nextValueRowId=${nextValueRowId} which is not less than ${\n                  parentOutputIndex.length}`);\n        }\n\n        currentOutputIndex = parentOutputIndex[nextValueRowId];\n      }\n      result.push(currentOutputIndex);\n    }\n\n    if (result.length !== valueRowIds.length) {\n      throw new Error('Invalid row ids.');\n    }\n\n    return result;\n  }\n\n  private calculateOutputIndex(\n      dimension: number, parentOutputIndex: number[],\n      outputIndexMultiplier: number, outputSize: number) {\n    const rowPartitionTensor = this.getRowPartitionTensor(dimension);\n    const partitionType = this.getRowPartitionTypeByDimension(dimension);\n    switch (partitionType) {\n      case RowPartitionType.VALUE_ROWIDS:\n        return this.calculateOutputIndexValueRowID(\n            rowPartitionTensor, parentOutputIndex, outputIndexMultiplier,\n            outputSize);\n      case RowPartitionType.ROW_SPLITS:\n        if (rowPartitionTensor.length - 1 > parentOutputIndex.length) {\n          throw new Error(`Row partition size is greater than output size: ${\n              rowPartitionTensor.length - 1} > ${parentOutputIndex.length}`);\n        }\n        return this.calculateOutputIndexRowSplit(\n            rowPartitionTensor, parentOutputIndex, outputIndexMultiplier,\n            outputSize);\n      default:\n        throw new Error(\n            `Unsupported partition type: ${RowPartitionType[partitionType]}`);\n    }\n  }\n\n  private getFirstDimensionSize() {\n    const firstPartitionTensor = this.rowPartitionValues[0];\n    if (this.rowPartitionTypes.length === 0) {\n      throw new Error('No row_partition_types given.');\n    }\n    const firstPartitionType = this.rowPartitionTypes[0];\n    switch (firstPartitionType) {\n      case RowPartitionType.FIRST_DIM_SIZE:\n        return firstPartitionTensor[0];\n      case RowPartitionType.VALUE_ROWIDS:\n        throw new Error('Cannot handle VALUE_ROWIDS in first dimension.');\n      case RowPartitionType.ROW_SPLITS:\n        return this.rowPartitionValuesShapes[0][0] - 1;\n      default:\n        throw new Error(\n            `Cannot handle type ${RowPartitionType[firstPartitionType]}`);\n    }\n  }\n\n  compute(): [number[], TypedArray] {\n    const firstPartitionTensor = this.rowPartitionValues[0];\n    if (firstPartitionTensor.length <= 0) {\n      throw new Error(\n          'Invalid first partition input. ' +\n          'Tensor requires at least one element.');\n    }\n    const firstDimension = this.getFirstDimensionSize();\n    const outputSize = this.calculateOutputSize(firstDimension);\n    const multiplier: number[] = new Array(this.raggedRank + 1);\n\n    multiplier[multiplier.length - 1] = 1;\n    for (let i = multiplier.length - 2; i >= 0; --i) {\n      multiplier[i] = multiplier[i + 1] * outputSize[i + 1];\n    }\n    // Full size of the tensor.\n    const outputShape: number[] = makeShape(outputSize, false);\n    const outputTensor =\n        util.getArrayFromDType(\n            this.valuesDType, util.sizeFromShape(outputShape)) as TypedArray;\n\n    const fullSize = multiplier[0] * outputSize[0];\n    if (fullSize > 0) {\n      let outputIndex = this.calculateFirstParentOutputIndex(\n          firstDimension, multiplier[0], outputSize[0]);\n      for (let i = 1; i <= this.raggedRank; ++i) {\n        const newOutputIndex = this.calculateOutputIndex(\n            i - 1, outputIndex, multiplier[i], outputSize[i]);\n        outputIndex = newOutputIndex;\n      }\n\n      this.setOutput(this.raggedRank, outputIndex, outputTensor, outputShape);\n    }\n\n    return [outputShape, outputTensor];\n  }\n  setOutput(\n      raggedRank: number, outputIndex: number[], outputTensor: TypedArray,\n      outputShape: number[]) {\n    if (outputTensor.length === 0) {\n      return;\n    }\n\n    const valuesBase = this.values;\n    const outputBase = outputTensor;\n\n    let elementShape = outputShape.slice();\n    elementShape = elementShape.slice(raggedRank + 1);\n    const valueElementSize = util.sizeFromShape(elementShape);\n    const outputIndexSize = outputIndex.length;\n\n    // Broadcast the default value to value_element_size.  (We can skip this\n    // if defaultValueTensor.size == 1, since we use fill when that's true.)\n    let defaultValue = this.defaultValue;\n    if (defaultValue.length !== valueElementSize && defaultValue.length !== 1) {\n      const srcShape = this.defaultValueShape;\n      tidy(() => {\n        const defaultValueTensor = reshape(defaultValue, srcShape);\n        const bCastDefault = broadcastTo(defaultValueTensor, elementShape);\n        defaultValue = bCastDefault.dataSync();\n      });\n    }\n\n    // Loop through the outputIndex array, finding contiguous regions that\n    // should be copied.  Once we find the end of a contiguous region, copy it\n    // and add any necessary padding (with defaultValue).\n    let srcStart = 0;  // Start of contiguous region (in values)\n    let dstStart = 0;  // Destination for contiguous region (in output)\n    let dstEnd = 0;    // Destination for contiguous region (in output)\n    for (let srcI = 0; srcI <= outputIndexSize; ++srcI) {\n      // dstI is the destination where the value at srcI should be copied.\n      let dstI = srcI < outputIndexSize ? outputIndex[srcI] : -1;\n\n      // If we're still in a contiguous region, then update dstEnd go to the\n      // next srcI.\n      if (dstI === dstEnd) {\n        ++dstEnd;\n        continue;\n      }\n\n      // We found the end of contiguous region.  This can be because we found\n      // a gap (dstI > dstEnd), or a source value that shouldn't be copied\n      // because it's out-of-bounds (dstI == -1), or the end of the tensor\n      // (dstI === -1).\n      if (dstStart < dstEnd) {\n        // Copy the contiguous region.\n        const src = valuesBase.subarray(srcStart * valueElementSize);\n        const dst = outputBase.subarray(dstStart * valueElementSize);\n        const nVals = (dstEnd - dstStart) * valueElementSize;\n        copyArray(dst, src, nVals);\n      }\n\n      // Add any necessary padding (w/ defaultValue).\n      if (srcI >= outputIndexSize) {\n        // We reached the end of values: pad to the end of output.\n        const outputSize = outputTensor.length;\n        dstI = Math.floor(outputSize / valueElementSize);\n      }\n      if (dstI > dstEnd) {\n        if (this.defaultValue.length === 1) {\n          outputBase\n              .subarray(dstEnd * valueElementSize, dstI * valueElementSize)\n              .fill(this.defaultValue[0]);\n          dstEnd = dstI;\n        } else {\n          while (dstI > dstEnd) {\n            const dst = outputBase.slice(dstEnd * valueElementSize);\n            copyArray(dst, defaultValue, valueElementSize);\n            ++dstEnd;\n          }\n        }\n      }\n\n      // Update indices.\n      if (dstI < 0) {\n        // srcI should be skipped -- leave it out of the contiguous region.\n        srcStart = srcI + 1;\n        dstStart = dstEnd;\n      } else {\n        // srcI should be copied -- include it in the contiguous region.\n        srcStart = srcI;\n        dstStart = dstEnd;\n        dstEnd = dstStart + 1;\n      }\n    }\n  }\n}\n\nfunction copyArray(dst: TypedArray, src: TypedArray, size: number) {\n  for (let i = 0; i < size; i++) {\n    dst[i] = src[i];\n  }\n}\n\nfunction makeShape(shape: number[]|TypedArray, isPartial: boolean) {\n  const out: number[] = [];\n  for (let dim of shape) {\n    if (dim < 0) {\n      if (!isPartial) {\n        throw new Error(`Dimension ${dim} must be >= 0`);\n      }\n      if (dim < -1) {\n        throw new Error(`Dimension ${dim} must be >= -1`);\n      }\n      dim = -1;\n    }\n    out.push(dim);\n  }\n\n  return out;\n}\n\nexport function raggedTensorToTensorImpl(\n    shape: TypedArray, shapesShape: number[], values: TypedArray,\n    valuesShape: number[], valuesDType: DataType, defaultValue: TypedArray,\n    defaultValueShape: number[], rowPartitionValues: TypedArray[],\n    rowPartitionValuesShapes: number[][],\n    rowPartitionTypes: string[]): [number[], TypedArray] {\n  return new RaggedTensorToTensorOp(\n             shape, shapesShape, values, valuesShape, valuesDType, defaultValue,\n             defaultValueShape, rowPartitionValues, rowPartitionValuesShapes,\n             rowPartitionTypes)\n      .compute();\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Rsqrt} from '@tensorflow/tfjs-core';\n\nimport {createSimpleUnaryImpl} from '../utils/unary_impl';\nimport {unaryKernelFuncFromImpl} from '../utils/unary_utils';\n\nexport const rsqrtImpl = createSimpleUnaryImpl((xi) => 1 / Math.sqrt(xi));\nexport const rsqrt = unaryKernelFuncFromImpl(Rsqrt, rsqrtImpl);\n\nexport const rsqrtConfig: KernelConfig = {\n  kernelName: Rsqrt,\n  backendName: 'cpu',\n  kernelFunc: rsqrt,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {util} from '@tensorflow/tfjs-core';\n\n/**\n * The StringNGramsOp class creates ngrams from ragged string data.\n * The constructor contains all attributes related to the operation such as\n * padding widths and strings, and the compute function can be used to\n * compute the ngrams for different ragged tensor inputs.\n */\nclass StringNGramsOp {\n  private separator: Uint8Array;\n  private nGramWidths: number[];\n  private padWidth: number;\n  private leftPad: Uint8Array;\n  private rightPad: Uint8Array;\n  private preserveShort: boolean;\n\n  constructor(\n      separator: string, nGramWidths: number[], leftPad: string,\n      rightPad: string, padWidth: number, preserveShortSequences: boolean) {\n    this.separator = util.encodeString(separator);\n    this.nGramWidths = nGramWidths;\n    this.leftPad = util.encodeString(leftPad);\n    this.rightPad = util.encodeString(rightPad);\n    this.padWidth = padWidth;\n    this.preserveShort = preserveShortSequences;\n  }\n\n  private getPadWidth(nGramWidth: number) {\n    // Ngrams can be padded with either a fixed pad width or a dynamic pad\n    // width depending on the 'padWidth' arg, but in no case should the padding\n    // ever be wider than 'nGramWidth' - 1.\n    return Math.min(\n        this.padWidth < 0 ? nGramWidth - 1 : this.padWidth, nGramWidth - 1);\n  }\n\n  private getNumNGrams(length: number, nGramWidth: number) {\n    const padWidth = this.getPadWidth(nGramWidth);\n    return Math.max(0, ((length + 2 * padWidth) - nGramWidth) + 1);\n  }\n\n  private createNGrams(\n      data: Uint8Array[], splitIndex: number, output: Uint8Array[],\n      outputStartIndex: number, numNGrams: number, nGramWidth: number) {\n    for (let nGramIndex = 0; nGramIndex < numNGrams; ++nGramIndex) {\n      const padWidth = this.getPadWidth(nGramWidth);\n      const leftPadding = Math.max(0, padWidth - nGramIndex);\n      const rightPadding =\n          Math.max(0, padWidth - (numNGrams - (nGramIndex + 1)));\n      const numTokens = nGramWidth - (leftPadding + rightPadding);\n      const dataStartIndex =\n          splitIndex + (leftPadding > 0 ? 0 : nGramIndex - padWidth);\n\n      // Calculate the total expected size of the nGram so we can reserve the\n      // correct amount of space in the string.\n      let nGramSize = 0;\n      // Size of the left padding.\n      nGramSize += leftPadding * this.leftPad.length;\n      // Size of the tokens.\n      for (let n = 0; n < numTokens; ++n) {\n        nGramSize += data[dataStartIndex + n].length;\n      }\n      // Size of the right padding.\n      nGramSize += rightPadding * this.rightPad.length;\n      // Size of the separators.\n      const numSeparators = leftPadding + rightPadding + numTokens - 1;\n      nGramSize += numSeparators * this.separator.length;\n\n      // Build the nGram.\n      output[outputStartIndex + nGramIndex] = new Uint8Array(nGramSize);\n      const nGram = output[outputStartIndex + nGramIndex];\n\n      let nextNGramIndex = 0;\n      const appendToNGram = (str: Uint8Array) =>\n          str.forEach((value) => nGram[nextNGramIndex++] = value);\n\n      for (let n = 0; n < leftPadding; ++n) {\n        appendToNGram(this.leftPad);\n        appendToNGram(this.separator);\n      }\n      // Only output first numTokens - 1 pairs of data and separator\n      for (let n = 0; n < numTokens - 1; ++n) {\n        appendToNGram(data[dataStartIndex + n]);\n        appendToNGram(this.separator);\n      }\n      // Handle case when there are no tokens or no right padding as these\n      // can result in consecutive separators.\n      if (numTokens > 0) {\n        // If we have tokens, then output last and then pair each separator\n        // with the right padding that follows, to ensure nGram ends either with\n        // the token or with the right pad.\n        appendToNGram(data[dataStartIndex + numTokens - 1]);\n        for (let n = 0; n < rightPadding; ++n) {\n          appendToNGram(this.separator);\n          appendToNGram(this.rightPad);\n        }\n      } else {\n        // If we don't have tokens, then the last item inserted into the nGram\n        // has been the separator from the left padding loop above. Hence,\n        // output right pad and separator and make sure to finish with a\n        // padding, not a separator.\n        for (let n = 0; n < rightPadding - 1; ++n) {\n          appendToNGram(this.rightPad);\n          appendToNGram(this.separator);\n        }\n        appendToNGram(this.rightPad);\n      }\n    }\n  }\n\n  // Data and splits together form the definition of the ragged tensor,\n  // where data is 1 dimensional and contains the values of the tensor\n  // and splits denotes the indices at which each row starts.\n  public compute(data: Uint8Array[], splits: Int32Array):\n      [Uint8Array[], Int32Array] {\n    // Validate that the splits are valid indices into data, only if there are\n    // splits specified.\n    const inputDataSize = data.length;\n    const splitsSize = splits.length;\n    if (splitsSize > 0) {\n      let prevSplit = splits[0];\n      if (prevSplit !== 0) {\n        throw new Error(`First split value must be 0, got ${prevSplit}`);\n      }\n      for (let i = 1; i < splitsSize; ++i) {\n        let validSplits = splits[i] >= prevSplit;\n        validSplits = validSplits && (splits[i] <= inputDataSize);\n        if (!validSplits) {\n          throw new Error(`Invalid split value ${splits[i]}, must be in [${\n              prevSplit}, ${inputDataSize}]`);\n        }\n        prevSplit = splits[i];\n      }\n      if (prevSplit !== inputDataSize) {\n        throw new Error(`Last split value must be data size. Expected ${\n            inputDataSize}, got ${prevSplit}`);\n      }\n    }\n\n    const numBatchItems = splitsSize - 1;\n    const nGramsSplits = util.getArrayFromDType('int32', splitsSize);\n    // If there is no data or size, return an empty ragged tensor.\n    if (inputDataSize === 0 || splitsSize === 0) {\n      const empty: Uint8Array[] = new Array(inputDataSize);\n      for (let i = 0; i <= numBatchItems; ++i) {\n        nGramsSplits[i] = 0;\n      }\n      return [empty, nGramsSplits];\n    }\n\n    nGramsSplits[0] = 0;\n    for (let i = 1; i <= numBatchItems; ++i) {\n      const length = splits[i] - splits[i - 1];\n      let numNGrams = 0;\n      this.nGramWidths.forEach((nGramWidth) => {\n        numNGrams += this.getNumNGrams(length, nGramWidth);\n      });\n      if (this.preserveShort && length > 0 && numNGrams === 0) {\n        numNGrams = 1;\n      }\n      nGramsSplits[i] = nGramsSplits[i - 1] + numNGrams;\n    }\n\n    const nGrams: Uint8Array[] = new Array(nGramsSplits[numBatchItems]);\n\n    for (let i = 0; i < numBatchItems; ++i) {\n      const splitIndex = splits[i];\n      let outputStartIdx = nGramsSplits[i];\n      this.nGramWidths.forEach((nGramWidth) => {\n        const length = splits[i + 1] - splits[i];\n        const numNGrams = this.getNumNGrams(length, nGramWidth);\n        this.createNGrams(\n            data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);\n        outputStartIdx += numNGrams;\n      });\n      // If we're preserving short sequences, check to see if no sequence was\n      // generated by comparing the current output start idx to the original\n      // one (nGramSplitsdata). If no ngrams were generated, then they will\n      // be equal (since we increment outputStartIdx by numNGrams every\n      // time we create a set of ngrams.)\n      if (this.preserveShort && outputStartIdx === nGramsSplits[i]) {\n        const dataLength = splits[i + 1] - splits[i];\n        // One legitimate reason to not have any ngrams when this.preserveShort\n        // is true is if the sequence itself is empty. In that case, move on.\n        if (dataLength === 0) {\n          continue;\n        }\n        // We don't have to worry about dynamic padding sizes here: if padding\n        // was dynamic, every sequence would have had sufficient padding to\n        // generate at least one nGram.\n        const nGramWidth = dataLength + 2 * this.padWidth;\n        const numNGrams = 1;\n        this.createNGrams(\n            data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);\n      }\n    }\n    return [nGrams, nGramsSplits];\n  }\n}\n\nexport function stringNGramsImpl(\n    data: Uint8Array[], dataSplits: Int32Array, separator: string,\n    nGramWidths: number[], leftPad: string, rightPad: string, padWidth: number,\n    preserveShortSequences: boolean): [Uint8Array[], Int32Array] {\n  return new StringNGramsOp(\n             separator, nGramWidths, leftPad, rightPad, padWidth,\n             preserveShortSequences)\n      .compute(data, dataSplits);\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sub} from '@tensorflow/tfjs-core';\n\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {binaryKernelFunc, createComplexBinaryKernelImpl} from '../utils/binary_utils';\n\nexport const subImpl = createSimpleBinaryKernelImpl(\n    ((aValue: number, bValue: number) => aValue - bValue));\nexport const subComplexImpl =\n    createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {\n      return {real: aReal - bReal, imag: aImag - bImag};\n    }));\nexport const sub = binaryKernelFunc(Sub, subImpl, subComplexImpl);\n\nexport const subConfig: KernelConfig = {\n  kernelName: Sub,\n  backendName: 'cpu',\n  kernelFunc: sub\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/** An implementation of the TopK kernel shared between webgl and cpu. */\n\nimport {buffer, NumericDataType, Rank, ShapeMap, Tensor, TensorBuffer, TypedArray, util} from '@tensorflow/tfjs-core';\n\ntype Pair = {\n  value: number,\n  index: number\n};\n\nconst comparePair = (a: Pair, b: Pair) => {\n  const valueDiff = b.value - a.value;\n  return valueDiff === 0 ? a.index - b.index : valueDiff;\n};\n\n/**\n * Partitions array where all elements smaller than the (k+1) smallest element\n * are found to the left of it, and all larger to the right of it.\n * Based on the Floyd-Rivest Algorithm, ref:\n * https://en.wikipedia.org/wiki/Floyd%E2%80%93Rivest_algorithm\n * @param array: Array to partition\n * @param left: Left index for the interval\n * @param right: Right index for the interval\n * @param k: Desired index value, where array[k] is the (k+1)th smallest element\n *           when left = 0\n */\nfunction select(array: Pair[], k: number, left = 0, right = array.length - 1) {\n  while (right > left) {\n    // Use select recursively to sample a smaller set of size s\n    // the arbitrary constants 600 and 0.5 are used in the original\n    // version to minimize execution time.\n    if (right - left > 600) {\n      const n = right - left + 1;\n      const i = k - left + 1;\n      const z = Math.log(n);\n      const s = 0.5 * Math.exp(2 * z / 3);\n      const sd = 0.5 * Math.sqrt(z * s * (n - s) / n) * Math.sign(i - n / 2);\n      const newLeft = Math.max(left, Math.floor(k - i * s / n + sd));\n      const newRight = Math.min(right, Math.floor(k + (n - i) * s / n + sd));\n      select(array, k, newLeft, newRight);\n    }\n    // partition the elements between left and right around t\n    const t = array[k];\n    let i = left;\n    let j = right;\n\n    util.swap(array, left, k);\n\n    if (comparePair(array[right], t) > 0) {\n      util.swap(array, left, right);\n    }\n    while (i < j) {\n      util.swap(array, i, j);\n      i++;\n      j--;\n      while (comparePair(array[i], t) < 0) {\n        i = i + 1;\n      }\n      while (comparePair(array[j], t) > 0) {\n        j = j - 1;\n      }\n    }\n    if (comparePair(array[left], t) === 0) {\n      util.swap(array, left, j);\n    } else {\n      j = j + 1;\n      util.swap(array, j, right);\n    }\n    // Adjust left and right towards the boundaries of the subset\n    // containing the (k - left + 1)th smallest element.\n    if (j <= k) {\n      left = j + 1;\n    }\n    if (k <= j) {\n      right = j - 1;\n    }\n  }\n}\n\nexport function topKImpl<T extends Tensor, R extends Rank>(\n    x: TypedArray, xShape: number[], xDtype: NumericDataType, k: number,\n    sorted: boolean):\n    [TensorBuffer<R, NumericDataType>, TensorBuffer<R, 'int32'>] {\n  // Reshape into a 2d tensor [batch, lastDim] and compute topk along lastDim.\n  const lastDim = xShape[xShape.length - 1];\n  const [batch, size] = [x.length / lastDim, lastDim];\n  const allTopKVals = util.getTypedArrayFromDType(xDtype, batch * k);\n  const allTopKIndices = util.getTypedArrayFromDType('int32', batch * k);\n\n  for (let b = 0; b < batch; b++) {\n    const offset = b * size;\n    const vals = x.subarray(offset, offset + size);\n\n    let valAndInd: Pair[] = new Array(vals.length);\n    vals.forEach(\n        (value: number, index: number) => valAndInd[index] = {value, index});\n\n    if (k < valAndInd.length) {\n      select(valAndInd, k);\n      valAndInd = valAndInd.slice(0, k);\n    }\n\n    if (sorted) {\n      valAndInd.sort(comparePair);\n    }\n    \n    const outOffset = b * k;\n    const topKVals = allTopKVals.subarray(outOffset, outOffset + k);\n    const topKIndices = allTopKIndices.subarray(outOffset, outOffset + k);\n    for (let i = 0; i < k; i++) {\n      topKVals[i] = valAndInd[i].value;\n      topKIndices[i] = valAndInd[i].index;\n    }\n  }\n  // Reshape back to the original input shape, except that the last\n  // dimension is k.\n  const outputShape = xShape.slice();\n  outputShape[outputShape.length - 1] = k;\n\n  return [\n    buffer(outputShape as ShapeMap[R], xDtype, allTopKVals),\n    buffer(outputShape as ShapeMap[R], 'int32', allTopKIndices)\n  ];\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// Import shared functionality from tfjs-backend-cpu without triggering\n// side effects.\n// tslint:disable-next-line: no-imports-from-dist\nimport * as shared from '@tensorflow/tfjs-backend-cpu/dist/shared';\n// tslint:disable-next-line: no-imports-from-dist\nimport {SimpleBinaryKernelImpl} from '@tensorflow/tfjs-backend-cpu/dist/shared';\n// tslint:disable-next-line: no-imports-from-dist\nimport {SimpleUnaryImpl} from '@tensorflow/tfjs-backend-cpu/dist/utils/unary_types';\n\nexport type SimpleBinaryKernelImplCPU = SimpleBinaryKernelImpl;\nexport type SimpleUnaryKernelImplCPU = SimpleUnaryImpl;\nconst {\n  addImpl: addImplCPU,\n  castImpl: castImplCPU,\n  ceilImpl: ceilImplCPU,\n  concatImpl: concatImplCPU,\n  equalImpl: equalImplCPU,\n  expImpl: expImplCPU,\n  expm1Impl: expm1ImplCPU,\n  floorImpl: floorImplCPU,\n  gatherNdImpl: gatherNdImplCPU,\n  gatherV2Impl: gatherV2ImplCPU,\n  greaterEqualImpl: greaterEqualImplCPU,\n  greaterImpl: greaterImplCPU,\n  lessEqualImpl: lessEqualImplCPU,\n  lessImpl: lessImplCPU,\n  logImpl: logImplCPU,\n  maxImpl: maxImplCPU,\n  maximumImpl: maximumImplCPU,\n  minimumImpl: minimumImplCPU,\n  multiplyImpl: multiplyImplCPU,\n  negImpl: negImplCPU,\n  notEqualImpl: notEqualImplCPU,\n  prodImpl: prodImplCPU,\n  rangeImpl: rangeImplCPU,\n  rsqrtImpl: rsqrtImplCPU,\n  scatterImpl: scatterImplCPU,\n  simpleAbsImpl: simpleAbsImplCPU,\n  sliceImpl: sliceImplCPU,\n  stridedSliceImpl: stridedSliceImplCPU,\n  stringNGramsImpl: stringNGramsImplCPU,\n  subImpl: subImplCPU,\n  tileImpl: tileImplCPU,\n  topKImpl: topKImplCPU,\n  transposeImpl: transposeImplCPU,\n  uniqueImpl: uniqueImplCPU,\n} = shared;\n\nexport {\n  addImplCPU,\n  castImplCPU,\n  ceilImplCPU,\n  concatImplCPU,\n  equalImplCPU,\n  expImplCPU,\n  expm1ImplCPU,\n  floorImplCPU,\n  gatherNdImplCPU,\n  gatherV2ImplCPU,\n  greaterEqualImplCPU,\n  greaterImplCPU,\n  lessEqualImplCPU,\n  lessImplCPU,\n  logImplCPU,\n  maxImplCPU,\n  maximumImplCPU,\n  minimumImplCPU,\n  multiplyImplCPU,\n  prodImplCPU,\n  negImplCPU,\n  notEqualImplCPU,\n  scatterImplCPU,\n  simpleAbsImplCPU,\n  sliceImplCPU,\n  stridedSliceImplCPU,\n  stringNGramsImplCPU,\n  subImplCPU,\n  rangeImplCPU,\n  rsqrtImplCPU,\n  tileImplCPU,\n  topKImplCPU,\n  transposeImplCPU,\n  uniqueImplCPU,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {Cast, CastAttrs, CastInputs, DataType, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {createSimpleBinaryKernelImpl} from '../utils/binary_impl';\nimport {zeros} from '../utils/zeros_impl';\n\nimport {complex} from './Complex';\nimport {identity} from './Identity';\nimport {real} from './Real';\n\nexport function castImpl(\n    values: TypedArray, shape: number[], inputType: DataType,\n    dtype: DataType): [number[], DataType, TypedArray] {\n  if (dtype === 'int32') {\n    const resultValues = Int32Array.from(values);\n    return [shape, 'int32', resultValues];\n  }\n\n  if (dtype === 'bool') {\n    // This is essentially the result of notEqual(x, 0). We avoid using\n    // kernel notEqual to avoid circular dependency, i.e. binary_utils ->\n    // cast -> notEqual -> binary_utils.\n    const zero = util.toTypedArray([0], inputType);\n\n    const [resultData, resultShape] = createSimpleBinaryKernelImpl(\n        (a, b) => (a !== b) ? 1 : 0)(shape, [], values, zero, 'bool');\n\n    return [resultShape, 'bool', resultData];\n  }\n  throw new Error(`Error in Cast: failed to cast ${inputType} to ${dtype}`);\n}\n\nexport function cast(\n    args: {inputs: CastInputs, backend: MathBackendCPU, attrs: CastAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {dtype} = attrs;\n\n  // Casting to complex64.\n  if (dtype === 'complex64') {\n    if (x.dtype === 'complex64') {\n      return identity({inputs: {x}, backend});\n    }\n\n    const zerosTensorInfo = zeros(backend, x.shape, x.dtype);\n    const floatX = cast({inputs: {x}, backend, attrs: {dtype: 'float32'}});\n\n    const result =\n        complex({inputs: {real: floatX, imag: zerosTensorInfo}, backend});\n\n    backend.disposeIntermediateTensorInfo(zerosTensorInfo);\n    backend.disposeIntermediateTensorInfo(floatX);\n\n    return result;\n  }\n\n  // Casting from complex64\n  if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const result = cast({inputs: {x: realPart}, backend, attrs: {dtype}});\n\n    backend.disposeIntermediateTensorInfo(realPart);\n\n    return result;\n  }\n\n  if (!util.hasEncodingLoss(x.dtype, dtype)) {\n    // We don't change the underlying data, since we cast to higher\n    // precision.\n    const result = identity({inputs: {x}, backend});\n    return {dataId: result.dataId, shape: result.shape, dtype};\n  }\n\n  const values = backend.data.get(x.dataId).values as TypedArray;\n  const [resultShape, resultType, resultData] =\n      castImpl(values, x.shape, x.dtype, dtype);\n  return backend.makeTensorInfo(resultShape, resultType, resultData);\n}\n\nexport const castConfig: KernelConfig = {\n  kernelName: Cast,\n  backendName: 'cpu',\n  kernelFunc: cast as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BackendValues, DataType, TypedArray, util} from '@tensorflow/tfjs-core';\n\nexport function concatImpl(\n    inputs: Array<{vals: BackendValues, shape: number[]}>, outShape: number[],\n    dtype: DataType, simplyConcat: boolean): TypedArray|string[] {\n  const outVals = util.getArrayFromDType(dtype, util.sizeFromShape(outShape));\n\n  if (simplyConcat && dtype !== 'string') {\n    // Use built-in TypedArray.set() method for speed.\n    let offset = 0;\n    inputs.forEach(input => {\n      const size = util.sizeFromShape(input.shape);\n\n      (outVals as TypedArray).set(input.vals as TypedArray, offset);\n      offset += size;\n    });\n  } else {\n    let colOffset = 0;\n\n    inputs.forEach(input => {\n      const decodedData = dtype === 'string' ?\n          backend_util.fromUint8ToStringArray(input.vals as Uint8Array[]) :\n          input.vals as TypedArray;\n\n      let tIdx = 0;\n\n      for (let row = 0; row < input.shape[0]; ++row) {\n        const resIdx = row * outShape[1] + colOffset;\n        for (let col = 0; col < input.shape[1]; ++col) {\n          outVals[resIdx + col] = decodedData[tIdx++];\n        }\n      }\n\n      colOffset += input.shape[1];\n    });\n  }\n\n  return outVals;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, DataType, Rank, TensorBuffer, TypedArray} from '@tensorflow/tfjs-core';\n\nexport function gatherNdImpl<R extends Rank>(\n    indicesData: TypedArray, paramsBuf: TensorBuffer<R>, dtype: DataType,\n    numSlices: number, sliceRank: number, sliceSize: number, strides: number[],\n    paramsShape: number[], paramsSize: number): TensorBuffer<R> {\n  const outBuf = buffer([numSlices, sliceSize], dtype);\n\n  for (let i = 0; i < numSlices; i++) {\n    const index = [];\n    let flattenIndex = 0;\n    for (let j = 0; j < sliceRank; j++) {\n      const dim = indicesData[i * sliceRank + j];\n      flattenIndex += dim * strides[j];\n      index.push(dim);\n    }\n    if (flattenIndex < 0 || flattenIndex >= paramsSize / sliceSize) {\n      throw new Error(\n          `Invalid indices: ${index} does not index into ${paramsShape}`);\n    }\n\n    for (let k = 0; k < sliceSize; k++) {\n      outBuf.values[i * sliceSize + k] =\n          paramsBuf.get(...paramsBuf.indexToLoc(flattenIndex * sliceSize + k));\n    }\n  }\n\n  return outBuf as TensorBuffer<R>;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, DataType, Rank, TensorBuffer} from '@tensorflow/tfjs-core';\n\nexport function gatherV2Impl<R extends Rank, D extends DataType>(\n    xBuf: TensorBuffer<R, D>, indicesBuf: TensorBuffer<R, D>,\n    flattenOutputShape: number[]): TensorBuffer<R, D> {\n  const outBuf = buffer(flattenOutputShape, xBuf.dtype);\n  for (let i = 0; i < outBuf.size; ++i) {\n    const newLoc = outBuf.indexToLoc(i);\n\n    const originalLoc: number[] = newLoc.slice();\n    const batchIdx = originalLoc[0];\n    const indicesIdx = originalLoc[2];\n    const indicesIndex = indicesBuf.locToIndex([batchIdx, indicesIdx]);\n    originalLoc[2] = indicesBuf.values[indicesIndex] as number;\n\n    const originalIndex = xBuf.locToIndex(originalLoc);\n\n    if (0 <= originalIndex && originalIndex < xBuf.values.length) {\n      outBuf.values[i] = xBuf.values[originalIndex];\n    } // Else, index is out of bounds, so leave the default zero val in outBuf.\n  }\n\n  return outBuf as TensorBuffer<R, D>;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, NumericDataType, TypedArray, util} from '@tensorflow/tfjs-core';\n\nexport function maxImpl(\n    aVals: TypedArray, reduceSize: number, outShape: number[],\n    dtype: DataType): TypedArray {\n  const vals = util.getTypedArrayFromDType(\n      dtype as NumericDataType, util.sizeFromShape(outShape));\n\n  for (let i = 0; i < vals.length; ++i) {\n    const offset = i * reduceSize;\n    let max = aVals[offset];\n    for (let j = 0; j < reduceSize; ++j) {\n      const value = aVals[offset + j];\n      if (Number.isNaN(value) ||\n          value > max) {  // comparison with NaN always return false\n        max = value;\n      }\n    }\n    vals[i] = max;\n  }\n  return vals;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, KernelConfig, KernelFunc, Neg, TensorInfo, TypedArray, UnaryInputs, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {multiplyImpl} from './Multiply';\n\nexport function negImpl(xVals: TypedArray, xShape: number[], xDtype: DataType):\n    [TypedArray, number[]] {\n  const minusOne =\n      util.createScalarValue(-1 as {} as 'float32', xDtype) as TypedArray;\n  return multiplyImpl([], xShape, minusOne, xVals, xDtype);\n}\n\nexport function neg(args: {inputs: UnaryInputs, backend: MathBackendCPU}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  assertNotComplex(x, 'neg');\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const [res, newShape] = negImpl(xVals, x.shape, x.dtype);\n\n  return backend.makeTensorInfo(newShape, x.dtype, res);\n}\n\nexport const negConfig: KernelConfig = {\n  kernelName: Neg,\n  backendName: 'cpu',\n  kernelFunc: neg as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DataType, KernelConfig, KernelFunc, Prod, ProdAttrs, ProdInputs, TensorInfo, TypedArray, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\nimport {transpose} from './Transpose';\n\nexport function prodImpl(\n    xShape: number[], xDtype: DataType, xVals: TypedArray,\n    reductionAxes: number[]):\n    {outVals: TypedArray, outShape: number[], outDtype: DataType} {\n  const [outShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes(xShape, reductionAxes);\n  const outDtype = upcastType(xDtype, 'int32');\n  const outVals = util.makeZerosTypedArray(\n                      util.sizeFromShape(outShape), outDtype) as TypedArray;\n  const reduceSize = util.sizeFromShape(reduceShape);\n\n  for (let i = 0; i < outVals.length; ++i) {\n    const offset = i * reduceSize;\n    let prod = 1;\n    for (let j = 0; j < reduceSize; ++j) {\n      prod *= xVals[offset + j];\n    }\n    outVals[i] = prod;\n  }\n\n  return {outVals, outShape, outDtype};\n}\n\nexport function prod(\n    args: {inputs: ProdInputs, backend: MathBackendCPU, attrs: ProdAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  assertNotComplex(x, 'prod');\n\n  const xRank = x.shape.length;\n  const axes = util.parseAxisParam(axis, x.shape);\n\n  const permutation = backend_util.getAxesPermutation(axes, xRank);\n  let reductionAxes = axes;\n  let permutedX = x;\n  const intermediateTensorInfos = [];\n  if (permutation != null) {\n    permutedX = transpose({inputs: {x}, backend, attrs: {perm: permutation}});\n    intermediateTensorInfos.push(permutedX);\n    reductionAxes = backend_util.getInnerMostAxes(reductionAxes.length, xRank);\n  }\n\n  const xVals = backend.data.get(permutedX.dataId).values as TypedArray;\n  const {outVals, outShape, outDtype} =\n      prodImpl(permutedX.shape, permutedX.dtype, xVals, reductionAxes);\n\n  let resultShape = outShape;\n  if (keepDims) {\n    resultShape = backend_util.expandShapeToKeepDim(outShape, axes);\n  }\n\n  intermediateTensorInfos.forEach(\n      t => backend.disposeIntermediateTensorInfo(t));\n\n  return backend.makeTensorInfo(resultShape, outDtype, outVals);\n}\n\nexport const prodConfig: KernelConfig = {\n  kernelName: Prod,\n  backendName: 'cpu',\n  kernelFunc: prod as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataTypeMap, util} from '@tensorflow/tfjs-core';\n\nexport function rangeImpl(\n    start: number, stop: number, step: number,\n    dtype: 'float32'|'int32'): DataTypeMap['float32' | 'int32'] {\n  const sameStartStop = start === stop;\n  const increasingRangeNegativeStep = start < stop && step < 0;\n  const decreasingRangePositiveStep = stop < start && step > 1;\n\n  if (sameStartStop || increasingRangeNegativeStep ||\n      decreasingRangePositiveStep) {\n    return util.makeZerosTypedArray(0, dtype);\n  }\n\n  const numElements = Math.abs(Math.ceil((stop - start) / step));\n  const values = util.makeZerosTypedArray(numElements, dtype);\n\n  if (stop < start && step === 1) {\n    // Auto adjust the step's sign if it hasn't been set\n    // (or was set to 1)\n    step = -1;\n  }\n\n  values[0] = start;\n  for (let i = 1; i < values.length; i++) {\n    values[i] = values[i - 1] + step;\n  }\n  return values;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {buffer, Rank, ShapeMap, TensorBuffer, TypedArray} from '@tensorflow/tfjs-core';\n\ninterface DefaultValueTypeMap {\n  bool: boolean;\n  int32: number;\n  float32: number;\n  string: string;\n}\n\nexport function\nscatterImpl<R extends Rank, D extends 'float32'|'int32'|'bool'|'string'>(\n    indices: TensorBuffer<R, 'int32'>, updates: TensorBuffer<R, D>,\n    shape: number[], outputSize: number, sliceSize: number, numUpdates: number,\n    sliceRank: number, strides: number[], defaultValue: DefaultValueTypeMap[D],\n    sumDupeIndices: boolean): TensorBuffer<R, D> {\n  const flattenShape = [outputSize / sliceSize, sliceSize];\n\n  const indicesData = indices.values as TypedArray;\n  const updatesData = updates.values;\n\n  if (outputSize === 0) {\n    return buffer(shape as ShapeMap[R], updates.dtype);\n  }\n\n  const outBuf = buffer(flattenShape, updates.dtype);\n  if (typeof defaultValue === 'string') {\n    (outBuf.values as string[]).fill(defaultValue);\n  } else if (typeof defaultValue === 'number') {\n    (outBuf.values as TypedArray).fill(defaultValue);\n  } else if (typeof defaultValue === 'boolean') {\n    (outBuf.values as TypedArray).fill(+defaultValue);\n  }\n\n  for (let i = 0; i < numUpdates; i++) {\n    const index = [];\n    let flattenIndex = 0;\n    for (let j = 0; j < sliceRank; j++) {\n      const dim = indicesData[i * sliceRank + j];\n      index.push(dim);\n      flattenIndex += dim * strides[j];\n    }\n\n    if (flattenIndex < 0 || flattenIndex >= outputSize / sliceSize) {\n      throw new Error(`Invalid indices: ${index} does not index into ${shape}`);\n    }\n\n    for (let k = 0; k < sliceSize; k++) {\n      if (sumDupeIndices) {\n        (outBuf.values as TypedArray)[flattenIndex * sliceSize + k] +=\n            (updatesData as TypedArray)[i * sliceSize + k];\n      } else {\n        outBuf.values[flattenIndex * sliceSize + k] = updates.rank === 0 ?\n            updatesData[0] :\n            updatesData[i * sliceSize + k];\n      }\n    }\n  }\n\n  return outBuf as TensorBuffer<R, D>;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Abs, AbsInputs, KernelConfig, KernelFunc, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function simpleAbsImpl(vals: TypedArray): Float32Array {\n  const resultValues = new Float32Array(vals.length);\n  for (let i = 0; i < vals.length; ++i) {\n    resultValues[i] = Math.abs(vals[i]);\n  }\n  return resultValues;\n}\n\nexport const abs = (args: {inputs: AbsInputs, backend: MathBackendCPU}) => {\n  const {x} = args.inputs;\n  const cpuBackend = args.backend;\n\n  assertNotComplex(x, 'abs');\n\n  let resultValues = new Float32Array(util.sizeFromShape(x.shape));\n  const values = cpuBackend.data.get(x.dataId).values as TypedArray;\n  resultValues = simpleAbsImpl(values);\n\n  return cpuBackend.makeOutput(resultValues, x.shape, x.dtype);\n};\n\nexport const absConfig: KernelConfig = {\n  kernelName: Abs,\n  backendName: 'cpu',\n  kernelFunc: abs as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BackendValues, buffer, DataType, KernelConfig, KernelFunc, Slice, slice_util, SliceAttrs, SliceInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function sliceImpl(\n    vals: BackendValues, begin: number[], size: number[], shape: number[],\n    dtype: DataType): BackendValues {\n  const isContinous = slice_util.isSliceContinous(shape, begin, size);\n  const length = util.sizeFromShape(size);\n  const xStrides = util.computeStrides(shape);\n\n  if (isContinous) {\n    const flatOffset = slice_util.computeFlatOffset(begin, xStrides);\n\n    if (dtype === 'string') {\n      return (vals as Uint8Array[]).slice(flatOffset, flatOffset + length);\n    }\n\n    return (vals as TypedArray).subarray(flatOffset, flatOffset + length);\n  }\n\n  const decodedData = dtype === 'string' ?\n      backend_util.fromUint8ToStringArray(vals as Uint8Array[]) :\n      vals as TypedArray;\n\n  const inBuf = buffer(shape, dtype, decodedData);\n  const outBuf = buffer(size, dtype);\n  for (let i = 0; i < outBuf.size; ++i) {\n    const outLoc = outBuf.indexToLoc(i);\n    const inLoc = outLoc.map((idx: number, j) => idx + begin[j]);\n    outBuf.set(inBuf.get(...inLoc), ...outLoc);\n  }\n\n  if (dtype === 'string') {\n    return backend_util.fromStringArrayToUint8(outBuf.values as string[]);\n  }\n  return outBuf.values as TypedArray;\n}\n\nexport function slice(\n    args: {inputs: SliceInputs, backend: MathBackendCPU, attrs: SliceAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {begin, size} = attrs;\n\n  assertNotComplex(x, 'slice');\n\n  const [$begin, $size] = slice_util.parseSliceParams(x, begin, size);\n  slice_util.assertParamsValid(x, $begin, $size);\n\n  const vals = backend.data.get(x.dataId).values;\n  const outVals = sliceImpl(vals, $begin, $size, x.shape, x.dtype);\n  return backend.makeTensorInfo($size, x.dtype, outVals);\n}\n\nexport const sliceConfig: KernelConfig = {\n  kernelName: Slice,\n  backendName: 'cpu',\n  kernelFunc: slice as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, Rank, TensorBuffer} from '@tensorflow/tfjs-core';\n\nexport function stridedSliceImpl<R extends Rank>(\n    outShape: number[], xBuf: TensorBuffer<R>, strides: number[],\n    begin: number[]): TensorBuffer<R> {\n  const outBuf = buffer(outShape, xBuf.dtype);\n\n  for (let i = 0; i < outBuf.size; i++) {\n    const loc = outBuf.indexToLoc(i);\n\n    const newLoc: number[] = new Array(loc.length);\n    for (let j = 0; j < newLoc.length; j++) {\n      newLoc[j] = loc[j] * strides[j] + begin[j];\n    }\n    outBuf.set(xBuf.get(...newLoc), ...loc);\n  }\n\n  return outBuf as TensorBuffer<R>;\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, DataType, Rank, TensorBuffer} from '@tensorflow/tfjs-core';\n\n/**\n * An implementation of the tile kernel shared between webgl and cpu for string\n * tensors only.\n */\n\nexport function tileImpl<R extends Rank>(\n    xBuf: TensorBuffer<R, DataType>,\n    reps: number[]): TensorBuffer<R, DataType> {\n  const newShape: number[] = new Array(xBuf.rank);\n  for (let i = 0; i < newShape.length; i++) {\n    newShape[i] = xBuf.shape[i] * reps[i];\n  }\n  const result = buffer(newShape, xBuf.dtype);\n  for (let i = 0; i < result.values.length; ++i) {\n    const newLoc = result.indexToLoc(i);\n\n    const originalLoc: number[] = new Array(xBuf.rank);\n    for (let j = 0; j < originalLoc.length; j++) {\n      originalLoc[j] = newLoc[j] % xBuf.shape[j];\n    }\n\n    const originalIndex = xBuf.locToIndex(originalLoc);\n\n    result.values[i] = xBuf.values[originalIndex];\n  }\n  return result as TensorBuffer<R, DataType>;\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, NumericDataType, TypedArray} from '@tensorflow/tfjs-core';\nimport {util} from '@tensorflow/tfjs-core';\n\nexport function transposeImpl(\n    xVals: TypedArray, xShape: number[], dtype: DataType, perm: number[],\n    newShape: number[]): TypedArray {\n  const xRank = xShape.length;\n  const xSize = util.sizeFromShape(xShape);\n  const xStrides = util.computeStrides(xShape);\n  const newStrides = util.computeStrides(newShape);\n\n  const result = util.getTypedArrayFromDType(\n      dtype as NumericDataType, util.sizeFromShape(newShape));\n\n  for (let i = 0; i < xSize; ++i) {\n    const loc = util.indexToLoc(i, xRank, xStrides);\n\n    // Permute location.\n    const newLoc: number[] = new Array(loc.length);\n    for (let i = 0; i < newLoc.length; i++) {\n      newLoc[i] = loc[perm[i]];\n    }\n\n    const newIndex = util.locToIndex(newLoc, xRank, newStrides);\n    result[newIndex] = xVals[i];\n  }\n  return result;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Abs, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {simpleAbsImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const abs =\n    unaryKernelFunc({opType: UnaryOpType.ABS, cpuKernelImpl: simpleAbsImplCPU});\n\nexport const absConfig: KernelConfig = {\n  kernelName: Abs,\n  backendName: 'webgpu',\n  kernelFunc: abs\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Add, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {addImplCPU as cpuAdd} from '../kernel_utils/shared';\n\nexport const addKernelFunc = binaryKernelFunc(\n    {opType: BinaryOpType.ADD, cpuKernelImpl: cpuAdd, supportsComplex: true});\n\nexport const addConfig: KernelConfig = {\n  kernelName: Add,\n  backendName: 'webgpu',\n  kernelFunc: addKernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class AddNPackedProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames: string[];\n  workPerThread = 1;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(shapes: number[][]) {\n    this.outputShape = shapes[0];\n    this.variableNames = shapes.map((_, i) => `T${i}`);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n    this.shaderKey = 'addN';\n  }\n\n  getUserCode(): string {\n    const snippets: string[] = [];\n    // Get target elements from every input tensor.\n    this.variableNames.forEach(variable => {\n      snippets.push(`let v${variable} = get${variable}ByOutputCoords(coords);`);\n    });\n    // Calculate the sum of all elements.\n    const operation = this.variableNames\n                          .map(variable => {\n                            return `v${variable}`;\n                          })\n                          .join(' + ');\n\n    const userCode = `\n      ${main('index')} {\n        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {\n          let flatIndex = index * ${this.workPerThread} + i;\n          if (flatIndex < uniforms.size) {\n            let coords = getCoordsFromIndex(flatIndex);\n            ${snippets.join('\\n        ')}\n            setOutputAtIndex(flatIndex, ${operation});\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AddN, AddNInputs, KernelConfig, KernelFunc, TensorInfo, upcastType} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {AddNPackedProgram} from '../addn_packed_webgpu';\nimport {identity} from './Identity';\n\nexport function addN(args: {inputs: AddNInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n\n  const tensors = inputs;\n  if (tensors.length === 1) {\n    return identity({inputs: {x: tensors[0]}, backend});\n  }\n\n  const dtype =\n      tensors.map(t => t.dtype).reduce((d1, d2) => upcastType(d1, d2));\n  const shapes = tensors.map(t => t.shape);\n  const program = new AddNPackedProgram(shapes);\n  return backend.runWebGPUProgram(program, tensors, dtype);\n}\n\nexport const addNConfig: KernelConfig = {\n  kernelName: AddN,\n  backendName: 'webgpu',\n  kernelFunc: addN as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, util} from '@tensorflow/tfjs-core';\nimport {getCoordsXYZ, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ArgMinMaxProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  variableNames = ['x'];\n  uniforms = 'infinityValue : f32,';\n  inputShape: number[];\n  reductionFactor: number;\n  op: string;\n  size = true;\n  private type: string;\n\n  constructor(inputShape: number[], axis: number, reduceType: 'min'|'max') {\n    const axes = [axis];\n\n    this.op = reduceType === 'min' ? '<' : '>';\n\n    // |outShape| is the shape with the removed axis\n    const [outputShape, reduceShape] =\n        backend_util.computeOutAndReduceShapes(inputShape, axes);\n\n    this.outputShape = outputShape.length === 0 ? [1] : outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    // The shared algorithm is mainly used for large reduce size. It fully\n    // utilizes the threads in one workgroup to do the reduction. However,\n    // when the reduce size is very small or the output shape is too large. It's\n    // better to use the plain algorithm to reduce the number of workgroups to\n    // speedup. The threthold can be further tuned.\n    if (util.sizeFromShape(reduceShape) < 32 ||\n        util.sizeFromShape(outputShape) > 1000) {\n      this.type = 'plain';\n      this.dispatch = computeDispatch(\n          this.dispatchLayout, this.outputShape, this.workGroupSize);\n    } else {\n      this.type = 'shared';\n      // A work group only outputs a data, so we transfer [1, 1, 1] to compute\n      // dispatch size.\n      this.dispatch =\n          computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);\n    }\n\n    this.inputShape = inputShape;\n    this.shaderKey = `argMinMax_${this.op}_${this.type}`;\n  }\n\n  getUserCode(): string {\n    const getInputShapeLastDim = () => {\n      if (this.inputShape.length === 1) {\n        return 'uniforms.xShape';\n      } else {\n        return `uniforms.xShape.${getCoordsXYZ(this.inputShape.length - 1)}`;\n      }\n    };\n\n    const splitOutputCoords = () => {\n      let snippet = '';\n      if (this.outputShape.length === 1) {\n        if (this.inputShape.length !== 1) {\n          snippet += 'outputCoords,';\n        }\n      } else {\n        for (let i = 0; i < this.outputShape.length; i++) {\n          snippet += `outputCoords.${getCoordsXYZ(i)},`;\n        }\n      }\n      return snippet;\n    };\n\n    if (this.type === 'shared') {\n      const sharedMemorySnippet = `\n      var<workgroup> xBestIndices : array<i32, ${this.workGroupSize[0]}>;\n      var<workgroup> xBestValues : array<f32, ${this.workGroupSize[0]}>;\n    `;\n      const userCode = `\n      fn DIV_CEIL(a : u32, b : u32) -> u32 {\n        return ((a - 1u) / b + 1u);\n      }\n\n      ${sharedMemorySnippet}\n\n      ${main('index')} {\n        let outputIndex = index / i32(workGroupSizeX);\n        let reduceLength = ${getInputShapeLastDim()};\n\n        var bestIndex = i32(localId.x);\n        var bestValue = uniforms.infinityValue;\n        let outputCoords = getCoordsFromIndex(outputIndex);\n        for (var k = i32(localId.x); k < reduceLength && outputIndex < uniforms.size;\n            k = k + i32(workGroupSizeX)) {\n          let candidate = getX(${splitOutputCoords()} k);\n          if (!isnan(candidate) && candidate ${this.op} bestValue) {\n            bestValue = candidate;\n            bestIndex = k;\n          }\n        }\n        xBestValues[localId.x] = bestValue;\n        xBestIndices[localId.x] = bestIndex;\n        workgroupBarrier();\n\n        var reduceSize = min(u32(reduceLength), workGroupSizeX);\n        for (var currentSize = reduceSize / 2u; reduceSize > 1u;\n            currentSize = reduceSize / 2u) {\n          let interval = DIV_CEIL(reduceSize, 2u);\n          if (localId.x < currentSize) {\n            let candidate = xBestValues[localId.x + interval];\n            if (candidate ${this.op} bestValue) {\n              bestValue = candidate;\n              xBestValues[localId.x] = bestValue;\n              xBestIndices[localId.x] = xBestIndices[localId.x + interval];\n            }\n          }\n          reduceSize = interval;\n          workgroupBarrier();\n        }\n\n        if (localId.x == 0u && outputIndex < uniforms.size) {\n          setOutputAtIndexI32(outputIndex, xBestIndices[localId.x]);\n        }\n      }\n    `;\n      return userCode;\n    } else {\n      const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let outputCoords = getCoordsFromIndex(index);\n          var bestIndex = 0;\n          var bestValue = getX(${splitOutputCoords()} 0);\n          let reduceLength = ${getInputShapeLastDim()};\n          for (var i = 1; i < reduceLength; i++) {\n            let candidate = getX(${splitOutputCoords()} i);\n            if (candidate ${this.op} bestValue) {\n              bestValue = candidate;\n              bestIndex = i;\n            }\n          }\n          setOutputAtIndexI32(index, bestIndex);\n        }\n      }\n      `;\n      return userCode;\n    }\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getWorkGroupSizeString, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch} from './webgpu_util';\n\nexport class TransposeSharedProgram implements WebGPUProgram {\n  variableNames = ['A'];\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[]};\n  dispatch: [number, number, number];\n  // Note that the maximum number of workgroup invocations by webgpu is 256.\n  workGroupSize: [number, number, number] = [16, 16, 1];\n\n  constructor(aShape: number[], newDim: number[]) {\n    const outputShape: number[] = new Array(aShape.length);\n    for (let i = 0; i < outputShape.length; i++) {\n      outputShape[i] = aShape[newDim[i]];\n    }\n    this.outputShape = outputShape;\n    this.dispatchLayout = {x: [0], y: [1]};\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize, [1, 1, 1]);\n\n    this.shaderKey = 'transposeShared';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      const TILE_DIM = ${this.workGroupSize[0]};\n      var<workgroup> tile : array<array<f32, ${this.workGroupSize[0] + 1}>, ${\n        this.workGroupSize[0]}>;\n      ${getWorkGroupSizeString()}\n      fn _start(@builtin(local_invocation_id) localId : vec3<u32>,\n                @builtin(workgroup_id) workgroupId : vec3<u32>) {\n        var x = i32(workgroupId.x) * TILE_DIM + i32(localId.x);\n        var y = i32(workgroupId.y) * TILE_DIM + i32(localId.y);\n        let width = uniforms.outShape[0];\n        let height = uniforms.outShape[1];\n        if (x < width && y < height) {\n          tile[localId.y][localId.x] = A[y * width + x];\n        }\n        workgroupBarrier();\n\n        x = i32(workgroupId.y) * TILE_DIM + i32(localId.x);\n        y = i32(workgroupId.x) * TILE_DIM + i32(localId.y);\n        if (x < height && y < width) {\n          setOutputAtIndex((y * height + x), tile[localId.x]\n            [localId.y]);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getCoordsXYZ, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class TransposeProgram implements WebGPUProgram {\n  variableNames = ['A'];\n  shaderKey: string;\n  outputShape: number[];\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workPerThread = 1;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  newDim: number[];\n  size = true;\n\n  constructor(aShape: number[], newDim: number[]) {\n    const outputShape: number[] = new Array(aShape.length);\n    for (let i = 0; i < outputShape.length; i++) {\n      outputShape[i] = aShape[newDim[i]];\n    }\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n\n    this.newDim = newDim;\n    this.shaderKey = `transpose_${newDim}`;\n  }\n\n  getUserCode(): string {\n    const dtype = getCoordsDataType(this.outputShape.length);\n    const switched = getSwitchedCoords(this.newDim);\n\n    const userCode = `\n      ${main('index')} {\n        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {\n          let flatIndex = index * ${this.workPerThread} + i;\n          if(flatIndex < uniforms.size) {\n            let resRC = getCoordsFromIndex(flatIndex);\n            setOutputAtIndex(flatIndex, A[getIndexFromCoords${\n        this.outputShape.length}D(\n              ${dtype}(${switched}), uniforms.aShape)]);\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n\nfunction getSwitchedCoords(newDim: number[]): string {\n  const rank = newDim.length;\n  if (rank > 6) {\n    throw Error(`Transpose for rank ${rank} is not yet supported`);\n  }\n  const switchedCoords = new Array(rank);\n  for (let i = 0; i < newDim.length; i++) {\n    switchedCoords[newDim[i]] = `resRC.${getCoordsXYZ(i)}`;\n  }\n\n  return switchedCoords.join();\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Transpose, TransposeAttrs, TransposeInputs, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {transposeImplCPU as cpuTranspose} from '../kernel_utils/shared';\n\nimport {TransposeSharedProgram} from '../transpose_shared_webgpu';\nimport {TransposeProgram} from '../transpose_webgpu';\n\nexport function transpose(args: {\n  inputs: TransposeInputs,\n  attrs: TransposeAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {perm} = attrs;\n  const webgpuBackend = backend;\n\n  const xRank = x.shape.length;\n  const newShape: number[] = new Array(xRank);\n  for (let i = 0; i < newShape.length; i++) {\n    newShape[i] = x.shape[perm[i]];\n  }\n  if (backend.shouldExecuteOnCPU([x])) {\n    const xData = webgpuBackend.tensorMap.get(x.dataId);\n    const values = xData.values as TypedArray;\n    const outValues = cpuTranspose(values, x.shape, x.dtype, perm, newShape);\n    return backend.makeTensorInfo(newShape, x.dtype, outValues);\n  }\n  if (x.shape.length === 2 && util.arraysEqual(perm, [1, 0])) {\n    const program = new TransposeSharedProgram(x.shape, perm);\n    return webgpuBackend.runWebGPUProgram(program, [x], x.dtype);\n  }\n  const program = new TransposeProgram(x.shape, perm);\n  return webgpuBackend.runWebGPUProgram(program, [x], x.dtype);\n}\n\nexport const transposeConfig: KernelConfig = {\n  kernelName: Transpose,\n  backendName: 'webgpu',\n  kernelFunc: transpose as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ArgMax, ArgMaxAttrs, ArgMaxInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {ArgMinMaxProgram} from '../argminmax_webgpu';\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {transpose} from './Transpose';\n\nexport function argMax(\n    args: {inputs: ArgMaxInputs, backend: WebGPUBackend, attrs: ArgMaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis} = attrs;\n\n  let axes = util.parseAxisParam(axis, x.shape);\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  const intermediateTensorInfos = [];\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    intermediateTensorInfos.push($x);\n    axes = backend_util.getInnerMostAxes(axes.length, $x.shape.length);\n  }\n\n  backend_util.assertAxesAreInnerMostDims('argMax', [axes[0]], $x.shape.length);\n  const program = new ArgMinMaxProgram($x.shape, axes[0], 'max');\n  const uniformData = [{type: 'float32', data: [Number.NEGATIVE_INFINITY]}];\n  const out = backend.runWebGPUProgram(program, [$x], 'int32', uniformData);\n  intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));\n  return out;\n}\n\nexport const argMaxConfig: KernelConfig = {\n  kernelName: ArgMax,\n  backendName: 'webgpu',\n  kernelFunc: argMax as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ArgMin, ArgMinAttrs, ArgMinInputs, backend_util, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {ArgMinMaxProgram} from '../argminmax_webgpu';\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {transpose} from './Transpose';\n\nexport function argMin(\n    args: {inputs: ArgMinInputs, backend: WebGPUBackend, attrs: ArgMinAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis} = attrs;\n\n  let axes = util.parseAxisParam(axis, x.shape);\n  const permutedAxes = backend_util.getAxesPermutation(axes, x.shape.length);\n  let $x = x;\n  const intermediateTensorInfos = [];\n  if (permutedAxes != null) {\n    $x = transpose({inputs: {x}, backend, attrs: {perm: permutedAxes}});\n    intermediateTensorInfos.push($x);\n    axes = backend_util.getInnerMostAxes(axes.length, $x.shape.length);\n  }\n\n  backend_util.assertAxesAreInnerMostDims('argMin', [axes[0]], $x.shape.length);\n  const program = new ArgMinMaxProgram($x.shape, axes[0], 'min');\n  const uniformData = [{type: 'float32', data: [Number.POSITIVE_INFINITY]}];\n  const out = backend.runWebGPUProgram(program, [$x], 'int32', uniformData);\n  intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));\n  return out;\n}\n\nexport const argMinConfig: KernelConfig = {\n  kernelName: ArgMin,\n  backendName: 'webgpu',\n  kernelFunc: argMin as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Atan2, KernelConfig} from '@tensorflow/tfjs-core';\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const atan2 = binaryKernelFunc({opType: BinaryOpType.ATAN2});\n\nexport const atan2Config: KernelConfig = {\n  kernelName: Atan2,\n  backendName: 'webgpu',\n  kernelFunc: atan2\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class Pool2DProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms =\n      `stride : vec2<i32>, pad : vec2<i32>, dilation : vec2<i32>, convDims : vec2<i32>, filterDims : vec2<i32>,`;\n  // TODO(jiajia.qin@intel.com): Dynamically choose different workGroupSize for\n  // different output shapes.\n  workGroupSize: [number, number, number] = [128, 1, 1];\n  poolType: 'max'|'avg';\n  size = true;\n\n  constructor(convInfo: backend_util.Conv2DInfo, poolType: 'max'|'avg') {\n    this.outputShape = convInfo.outShape;\n\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = `pool2D_${poolType}`;\n    this.poolType = poolType;\n  }\n\n  getUserCode(): string {\n    let updateSnippet = `resultValue = max(value, resultValue);`;\n    if (this.poolType === 'avg') {\n      updateSnippet = `resultValue = resultValue + value; count = count + 1.0;`;\n    }\n\n    let returnValue = `resultValue`;\n    if (this.poolType === 'avg') {\n      returnValue = `resultValue / count`;\n    }\n\n    const userCode = `\n      ${main('index')} {\n      if (index < uniforms.size) {\n        let coords = getCoordsFromIndex(index);\n          let batch = coords[0];\n          let xRCCorner = vec2<i32>(coords.yz) * uniforms.stride - uniforms.pad;\n          let xRCorner = xRCCorner.x;\n          let xCCorner = xRCCorner.y;\n\n          var resultValue = ${\n        this.poolType === 'avg' ? '0.0' : '-1.0 / pow(10.0, -20.0)'};\n          var count = 0.0;\n\n          for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + uniforms.dilation.x) {\n            let xR = xRCorner + wR;\n\n            if (xR < 0 || xR >= uniforms.convDims.x) {\n              continue;\n            }\n\n            for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + uniforms.dilation.y) {\n              let xC = xCCorner + wC;\n              if (xC < 0 || xC >= uniforms.convDims.y) {\n                continue;\n              }\n\n              let value = getX(batch, xR, xC, coords[3]);\n              ${updateSnippet}\n            }\n          }\n\n          setOutputAtIndex(index, ${returnValue});\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class PoolWithFilterSizeEqualsOneProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms = `stride : vec2<i32>,`;\n  workGroupSize: [number, number, number] = [256, 1, 1];\n  size = true;\n\n  constructor(convInfo: backend_util.Conv2DInfo) {\n    this.outputShape = convInfo.outShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = 'poolWithFilterSizeEqualsOne';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getCoordsFromIndex(index);\n          let batch = coords[0];\n          let d = coords[3];\n\n          let xRCCorner = coords.yz * uniforms.stride;\n          let xRCorner = xRCCorner.x;\n          let xCCorner = xRCCorner.y;\n\n          let value = getX(batch, xRCorner, xCCorner, d);\n          setOutputAtIndex(index, value);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ReduceProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  variableNames = ['x'];\n  uniforms = 'reduceSize : i32,';\n  reduceType: 'max'|'mean'|'min'|'prod'|'sum';\n  inputShape: number[];\n  size = true;\n\n  constructor(\n      reduceInfo: backend_util.ReduceInfo,\n      reduceType: 'max'|'mean'|'min'|'prod'|'sum') {\n    this.inputShape = [reduceInfo.batchSize, reduceInfo.inSize];\n    const [outputShape, ] =\n        backend_util.computeOutAndReduceShapes(this.inputShape, [1]);\n    this.outputShape = outputShape.length === 0 ? [1] : outputShape;\n\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    // A work group only outputs a data, so we transfer [1, 1, 1] to compute\n    // dispatch size.\n    this.dispatch =\n        computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);\n\n    this.reduceType = reduceType;\n    this.shaderKey = `reduce_${reduceType}`;\n  }\n\n  getUserCode(): string {\n    let reduceOp = ``;\n    let initValue = '0.0';\n    if (this.reduceType === 'min' || this.reduceType === 'max') {\n      reduceOp = `\n         if (isnan(candidate)) {\n          bestValue = uniforms.NAN;\n         } else if (!isnan(bestValue) && candidate ${\n          this.reduceType === 'min' ? '<' : '>'} bestValue)\n           {  bestValue = candidate; }`;\n      initValue = 'f32(x[offset])';\n    } else if (this.reduceType === 'sum' || this.reduceType === 'mean') {\n      reduceOp = ' bestValue = bestValue + candidate; ';\n    } else if (this.reduceType === 'prod') {\n      reduceOp = ' bestValue = bestValue * candidate; ';\n      initValue = '1.0';\n    }\n\n    const outputSnippet = this.reduceType === 'mean' ?\n        // tslint:disable-next-line:max-line-length\n        `setOutputAtIndex(outputIndex, bestValue / f32(uniforms.reduceSize));` :\n        `setOutputAtIndex(outputIndex, bestValue);`;\n\n    const sharedMemorySnippet = `\n         var<workgroup> xBestValues : array<f32, ${this.workGroupSize[0]}>;\n       `;\n\n    const userCode = `\n       fn DIV_CEIL(a : u32, b : u32) -> u32 {\n        return ((a - 1u) / b + 1u);\n       }\n\n       ${sharedMemorySnippet}\n       fn getOffset(outputIndex : i32) -> i32 {\n         let outputCoords = getCoordsFromIndex(outputIndex);\n         let offset = ${\n        this.outputShape.length === 1 ?\n            'outputCoords' :\n            'outputCoords[0]'} * uniforms.reduceSize;\n          return offset;\n       }\n       ${main('index')} {\n         let outputIndex = index / i32(workGroupSizeX);\n         let offset = getOffset(outputIndex);\n         var bestValue = ${initValue};\n         let Length = uniforms.reduceSize;\n         let WorkPerThread = DIV_CEIL(u32(Length), workGroupSizeX);\n         for (var k = i32(localId.x); k < Length && outputIndex < uniforms.size;\n             k = k + i32(workGroupSizeX)) {\n           let candidate = f32(x[offset + k]);\n           ${reduceOp}\n         }\n         xBestValues[localId.x] = bestValue;\n         workgroupBarrier();\n\n         var reduceSize = min(u32(Length), workGroupSizeX);\n         for (var currentSize = reduceSize / 2u; reduceSize > 1u;\n             currentSize = reduceSize / 2u) {\n           let interval = DIV_CEIL(reduceSize, 2u);\n           if (localId.x < currentSize) {\n            let candidate = xBestValues[localId.x + interval];\n            ${reduceOp}\n            xBestValues[localId.x] = bestValue;\n           }\n           reduceSize = interval;\n           workgroupBarrier();\n         }\n\n         if (localId.x == 0u && outputIndex < uniforms.size) {\n          ${outputSnippet}\n        }\n       }\n     `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, sumOutType, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {maxImplCPU} from './shared';\nimport {prodImplCPU} from './shared';\nimport {ReduceProgram} from '../reduce_webgpu';\nimport {reshape} from '../kernels/Reshape';\nimport {transpose} from '../kernels/Transpose';\n\ntype ReduceTypes = 'max'|'mean'|'min'|'prod'|'sum';\n\nexport function reduce(\n    x: TensorInfo, axis: number|number[], keepDims: boolean,\n    reduceType: ReduceTypes, backend: WebGPUBackend): TensorInfo {\n  const xRank = x.shape.length;\n  const toDispose = [];\n\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, xRank);\n\n  let input = x;\n  if (permutedAxes != null) {\n    input = transpose({inputs: {x}, attrs: {perm: permutedAxes}, backend});\n    axes = backend_util.getInnerMostAxes(axes.length, xRank);\n    toDispose.push(input);\n  }\n\n  backend_util.assertAxesAreInnerMostDims(reduceType, axes, xRank);\n\n  const [reduceOutShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes(input.shape, axes);\n  let resOutShape = reduceOutShape;\n  if (keepDims) {\n    // rather than reshape at the end, set the target shape here.\n    resOutShape = backend_util.expandShapeToKeepDim(reduceOutShape, origAxes);\n  }\n\n  let res;\n  if ((reduceType === 'max' || reduceType === 'prod') &&\n      backend.shouldExecuteOnCPU([input])) {\n    const xVals = backend.tensorMap.get(input.dataId).values as TypedArray;\n    switch (reduceType) {\n      case 'max':\n        const outValues = maxImplCPU(\n            xVals, util.sizeFromShape(reduceShape), resOutShape, x.dtype);\n        res = backend.makeTensorInfo(resOutShape, x.dtype, outValues);\n        break;\n      case 'prod':\n        const {outVals, outShape, outDtype} =\n            prodImplCPU(input.shape, input.dtype, xVals, axes);\n        res = backend.makeTensorInfo(outShape, outDtype, outVals);\n        break;\n      default:\n        throw new Error(\n            `${reduceType} CPU implementation is not yet supported.`);\n    }\n  } else {\n    const inSize = util.sizeFromShape(reduceShape);\n    const xSize = util.sizeFromShape(input.shape);\n    const batchSize = xSize / inSize;\n\n    const reduceInfo = {windowSize: inSize, inSize, batchSize, outSize: 1};\n    const dtype = reduceType === 'mean' ? 'float32' : sumOutType(x.dtype);\n    const uniformData = [\n      {type: 'int32', data: [inSize]},\n    ];\n    const program = new ReduceProgram(reduceInfo, reduceType);\n    const reduced =\n        backend.runWebGPUProgram(program, [input], dtype, uniformData);\n    toDispose.push(reduced);\n\n    res = reshape({inputs: {x: reduced}, attrs: {shape: resOutShape}, backend});\n  }\n\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n\n  return res;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Max, MaxAttrs, MaxInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reduce} from '../kernel_utils/reduce';\n\nexport function max(\n    args: {inputs: MaxInputs, backend: WebGPUBackend, attrs: MaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {reductionIndices, keepDims} = attrs;\n\n  return reduce(x, reductionIndices, keepDims, 'max', backend);\n}\n\nexport const maxConfig: KernelConfig = {\n  kernelName: Max,\n  backendName: 'webgpu',\n  kernelFunc: max as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Mean, MeanAttrs, MeanInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reduce} from '../kernel_utils/reduce';\n\nexport function mean(\n    args: {inputs: MeanInputs, attrs: MeanAttrs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {keepDims, axis} = attrs;\n\n  return reduce(x, axis, keepDims, 'mean', backend);\n}\n\nexport const meanConfig: KernelConfig = {\n  kernelName: Mean,\n  backendName: 'webgpu',\n  kernelFunc: mean as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {Pool2DProgram} from '../pool2d_webgpu';\nimport {PoolWithFilterSizeEqualsOneProgram} from '../pool_filtersizeone_webgpu';\n\nimport {identity} from './Identity';\nimport {max} from './Max';\nimport {mean} from './Mean';\nimport {reshape} from './Reshape';\n\ntype PoolType = 'max'|'avg';\nexport function poolImpl(\n    x: TensorInfo, convInfo: backend_util.Conv2DInfo, poolType: PoolType,\n    backend: WebGPUBackend): TensorInfo {\n  if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 &&\n      util.arraysEqual(convInfo.inShape, convInfo.outShape)) {\n    return identity({inputs: {x}, backend});\n  }\n\n  if (convInfo.filterWidth === convInfo.inWidth &&\n      convInfo.filterHeight === convInfo.inHeight && convInfo.batchSize === 1 &&\n      convInfo.padInfo.type === 'VALID') {\n    const length = x.shape.length;\n    const reshapeX = reshape({\n      inputs: {x},\n      backend,\n      attrs: {\n        shape: [\n          x.shape[length - 3] * x.shape[length - 2] /* height * width */,\n          x.shape[length - 1] /* channel */\n        ]\n      }\n    });\n    let reduceX;\n    if (poolType === 'avg') {\n      reduceX = mean(\n          {inputs: {x: reshapeX}, backend, attrs: {axis: 0, keepDims: false}});\n    } else {\n      util.assert(poolType === 'max', () => `Invalid pool type ${poolType}`);\n      reduceX = max({\n        inputs: {x: reshapeX},\n        backend,\n        attrs: {reductionIndices: 0, keepDims: false}\n      });\n    }\n\n    const result = reshape(\n        {inputs: {x: reduceX}, backend, attrs: {shape: convInfo.outShape}});\n    backend.disposeData(reshapeX.dataId);\n    backend.disposeData(reduceX.dataId);\n    return result;\n  }\n\n  let program: Pool2DProgram|PoolWithFilterSizeEqualsOneProgram;\n  const dimensions =\n      [{type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]}];\n  if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1) {\n    program = new PoolWithFilterSizeEqualsOneProgram(convInfo);\n  } else {\n    if (poolType === 'avg') {\n      program = new Pool2DProgram(convInfo, 'avg');\n    } else {\n      util.assert(poolType === 'max', () => `Invalid pool type ${poolType}`);\n      program = new Pool2DProgram(convInfo, 'max');\n    }\n\n    dimensions.push(\n        {type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left]}, {\n          type: 'int32',\n          data: [convInfo.dilationHeight, convInfo.dilationWidth]\n        },\n        {type: 'int32', data: [convInfo.inHeight, convInfo.inWidth]}, {\n          type: 'int32',\n          data: [convInfo.effectiveFilterHeight, convInfo.effectiveFilterWidth]\n        });\n  }\n\n  return backend.runWebGPUProgram(program, [x], x.dtype, dimensions);\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {AvgPool, AvgPoolAttrs, AvgPoolInputs, backend_util, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {poolImpl} from './Pool_impl';\n\nexport function avgPool(\n    args: {inputs: AvgPoolInputs, backend: WebGPUBackend, attrs: AvgPoolAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n  const dilations = 1;\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      dilations, pad, dimRoundingMode);\n\n  return poolImpl(x, convInfo, 'avg', backend);\n}\n\nexport const avgPoolConfig: KernelConfig = {\n  kernelName: AvgPool,\n  backendName: 'webgpu',\n  kernelFunc: avgPool as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BatchMatMul, BatchMatMulAttrs, BatchMatMulInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {batchMatMulImpl} from './BatchMatMul_impl';\n\nexport function batchMatMul(args: {\n  inputs: BatchMatMulInputs,\n  attrs: BatchMatMulAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b} = inputs;\n  const {transposeA, transposeB} = attrs;\n\n  return batchMatMulImpl({a, b, transposeA, transposeB, backend});\n}\n\nexport const batchMatMulConfig: KernelConfig = {\n  kernelName: BatchMatMul,\n  backendName: 'webgpu',\n  kernelFunc: batchMatMul as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getCoordsXYZ, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class SliceProgram implements WebGPUProgram {\n  variableNames = ['source'];\n  uniforms: string;\n  outputShape: number[];\n  shaderKey: string;\n  rank: number;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workPerThread = 1;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  start: number[];\n  size = true;\n\n  constructor(start: number[], destSize: number[]) {\n    this.outputShape = destSize;\n    this.rank = destSize.length;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n\n    this.start = start;\n    this.uniforms = `start : ${getCoordsDataType(start.length)}, `;\n    this.shaderKey = 'slice';\n  }\n\n  getUserCode(): string {\n    const dtype = getCoordsDataType(this.rank);\n    const sourceCoords = getCoords(this.rank);\n    let coordSum;\n    if (this.start.length === 1) {\n      coordSum = this.outputShape.map((_, i) => {\n        return `sourceLoc = uniforms.start + coords;`;\n      });\n    } else {\n      coordSum = this.outputShape.map((_, i) => {\n        return `sourceLoc.${coords[i]} = uniforms.start.${\n            getCoordsXYZ(i)} + coords.${coords[i]};`;\n      });\n    }\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          var sourceLoc : ${dtype};\n          let coords = getCoordsFromIndex(index);\n          ${coordSum.join('\\n')}\n          setOutputAtIndex(index, getSource(${sourceCoords}));\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n\nconst coords = ['x', 'y', 'z', 'w', 'u', 'v'];\n\nfunction getCoords(rank: number): string {\n  if (rank === 1) {\n    return 'sourceLoc';\n  } else if (rank <= 6) {\n    return coords.slice(0, rank).map(coord => `sourceLoc.${coord}`).join(',');\n  } else {\n    throw Error(`Slicing for rank ${rank} is not yet supported`);\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Slice, slice_util, SliceAttrs, SliceInputs, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {sliceImplCPU} from '../kernel_utils/shared';\nimport {SliceProgram} from '../slice_webgpu';\n\nexport function slice(\n    args: {inputs: SliceInputs, backend: WebGPUBackend, attrs: SliceAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {begin, size} = attrs;\n\n  const [$begin, $size] = slice_util.parseSliceParams(x, begin, size);\n  slice_util.assertParamsValid(x, $begin, $size);\n\n  if (backend.shouldExecuteOnCPU([x]) || x.dtype === 'string') {\n    const xBufferInfo = backend.tensorMap.get(x.dataId);\n    const outValues = sliceImplCPU(\n        xBufferInfo.values as TypedArray, $begin, $size, x.shape, x.dtype);\n    return backend.makeTensorInfo($size, x.dtype, outValues);\n  }\n\n  if (util.sizeFromShape($size) === 0) {\n    return backend.makeTensorInfo($size, x.dtype, []);\n  }\n\n  // TODO(xing.xu): Add shadow slice support.\n  const program = new SliceProgram($begin, $size);\n  const uniformData = [{type: 'int32', data: $begin}];\n  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n}\n\nexport const sliceConfig: KernelConfig = {\n  kernelName: Slice,\n  backendName: 'webgpu',\n  kernelFunc: slice as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, BatchToSpaceND, BatchToSpaceNDAttrs, BatchToSpaceNDInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\nimport {transpose} from './Transpose';\n\nexport const batchToSpaceND = (args: {\n  inputs: BatchToSpaceNDInputs,\n  backend: WebGPUBackend,\n  attrs: BatchToSpaceNDAttrs\n}): TensorInfo => {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockShape, crops} = attrs;\n\n  util.assert(\n      x.shape.length <= 4,\n      () => 'batchToSpaceND for rank > 4 with a WebGPU backend not ' +\n          'implemented yet');\n  const prod = blockShape.reduce((a, b) => a * b);\n\n  const reshaped = backend_util.getReshaped(x.shape, blockShape, prod);\n  const permuted = backend_util.getPermuted(reshaped.length, blockShape.length);\n  const reshapedPermuted =\n      backend_util.getReshapedPermuted(x.shape, blockShape, prod);\n  const sliceBeginCoords =\n      backend_util.getSliceBeginCoords(crops, blockShape.length);\n  const sliceSize =\n      backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);\n\n  const toDispose = [];\n\n  const reshapedIntermediate =\n      reshape({inputs: {x}, backend, attrs: {shape: reshaped}});\n  const transposedIntermediate = transpose(\n      {inputs: {x: reshapedIntermediate}, backend, attrs: {perm: permuted}});\n  const reshapedIntermediate2 = reshape({\n    inputs: {x: transposedIntermediate},\n    backend,\n    attrs: {shape: reshapedPermuted}\n  });\n  const sliced = slice({\n    inputs: {x: reshapedIntermediate2},\n    backend,\n    attrs: {begin: sliceBeginCoords, size: sliceSize}\n  });\n\n  toDispose.push(reshapedIntermediate);\n  toDispose.push(transposedIntermediate);\n  toDispose.push(reshapedIntermediate2);\n\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n\n  return sliced;\n};\n\nexport const batchToSpaceNDConfig: KernelConfig = {\n  kernelName: BatchToSpaceND,\n  backendName: 'webgpu',\n  kernelFunc: batchToSpaceND as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, NotEqual} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {notEqualImplCPU as cpuNotEqual} from '../kernel_utils/shared';\n\nexport const notEqual = binaryKernelFunc({\n  opType: BinaryOpType.NOT_EQUAL,\n  dtype: 'bool',\n  cpuKernelImpl: cpuNotEqual\n});\n\nexport const notEqualConfig: KernelConfig = {\n  kernelName: NotEqual,\n  backendName: 'webgpu',\n  kernelFunc: notEqual\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Real, RealInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {identity} from './Identity';\n\nexport function real(args: {inputs: RealInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {input} = inputs;\n  const inputData = backend.tensorMap.get(input.dataId);\n\n  return identity({inputs: {x: inputData.complexTensorInfos.real}, backend});\n}\n\nexport const realConfig: KernelConfig = {\n  kernelName: Real,\n  backendName: 'webgpu',\n  kernelFunc: real as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport {BinaryInputs, Cast, CastAttrs, CastInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {castImplCPU} from '../kernel_utils/shared';\n\nimport {complex} from './Complex';\nimport {identity} from './Identity';\nimport {notEqual} from './NotEqual';\nimport {real} from './Real';\n\nimport {int} from '../kernel_utils/int';\n\nexport function cast(\n    args: {inputs: CastInputs, backend: WebGPUBackend, attrs: CastAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {dtype} = attrs;\n\n  // Casting to complex64.\n  if (dtype === 'complex64') {\n    if (x.dtype === 'complex64') {\n      return identity({inputs: {x}, backend});\n    }\n\n    // TODO: Import kernel function once zeros is modularized.\n    const zerosTensor = tf.zeros(x.shape);\n    const floatX = cast({inputs: {x}, backend, attrs: {dtype: 'float32'}});\n\n    const result =\n        complex({inputs: {real: floatX, imag: zerosTensor}, backend});\n\n    zerosTensor.dispose();\n    backend.disposeData(floatX.dataId);\n\n    return result;\n  }\n\n  // Casting from complex64\n  if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const result = cast({inputs: {x: realPart}, backend, attrs: {dtype}});\n    backend.disposeData(realPart.dataId);\n    return result;\n  }\n\n  if (!util.hasEncodingLoss(x.dtype, dtype)) {\n    // We don't change the underlying data, since we cast to higher\n    // precision.\n    const result = identity({inputs: {x}, backend});\n    return {dataId: result.dataId, shape: result.shape, dtype};\n  }\n\n  if (backend.shouldExecuteOnCPU([x])) {\n    const values = backend.tensorMap.get(x.dataId).values as TypedArray;\n    const [resultShape, resultType, resultData] =\n        castImplCPU(values, x.shape, x.dtype, dtype);\n    return backend.makeTensorInfo(resultShape, resultType, resultData);\n  }\n\n  if (dtype === 'int32') {\n    return int(x, backend);\n  }\n\n  if (dtype === 'bool') {\n    const zerosTensorInfo = backend.makeTensorInfo(\n        [], 'bool', util.getTypedArrayFromDType('bool', 1));\n\n    const binaryInputs: BinaryInputs = {a: x, b: zerosTensorInfo};\n\n    const result = notEqual({inputs: binaryInputs, backend}) as TensorInfo;\n    backend.disposeData(zerosTensorInfo.dataId);\n    return result;\n  }\n\n  throw new Error(`Error in Cast: failed to cast ${x.dtype} to ${dtype}`);\n}\n\nexport const castConfig: KernelConfig = {\n  kernelName: Cast,\n  backendName: 'webgpu',\n  kernelFunc: cast as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {TensorInfo} from '@tensorflow/tfjs-core';\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {UnaryOpType} from '../unary_op_util';\nimport {UnaryOpProgram} from '../unary_op_webgpu';\n\nexport function int(input: TensorInfo, backend: WebGPUBackend): TensorInfo {\n  const program = new UnaryOpProgram(input.shape, UnaryOpType.TO_INT);\n  const output = backend.runWebGPUProgram(program, [input], 'int32');\n  return {dataId: output.dataId, shape: output.shape, dtype: output.dtype};\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Ceil, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {ceilImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const ceil =\n    unaryKernelFunc({opType: UnaryOpType.CEIL, cpuKernelImpl: ceilImplCPU});\n\nexport const ceilConfig: KernelConfig = {\n  kernelName: Ceil,\n  backendName: 'webgpu',\n  kernelFunc: ceil\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ClipVec4Program implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  variableNames = ['A'];\n  uniforms = 'minVal : f32, maxVal : f32,';\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workPerThread = 4;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  isVec4 = true;\n  size = true;\n\n  constructor(outputShape: number[]) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n    this.shaderKey = 'clipVec4';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if(index < uniforms.size) {\n          let value = getAByOutputIndex(index);\n          var clampedValue : vec4<f32>;\n          for (var i = 0; i < 4; i = i + 1) {\n            if (isnan(value[i])) {\n              clampedValue[i] = value[i];\n            } else {\n              clampedValue[i] = clamp(value[i], uniforms.minVal, uniforms.maxVal);\n            }\n          }\n\n          setOutputAtIndex(index, clampedValue);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ClipProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  variableNames = ['A'];\n  uniforms = 'minVal : f32, maxVal : f32,';\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  minVal: number;\n  maxVal: number;\n  size = true;\n\n  constructor(outputShape: number[]) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = 'clip';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if(index < uniforms.size) {\n          let value = getAByOutputIndex(index);\n          if (isnan(value)) {\n            setOutputAtIndex(index, value);\n            return;\n          }\n          setOutputAtIndex(index, clamp(value, uniforms.minVal, uniforms.maxVal));\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ClipByValue, ClipByValueAttrs, ClipByValueInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {ClipVec4Program} from '../clip_vec4_webgpu';\nimport {ClipProgram} from '../clip_webgpu';\n\nexport function clipByValue(args: {\n  inputs: ClipByValueInputs,\n  backend: WebGPUBackend,\n  attrs: ClipByValueAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {clipValueMin, clipValueMax} = attrs;\n\n  let program: ClipProgram|ClipVec4Program;\n  const uniformData = [\n    {type: 'float32', data: [clipValueMin]},\n    {type: 'float32', data: [clipValueMax]}\n  ];\n  if (util.sizeFromShape(x.shape) % 4 === 0) {\n    program = new ClipVec4Program(x.shape);\n  } else {\n    program = new ClipProgram(x.shape);\n  }\n  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n}\n\nexport const clipByValueConfig: KernelConfig = {\n  kernelName: ClipByValue,\n  backendName: 'webgpu',\n  kernelFunc: clipByValue as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ConcatProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames: string[];\n  uniforms = '';\n  workPerThread = 1;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n  offsetLength: number;\n\n  constructor(shapes: Array<[number, number]>) {\n    this.outputShape =\n        backend_util.computeOutShape(shapes, 1 /* axis */) as [number, number];\n    this.variableNames = shapes.map((_, i) => `T${i}`);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n\n    this.offsetLength = shapes.length - 1;\n    for (let i = 0; i < this.offsetLength; i++) {\n      this.uniforms += `offset${i} : i32,`;\n    }\n    this.shaderKey = 'concat';\n  }\n\n  getUserCode(): string {\n    const snippets: string[] = [];\n    if (this.offsetLength > 0) {\n      snippets.push(\n          `if (yC < uniforms.offset0){ setOutputAtCoords(coords.x, coords.y, getT0(yR, yC)); }`);\n      for (let i = 1; i < this.offsetLength; i++) {\n        snippets.push(\n            `else if (yC < uniforms.offset${[i]}){ ` +\n            `setOutputAtCoords(coords.x, coords.y, getT${\n                i}(yR, yC - uniforms.offset${i - 1})); }`);\n      }\n      const lastIndex = this.offsetLength;\n      const lastShiftIndex = this.offsetLength - 1;\n      snippets.push(`else { setOutputAtCoords(coords.x, coords.y, getT${\n          lastIndex}(yR, yC - uniforms.offset${lastShiftIndex})); }`);\n    } else {\n      snippets.push(`setOutputAtCoords(coords.x, coords.y, getT0(yR, yC));`);\n    }\n\n    const userCode = `\n      ${main('index')} {\n        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {\n          let flatIndex = index * ${this.workPerThread} + i;\n          if(flatIndex < uniforms.size) {\n            let coords = getCoordsFromIndex(flatIndex);\n            let yR = coords.x;\n            let yC = coords.y;\n\n            ${snippets.join('\\n        ')}\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Imag, ImagInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {identity} from './Identity';\n\nexport function imag(args: {inputs: ImagInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {input} = inputs;\n  const inputData = backend.tensorMap.get(input.dataId);\n\n  return identity({inputs: {x: inputData.complexTensorInfos.imag}, backend});\n}\n\nexport const imagConfig: KernelConfig = {\n  kernelName: Imag,\n  backendName: 'webgpu',\n  kernelFunc: imag as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, ConcatInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {ConcatProgram} from '../concat_webgpu';\nimport {concatImplCPU} from '../kernel_utils/shared';\n\nimport {complex} from './Complex';\nimport {imag} from './Imag';\nimport {real} from './Real';\nimport {reshape} from './Reshape';\n\nexport function concatImpl(\n    inputs: ConcatInputs, axis: number, backend: WebGPUBackend): TensorInfo {\n  const dtype = inputs[0].dtype;\n  if (dtype === 'complex64') {\n    const reals = inputs.map((t) => real({inputs: {input: t}, backend}));\n    const imags = inputs.map((t) => imag({inputs: {input: t}, backend}));\n\n    const realConcated = concatImpl(reals, axis, backend);\n    const imagConcated = concatImpl(imags, axis, backend);\n\n    const result =\n        complex({inputs: {real: realConcated, imag: imagConcated}, backend});\n\n    reals.forEach(r => backend.disposeData(r.dataId));\n    imags.forEach(i => backend.disposeData(i.dataId));\n    backend.disposeData(realConcated.dataId);\n    backend.disposeData(imagConcated.dataId);\n\n    return result;\n  }\n\n  let runOnCpu = backend.shouldExecuteOnCPU(inputs);\n\n  // Run on cpu if dtype is string. For string, the backend represents it\n  // as Uint8Array[], where each Uint8Array is a character. Given that the\n  // computation is only on the outer array, uploading the whole data onto\n  // gpu is wasteful. Also, currently webgpu doesn't have a design to\n  // upload and retrieve Uint8Array[] between cpu and gpu. Therefore, we\n  // just run the kernel on cpu if dtype is string.\n  if (dtype === 'string') {\n    runOnCpu = true;\n  }\n\n  if (runOnCpu) {\n    // Any concat of n-dimensional tensors across any axis can be reduced to\n    // a concatenation of two-dimensional tensors across the axis 1 by first\n    // partitioning the axes of the original tensors into those less than the\n    // axis to be concatenated and the rest. Then reshape the tensors\n    // into a two-dimensional tensor by collapsing these two sets of axes and\n    // concatenate the resulting matrices across the axis 1, finally reshaping\n    // the result to have the proper shape.\n    const tensors2D = inputs.map(t => {\n      const innerSize = util.sizeFromShape(t.shape.slice(axis));\n      const shape = [-1, innerSize];\n      return reshape({inputs: {x: t}, backend, attrs: {shape}});\n    });\n\n    const inputsValShapes = tensors2D.map(t => {\n      return {vals: backend.readSync(t.dataId), shape: t.shape};\n    });\n\n    // Concats 2d tensors along axis=1.\n    const outShape =\n        backend_util.computeOutShape(tensors2D.map(t => t.shape), 1 /* axis */);\n    const simplyConcat = tensors2D[0].shape[0] === 1;\n    const outVals =\n        concatImplCPU(inputsValShapes, outShape, dtype, simplyConcat);\n\n    const finalOutShape =\n        backend_util.computeOutShape(inputs.map(t => t.shape), axis);\n\n    const outInfo = backend.makeTensorInfo(finalOutShape, dtype, outVals);\n\n    tensors2D.forEach(t => backend.disposeData(t.dataId));\n\n    return outInfo;\n  }\n\n  // There is a storage buffer limitation in compute stage, one for output so\n  // the maximum for input is limits.maxStorageBuffersPerShaderStage - 1\n  const maxInputNum = backend.device.limits.maxStorageBuffersPerShaderStage - 1;\n  if (inputs.length > maxInputNum) {\n    const reducedInputs = [];\n    for (let i = 0; i < inputs.length; i += maxInputNum) {\n      const subArray = inputs.slice(i, i + maxInputNum);\n      reducedInputs.push(concatImpl(subArray, axis, backend));\n    }\n    const result = concatImpl(reducedInputs, axis, backend);\n\n    for (const i of reducedInputs) {\n      backend.disposeData(i.dataId);\n    }\n\n    return result;\n  }\n\n  const {tensors2D, outShape} = computeTensors2D(inputs, axis, backend);\n  const shapes = (tensors2D).map(t => t.shape as [number, number]);\n  const program = new ConcatProgram(shapes);\n\n  const uniformData: Array<{type: string; data: number[]}> = [];\n  const offsets: number[] = new Array(shapes.length - 1);\n  if (offsets.length > 0) {\n    offsets[0] = shapes[0][1];\n    uniformData.push({type: 'int32', data: [offsets[0]]});\n    for (let i = 1; i < offsets.length; i++) {\n      offsets[i] = offsets[i - 1] + shapes[i][1];\n      uniformData.push({type: 'int32', data: [offsets[i]]});\n    }\n  }\n\n  const res = backend.runWebGPUProgram(\n      program, tensors2D, tensors2D[0].dtype, uniformData);\n  tensors2D.forEach(r => backend.disposeData(r.dataId));\n\n  const reshapedResult =\n      reshape({inputs: {x: res}, backend, attrs: {shape: outShape}});\n  backend.disposeData(res.dataId);\n  return reshapedResult;\n}\n\nfunction computeTensors2D(\n    inputs: ConcatInputs, axis: number, backend: WebGPUBackend) {\n  const outShape = backend_util.computeOutShape(inputs.map(t => t.shape), axis);\n  const tensors2D = inputs.map(t => reshape({\n                                 inputs: {x: t},\n                                 backend,\n                                 attrs: {\n                                   shape: [\n                                     util.sizeFromShape(t.shape.slice(0, axis)),\n                                     util.sizeFromShape(t.shape.slice(axis))\n                                   ]\n                                 }\n                               }));\n\n  return {tensors2D, outShape};\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Concat, ConcatAttrs, ConcatInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {concatImpl} from './Concat_impl';\nimport {identity} from './Identity';\n\nexport function concat(\n    args: {inputs: ConcatInputs, attrs: ConcatAttrs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {axis} = attrs;\n\n  const $axis = util.parseAxisParam(axis, inputs[0].shape)[0];\n\n  const shapes = inputs.map(t => t.shape);\n  backend_util.assertParamsConsistent(shapes, $axis);\n\n  const outShape =\n      backend_util.computeOutShape(inputs.map(t => t.shape), $axis);\n  if (util.sizeFromShape(outShape) === 0) {\n    return backend.makeTensorInfo(outShape, inputs[0].dtype, []);\n  }\n\n  // Keep only non-empty tensors (ignore tensors with 0 in their shape).\n  const $inputs = inputs.filter(t => util.sizeFromShape(t.shape) > 0);\n  if ($inputs.length === 1) {\n    return identity({inputs: {x: $inputs[0]}, backend});\n  }\n\n  return concatImpl($inputs, $axis, backend);\n}\n\nexport const concatConfig: KernelConfig = {\n  kernelName: Concat,\n  backendName: 'webgpu',\n  kernelFunc: concat as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet, biasActivationSnippet, typeSnippet} from './activation_util';\nimport {makeMatMulPackedSource, makeMatMulPackedVec4Source} from './matmul_packed_webgpu';\nimport {WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, computeWorkGroupSizeForConv2d, computeWorkPerThreadForConv2d} from './webgpu_util';\n\nfunction conv2dCommonSnippet(\n    isChannelsLast: boolean, fitAOuter: boolean, fitBOuter: boolean,\n    fitInner: boolean, addBias = false,\n    activation: backend_util.Activation = null,\n    hasPreluActivationWeights = false, innerElementSizeX = 4,\n    innerElementSizeW = 4, innerElementSize = 4) {\n  const getXSnippet = (innerElementSize: number) => {\n    switch (innerElementSize) {\n      case 1:\n        return 'resData = x[xIndex];';\n      case 3:\n        return 'resData = vec3<f32>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);';\n      case 4:\n        return 'resData = x[xIndex / 4];';\n      default:\n        throw new Error(\n            `innerElementSize ${innerElementSize} is not supported.`);\n    }\n  };\n  const getWSnippet = (innerElementSize: number) => {\n    switch (innerElementSize) {\n      case 1:\n        return 'return W[row * uniforms.wShape[3] + colIn];';\n      case 4:\n        return 'return W[row * uniforms.wShape[3] / 4 + colIn];';\n      default:\n        throw new Error(\n            `innerElementSize ${innerElementSize} is not supported.`);\n    }\n  };\n  const coordASnippet = isChannelsLast ? `\n      let coord = vec4<i32>(batch, xRow, xCol, xCh);\n      ` :\n                                         `\n      let coord = vec4<i32>(batch, xCh, xRow, xCol);\n      `;\n\n  const coordResSnippet = isChannelsLast ? `\n      let coords = vec4<i32>(\n        batch,\n        row / outWidth,\n        row % outWidth,\n        col);\n      ` :\n                                           `\n      let coords = vec4<i32>(\n        batch,\n        row,\n        col / outWidth,\n        col % outWidth);\n      `;\n\n  const xHight = isChannelsLast ? 'uniforms.xShape[1]' : 'uniforms.xShape[2]';\n  const xWidth = isChannelsLast ? 'uniforms.xShape[2]' : 'uniforms.xShape[3]';\n  const row = isChannelsLast ? 'row' : 'col';\n  const col = isChannelsLast ? 'col' : 'row';\n  const readXSnippet = `\n      let inChannels = uniforms.wShape[2];\n      let outWidth = ${\n      isChannelsLast ? 'uniforms.outShape[2]' : 'uniforms.outShape[3]'};\n      let outRow = ${row} / outWidth;\n      let outCol = ${row} % outWidth;\n\n      let WRow = ${col} / (uniforms.filterDims[1] * inChannels);\n      let WCol = ${col} / inChannels % uniforms.filterDims[1];\n      let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];\n      let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];\n      let xCh = ${col} % inChannels;\n      var resData = ${typeSnippet(innerElementSizeX)}(0.0);\n      // The bounds checking is always needed since we use it to pad zero for\n      // the 'same' padding type.\n      if (xRow >= 0 && xRow < ${xHight} && xCol >= 0 && xCol < ${xWidth}) {\n        ${coordASnippet}\n        let xIndex = getIndexFromCoords4D(coord, uniforms.xShape);\n        ${getXSnippet(innerElementSizeX)}\n      }\n      return resData;`;\n\n  const sampleX = isChannelsLast ? (fitAOuter && fitInner ? `\n      let col = colIn * ${innerElementSizeX};\n      ${readXSnippet}` :\n                                                            `\n      let col = colIn * ${innerElementSizeX};\n      if (row < uniforms.dimAOuter && col < uniforms.dimInner) {\n        ${readXSnippet}\n      }\n      return ${typeSnippet(innerElementSizeX)}(0.0);`) :\n                                   (fitInner && fitBOuter ? `\n      let col = colIn * ${innerElementSizeX};\n      ${readXSnippet}` :\n                                                            `\n      let col = colIn * ${innerElementSizeX};\n      if (row < uniforms.dimInner && col < uniforms.dimBOuter) {\n        ${readXSnippet}\n      }\n      return ${typeSnippet(innerElementSizeX)}(0.0);`);\n\n  const sampleW = `${getWSnippet(innerElementSizeW)}`;\n\n  const resType = typeSnippet(innerElementSize);\n  const aType = isChannelsLast ? typeSnippet(innerElementSizeX) :\n                                 typeSnippet(innerElementSizeW);\n  const bType = isChannelsLast ? typeSnippet(innerElementSizeW) :\n                                 typeSnippet(innerElementSizeX);\n  const userCode = `\n      ${\n      activationFnSnippet(\n          activation, hasPreluActivationWeights, innerElementSize === 4, 4)}\n      fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${aType} {\n        ${isChannelsLast ? sampleX : sampleW}\n      }\n\n      fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${bType} {\n        ${isChannelsLast ? sampleW : sampleX}\n      }\n\n      fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${resType}) {\n        let col = colIn * ${innerElementSize};\n        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)\n        {\n        var value = valueIn;\n        let outWidth = ${\n      isChannelsLast ? 'uniforms.outShape[2]' : 'uniforms.outShape[3]'};\n        ${coordResSnippet}\n        ${biasActivationSnippet(addBias, activation)}\n        setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\n        }\n      }`;\n  return userCode;\n}\n\nexport class Conv2DMMProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  variableTypes: string[];\n  uniforms =\n      `filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>, dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;\n  workGroupSize: [number, number, number];\n  elementsPerThread: [number, number, number];\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivationWeights: boolean;\n  isChannelsLast: boolean;\n  fitAOuter: boolean;\n  fitBOuter: boolean;\n  fitInner: boolean;\n  tileAOuter: number;\n  tileBOuter: number;\n  tileInner: number;\n  innerElementSize: number;\n  isVec4?: boolean;\n  private sequentialAccessByThreads: boolean;\n\n  constructor(\n      convInfo: backend_util.Conv2DInfo, dimAOuter: number, dimBOuter: number,\n      dimInner: number, addBias = false,\n      activation: backend_util.Activation = null,\n      hasPreluActivationWeights = false, sequentialAccessByThreads = false) {\n    this.outputShape = convInfo.outShape;\n    this.isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    this.isVec4 =\n        (((convInfo.inChannels % 4 === 0 || convInfo.inChannels % 3 === 0) &&\n          this.isChannelsLast) ||\n         (convInfo.outWidth % 4 === 0 && !this.isChannelsLast)) &&\n        convInfo.outChannels % 4 === 0;\n    this.dispatchLayout = this.isChannelsLast ? {x: [3], y: [1, 2], z: [0]} :\n                                                {x: [2, 3], y: [1], z: [0]};\n    this.workGroupSize = computeWorkGroupSizeForConv2d(\n        this.dispatchLayout, this.outputShape, this.isVec4);\n    this.elementsPerThread = computeWorkPerThreadForConv2d(\n        this.dispatchLayout, this.outputShape, this.isVec4);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        this.elementsPerThread);\n\n    if (this.isVec4) {\n      if (this.isChannelsLast && convInfo.inChannels % 4 !== 0) {\n        this.innerElementSize = 3;\n        this.variableTypes = ['f32', 'vec4<f32>'];\n      } else {\n        this.innerElementSize = 4;\n        this.variableTypes = ['vec4<f32>', 'vec4<f32>'];\n      }\n\n      if (addBias) {\n        this.variableNames.push('bias');\n        this.variableTypes.push('vec4<f32>');\n      }\n\n      if (hasPreluActivationWeights) {\n        this.variableNames.push('preluActivationWeights');\n        this.variableTypes.push('vec4<f32>');\n      }\n    } else {\n      this.innerElementSize = this.elementsPerThread[0];\n      if (addBias) {\n        this.variableNames.push('bias');\n      }\n\n      if (hasPreluActivationWeights) {\n        this.variableNames.push('preluActivationWeights');\n      }\n    }\n\n    this.sequentialAccessByThreads = sequentialAccessByThreads;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivationWeights = hasPreluActivationWeights;\n\n    this.tileAOuter = this.workGroupSize[1] * this.elementsPerThread[1];\n    this.tileBOuter = this.workGroupSize[0] * this.elementsPerThread[0];\n    this.tileInner = Math.max(\n        this.workGroupSize[0] * this.innerElementSize, this.workGroupSize[1]);\n\n    this.fitAOuter = dimAOuter % this.tileAOuter === 0;\n    this.fitBOuter = dimBOuter % this.tileBOuter === 0;\n    this.fitInner = dimInner % this.tileInner === 0;\n\n    this.shaderKey = `conv2DMM_${this.elementsPerThread}_${this.activation}}_${\n        this.fitAOuter}_${this.fitBOuter}_${this.fitInner}_${this.isVec4}_${\n        this.innerElementSize}_${this.isChannelsLast}_${\n        this.sequentialAccessByThreads}`;\n  }\n\n  getUserCode(): string {\n    const matMulSource = this.isVec4 ?\n        makeMatMulPackedVec4Source(\n            this.elementsPerThread, this.workGroupSize, !this.isChannelsLast,\n            this.tileInner) :\n        makeMatMulPackedSource(\n            this.elementsPerThread, this.workGroupSize, !this.isChannelsLast,\n            this.tileInner, false, null, this.sequentialAccessByThreads);\n    const elementsSize =\n        this.isVec4 ? [this.innerElementSize, 4, 4] : [1, 1, 1];\n    const userCode = `\n    ${\n        conv2dCommonSnippet(\n            this.isChannelsLast, this.fitAOuter, this.fitBOuter, this.fitInner,\n            this.addBias, this.activation, this.hasPreluActivationWeights,\n            elementsSize[0], elementsSize[1], elementsSize[2])}\n    ${matMulSource}\n  `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet, biasActivationSnippet} from './activation_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch} from './webgpu_util';\n\nexport class Conv2DNaiveProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  uniforms =\n      'filterDims: vec2<i32>, pad: vec2<i32>, stride: vec2<i32>, dilation: vec2<i32>,';\n  workGroupSize: [number, number, number] = [4, 4, 8];\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivationWeights: boolean;\n  isChannelsLast: boolean;\n\n  constructor(\n      convInfo: backend_util.Conv2DInfo, addBias = false,\n      activation: backend_util.Activation = null,\n      hasPreluActivationWeights = false) {\n    this.outputShape = convInfo.outShape;\n    this.isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    this.dispatchLayout = this.isChannelsLast ? {x: [2], y: [1], z: [0, 3]} :\n                                                {x: [3], y: [2], z: [0, 1]};\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivationWeights = hasPreluActivationWeights;\n\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n\n    if (hasPreluActivationWeights) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.shaderKey = `conv2dnaive_${this.activation}_${this.isChannelsLast}`;\n  }\n\n  getUserCode(): string {\n    const userCode = `\n       ${\n        activationFnSnippet(\n            this.activation, this.hasPreluActivationWeights, false, 4)}\n       fn readInp(batch : i32, row : i32, col : i32, chan : i32) -> f32{\n         let coords = vec4<i32>(batch, row, col, chan);\n         if (coordsInBounds4D(coords, uniforms.xShape)) {\n           return  getX(batch, row, col, chan);\n         } else {\n          return 0.0;\n         }\n       }\n       fn readFilt(row : i32, col : i32, xChannel : i32, outChannel : i32) -> f32{\n         let coords = vec4<i32>(row, col, xChannel, outChannel);\n         if(coordsInBounds4D(coords, uniforms.wShape)) {\n           return getW(row, col, xChannel, outChannel);\n          } else {\n            return 0.0;\n          }\n       }\n       fn writeResult(batch : i32, row : i32, col : i32, chan : i32, valueIn : f32) {\n         let coords = ${\n        this.isChannelsLast ? `vec4<i32>(batch, row, col, chan);` :\n                              `vec4<i32>(batch, chan, row, col);`}\n         if (coordsInBounds4D(coords, uniforms.outShape)) {\n           var value = valueIn;\n           ${biasActivationSnippet(this.addBias, this.activation)}\n           setOutputAtCoords(coords.x, coords.y, coords.z, coords.w, value);\n         }\n       }\n       ${main('index')} {\n         let coords = getOutputCoords();\n         let batch = coords[0];\n         let outChannel = ${this.isChannelsLast ? `coords[3];` : `coords[1];`}\n         let outRow = ${this.isChannelsLast ? `coords[1];` : `coords[2];`}\n         let outCol = ${this.isChannelsLast ? `coords[2];` : `coords[3];`}\n         var acc : f32 = 0.0;\n         for (var row = 0; row < uniforms.filterDims[0]; row = row + 1) {\n           for (var col = 0; col < uniforms.filterDims[1]; col = col + 1) {\n             let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * row - uniforms.pad[0];\n             let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * col - uniforms.pad[1];\n             for (var xChannel = 0; xChannel < ${\n        this.isChannelsLast ? `uniforms.xShape[3];` :\n                              `uniforms.xShape[1];`} xChannel = xChannel + 1) {\n               ${\n        this.isChannelsLast ? `let v = readInp(batch, xRow, xCol, xChannel);` :\n                              `let v = readInp(batch, xChannel, xRow, xCol);`}\n               let f = readFilt(row, col, xChannel, outChannel);\n               acc = acc + v * f;\n             }\n           }\n         }\n         writeResult(batch, outRow, outCol, outChannel, acc);\n       }\n     `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, env, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {Conv2DMMProgram} from '../conv2d_mm_webgpu';\nimport {Conv2DNaiveProgram} from '../conv2d_naive_webgpu';\nimport {WebGPUProgram} from '../webgpu_program';\n\nimport {batchMatMulImpl} from './BatchMatMul_impl';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: WebGPUBackend,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// conv2dByMatMul fuses height and width into one dimension to compute\n// batchMatMul, so bias and activation weights are also supposed to fuse the two\n// dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\nfunction getShapeForBatchMatMul(\n    shape: number[], isChannelsLast: boolean): number[] {\n  const length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ?\n        [\n          ...shape.slice(0, -3) /* batch */,\n          shape[length - 3] * shape[length - 2] /* height * width */,\n          shape[length - 1] /* channel */\n        ] :\n        [\n          ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,\n          shape[length - 2] * shape[length - 1] /* height * width */\n        ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nfunction conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = isChannelsLast ? false : true;\n  const transposeB = false;\n\n  const sameSize = isChannelsLast &&\n      convInfo.filterHeight === convInfo.inHeight &&\n      convInfo.filterWidth === convInfo.inWidth &&\n      convInfo.padInfo.type === 'VALID';\n  const intermediates: TensorInfo[] = [];\n  let xReshaped;\n  let filterReshaped;\n\n  if (sameSize) {\n    const sharedDim =\n        convInfo.inHeight * convInfo.inWidth * convInfo.inChannels;\n    xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {shape: [1, convInfo.batchSize, sharedDim]}\n    });\n    filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, sharedDim, convInfo.outChannels]}\n    });\n  } else {\n    xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {\n        shape: isChannelsLast ?\n            [\n              convInfo.batchSize, convInfo.inHeight * convInfo.inWidth,\n              convInfo.inChannels\n            ] :\n            [\n              convInfo.batchSize, convInfo.inChannels,\n              convInfo.inHeight * convInfo.inWidth\n            ]\n      }\n    });\n    filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n  }\n  intermediates.push(xReshaped);\n  intermediates.push(filterReshaped);\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  const result = batchMatMulImpl({\n    a: isChannelsLast ? xReshaped : filterReshaped,\n    b: isChannelsLast ? filterReshaped : xReshaped,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    activation,\n    preluActivationWeights,\n    leakyreluAlpha\n  });\n  const out = reshape(\n      {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n  intermediates.push(result);\n\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n\n  return out;\n}\n\nexport function conv2DImpl({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const sameSize = isChannelsLast &&\n      convInfo.filterHeight === convInfo.inHeight &&\n      convInfo.filterWidth === convInfo.inWidth &&\n      convInfo.padInfo.type === 'VALID';\n  const useNaiveConv2d = env().getBool('WEBGPU_USE_NAIVE_CONV2D_DEBUG');\n\n  if (!useNaiveConv2d &&\n      (sameSize ||\n       (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 &&\n        convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&\n        convInfo.strideHeight === 1 && convInfo.strideWidth === 1 &&\n        (convInfo.padInfo.type === 'SAME' ||\n         convInfo.padInfo.type === 'VALID')))) {\n    return conv2dByMatMul({\n      x,\n      filter,\n      convInfo,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n  }\n\n  let program: WebGPUProgram;\n  const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];\n  const dimensions = [\n    {type: 'int32', data: [convInfo.filterHeight, convInfo.filterWidth]},\n    {type: 'int32', data: [...padInfo]},\n    {type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]},\n    {type: 'int32', data: [convInfo.dilationHeight, convInfo.dilationWidth]}\n  ];\n  if (useNaiveConv2d) {\n    program = new Conv2DNaiveProgram(\n        convInfo, hasBias, activation, hasPreluActivationWeights);\n  } else {\n    const dimAOuter = isChannelsLast ? convInfo.outHeight * convInfo.outWidth :\n                                       convInfo.outChannels;\n    const dimBOuter = isChannelsLast ? convInfo.outChannels :\n                                       convInfo.outHeight * convInfo.outWidth;\n    const dimInner =\n        convInfo.filterHeight * convInfo.filterWidth * convInfo.inChannels;\n    dimensions.push(\n        {type: 'int32', data: [dimAOuter]}, {type: 'int32', data: [dimBOuter]},\n        {type: 'int32', data: [dimInner]});\n\n    // Experiments show that sequential access is more friendly for Intel GPUs.\n    const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n    program = new Conv2DMMProgram(\n        convInfo, dimAOuter, dimBOuter, dimInner, hasBias, activation,\n        hasPreluActivationWeights, sequentialAccessByThreads);\n  }\n\n  const intermediates: TensorInfo[] = [];\n  const inputVar: TensorInfo[] = [x, filter];\n  if (hasBias) {\n    if (!isChannelsLast && bias.shape.length === 1) {\n      bias = reshape(\n          {inputs: {x: bias}, backend, attrs: {shape: [bias.shape[0], 1, 1]}});\n      intermediates.push(bias);\n    }\n    inputVar.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    if (!isChannelsLast && preluActivationWeights.shape.length === 1) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: [preluActivationWeights.shape[0], 1, 1]}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n    inputVar.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({type: 'float32', data: [leakyreluAlpha]});\n    program.uniforms += ' alpha : f32,';\n  }\n  const out = backend.runWebGPUProgram(program, inputVar, x.dtype, dimensions);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return out;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv2D, Conv2DAttrs, Conv2DInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {conv2DImpl} from './Conv2D_impl';\n\nexport function conv2d(\n    args: {inputs: Conv2DInputs, attrs: Conv2DAttrs, backend: WebGPUBackend}) {\n  const {inputs, attrs, backend} = args;\n  const {x, filter} = inputs;\n  const {strides, pad, dataFormat, dilations, dimRoundingMode} = attrs;\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, dilations, pad,\n      dimRoundingMode, false /* depthwise */, $dataFormat);\n  return conv2DImpl({x, filter, convInfo, backend});\n}\n\nexport const conv2DConfig: KernelConfig = {\n  kernelName: Conv2D,\n  backendName: 'webgpu',\n  kernelFunc: conv2d as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, util} from '@tensorflow/tfjs-core';\nimport {typeSnippet} from './activation_util';\nimport {makeMatMulPackedSource, makeMatMulPackedVec4Source} from './matmul_packed_webgpu';\nimport {WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, computeWorkGroupSizeForConv2d, computeWorkPerThreadForConv2d} from './webgpu_util';\n\nfunction conv2dTransposeCommonSnippet(innerElementSize = 4) {\n  const getWSnippet = (innerElementSize: number) => {\n    switch (innerElementSize) {\n      case 1:\n        return 'return W[getIndexFromCoords4D(coord, uniforms.wShape)];';\n      case 4:\n        return `\n            let coord1 = vec4<i32>(coordX, coordY, col + 1, rowInner);\n            let coord2 = vec4<i32>(coordX, coordY, col + 2, rowInner);\n            let coord3 = vec4<i32>(coordX, coordY, col + 3, rowInner);\n            let v0 = W[getIndexFromCoords4D(coord, uniforms.wShape)];\n            let v1 = W[getIndexFromCoords4D(coord1, uniforms.wShape)];\n            let v2 = W[getIndexFromCoords4D(coord2, uniforms.wShape)];\n            let v3 = W[getIndexFromCoords4D(coord3, uniforms.wShape)];\n            return vec4<f32>(v0, v1, v2, v3);\n            `;\n      default:\n        throw new Error(\n            `innerElementSize ${innerElementSize} is not supported.`);\n    }\n  };\n\n  const readASnippet = `\n      let outRow = row / uniforms.outShape[2];\n      let outCol = row % uniforms.outShape[2];\n\n      let WRow = col / (uniforms.filterDims[1] * uniforms.outBackprop[3]);\n      let WCol = col / uniforms.outBackprop[3] % uniforms.filterDims[1];\n      let xR = f32(outRow - uniforms.pads[0] + WRow) / f32(uniforms.stride[0]);\n      let xC = f32(outCol - uniforms.pads[1] + WCol) / f32(uniforms.stride[1]);\n      if (xR < 0.0 || xR >= f32(uniforms.outBackprop[1]) || fract(xR) > 0.0) {\n        return ${typeSnippet(innerElementSize)}(0.0);\n      }\n      if (xC < 0.0 || xC >= f32(uniforms.outBackprop[2]) || fract(xC) > 0.0) {\n        return ${typeSnippet(innerElementSize)}(0.0);\n      }\n      let coord = vec4<i32>(\n          batch,\n          i32(xR),\n          i32(xC),\n          col % uniforms.outBackprop[3]);\n      return x[getIndexFromCoords4D(coord, uniforms.xShape)/${\n      innerElementSize}];`;\n\n  const sampleA = `if (row < uniforms.dimAOuter && col < uniforms.dimInner) {\n        ${readASnippet}\n      }\n      return ${typeSnippet(innerElementSize)}(0.0);`;\n\n  const userCode = `\n  fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${\n      typeSnippet(innerElementSize)} {\n    let col = colIn * ${innerElementSize};\n    ${sampleA}\n  }\n\n  fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${\n      typeSnippet(innerElementSize)} {\n    let col = colIn * ${innerElementSize};\n    let coordX = uniforms.filterDims.x - 1 -\n        row / (uniforms.filterDims[1] * uniforms.outBackprop[3]);\n    let coordY = uniforms.filterDims.y - 1 -\n        (row / uniforms.outBackprop[3]) % uniforms.filterDims[1];\n    if (row < uniforms.dimInner && col < uniforms.dimBOuter &&\n        coordX >= 0 && coordY >= 0) {\n      let rowInner = row % uniforms.outBackprop[3];\n      let coord = vec4<i32>(coordX, coordY, col, rowInner);\n      ${getWSnippet(innerElementSize)}\n    }\n    return ${typeSnippet(innerElementSize)}(0.0);\n  }\n\n  fn mm_write(batch: i32, row : i32, colIn : i32, valueInput : ${\n      typeSnippet(innerElementSize)}) {\n    let col = colIn * ${innerElementSize};\n    if (row < uniforms.dimAOuter && (col + ${\n      innerElementSize - 1}) < uniforms.dimBOuter) {\n      var value = valueInput;\n      let outCoord = vec4<i32>(\n          batch,\n          row / uniforms.outShape[2],\n          row % uniforms.outShape[2],\n          col);\n      result[getIndexFromCoords4D(outCoord, uniforms.outShape)/${\n      innerElementSize}] = value;\n    }\n  }`;\n  return userCode;\n}\n\nexport class Conv2DDerInputMMProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  variableTypes: string[];\n  uniforms =\n      'filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,';\n  workGroupSize: [number, number, number];\n  elementsPerThread: [number, number, number];\n  isVec4?: boolean;\n\n  constructor(convInfo: backend_util.Conv2DInfo) {\n    this.outputShape = convInfo.inShape;\n\n    util.assert(\n        convInfo.dataFormat === 'channelsLast',\n        () => 'TODO: NCHW is unimplemented');\n    this.isVec4 =\n        convInfo.inChannels % 4 === 0 && convInfo.outChannels % 4 === 0;\n    this.dispatchLayout = {x: [3], y: [1, 2], z: [0]};\n    this.workGroupSize = computeWorkGroupSizeForConv2d(\n        this.dispatchLayout, this.outputShape, this.isVec4);\n    this.elementsPerThread = computeWorkPerThreadForConv2d(\n        this.dispatchLayout, this.outputShape, this.isVec4);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        this.elementsPerThread);\n\n    if (this.isVec4) {\n      this.variableTypes = ['vec4<f32>', 'f32'];\n    }\n\n    this.shaderKey =\n        `conv2DDerInputMM_${this.isVec4}_${this.elementsPerThread}`;\n  }\n\n  getUserCode(): string {\n    const matMulSource = this.isVec4 ?\n        makeMatMulPackedVec4Source(this.elementsPerThread, this.workGroupSize) :\n        makeMatMulPackedSource(this.elementsPerThread, this.workGroupSize);\n    const userCode = `\n    ${conv2dTransposeCommonSnippet(this.isVec4 ? 4 : 1)}\n    ${matMulSource}\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class Conv2DDerInputProgram implements WebGPUProgram {\n  variableNames = ['dy', 'W'];\n  uniforms =\n      'filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>,';\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  isChannelsLast: boolean;\n  size = true;\n\n  constructor(convInfo: backend_util.Conv2DInfo) {\n    this.outputShape = convInfo.inShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    this.shaderKey = `conv2DDerInput_${this.isChannelsLast}`;\n  }\n\n  getUserCode(): string {\n    const rowDim = this.isChannelsLast ? 1 : 2;\n    const colDim = this.isChannelsLast ? 2 : 3;\n    const channelDim = this.isChannelsLast ? 3 : 1;\n    return `\n    ${main('index')} {\n      if(index < uniforms.size) {\n        let coords = getCoordsFromIndex(index);\n        let batch = coords[0];\n        let d1 = coords[${channelDim}];\n\n        let dyCorner = vec2<i32>(coords[${rowDim}], coords[${\n        colDim}]) - uniforms.pads;\n        let dyRCorner = dyCorner.x;\n        let dyCCorner = dyCorner.y;\n\n        // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).\n        // ? = to be determined. : = across all values in that axis.\n        var dotProd = 0.0;\n        for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + 1) {\n          let dyR = (f32(dyRCorner) + f32(wR)) / f32(uniforms.stride.x);\n          let wRPerm = uniforms.filterDims.x - 1 - wR;\n          if (dyR < 0.0 || dyR >= f32(uniforms.outBackprop[1]) || fract(dyR) > 0.0 ||\n              wRPerm < 0) {\n            continue;\n          }\n          let idyR = i32(dyR);\n\n          for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + 1) {\n            let dyC = (f32(dyCCorner) + f32(wC)) / f32(uniforms.stride.y);\n            let wCPerm = uniforms.filterDims.y - 1 - wC;\n            if (dyC < 0.0 || dyC >= f32(uniforms.outBackprop[2]) ||\n                fract(dyC) > 0.0 || wCPerm < 0) {\n              continue;\n            }\n            let idyC = i32(dyC);\n\n            for (var d2 = 0; d2 < uniforms.outBackprop[3]; d2 = d2 + 1) {\n              if (${this.isChannelsLast}) {\n                let xValue = getDy(batch, idyR, idyC, d2);\n                let wValue = getW(wRPerm, wCPerm, d1, d2);\n                dotProd = dotProd + xValue * wValue;\n              } else {\n                let xValue = getDy(batch, d2, idyR, idyC);\n                let wValue = getW(wRPerm, wCPerm, d1, d2);\n                dotProd = dotProd + xValue * wValue;\n              }\n\n            }\n          }\n        }\n        setOutputAtIndex(index, dotProd);\n      }\n    }\n  `;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Conv2DBackpropInput, Conv2DBackpropInputAttrs, Conv2DBackpropInputInputs, env, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {Conv2DDerInputMMProgram} from '../conv_backprop_mm_webgpu';\nimport {Conv2DDerInputProgram} from '../conv_backprop_webgpu';\n\nexport function conv2DBackpropInput(args: {\n  inputs: Conv2DBackpropInputInputs,\n  attrs: Conv2DBackpropInputAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {dy, filter} = inputs;\n  const {inputShape, strides, pad, dataFormat, dimRoundingMode} = attrs;\n\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      inputShape, filter.shape as [number, number, number, number], strides,\n      1 /* dilations */, pad, dimRoundingMode, false, $dataFormat);\n\n  const dimensions = [\n    {type: 'int32', data: [convInfo.filterHeight, convInfo.filterWidth]},\n    {\n      type: 'int32',\n      data: [\n        convInfo.filterHeight - 1 - convInfo.padInfo.top,\n        convInfo.filterWidth - 1 - convInfo.padInfo.left\n      ]\n    },\n    {type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]},\n    {\n      type: 'int32',\n      data: [\n        convInfo.batchSize, convInfo.outHeight, convInfo.outWidth,\n        convInfo.outChannels\n      ]\n    },\n  ];\n  let program: Conv2DDerInputProgram|Conv2DDerInputMMProgram;\n  // When filter size is small, Conv2DDerInputProgram is much faster than\n  // Conv2DDerInputMMProgram.\n  if (env().getBool('WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE') ||\n      convInfo.filterHeight <= 2 && convInfo.filterWidth <= 2 &&\n          convInfo.outChannels <= 16 && convInfo.inChannels === 1) {\n    program = new Conv2DDerInputProgram(convInfo);\n  } else {\n    program = new Conv2DDerInputMMProgram(convInfo);\n    const dimAOuter = convInfo.inHeight * convInfo.inWidth;\n    const dimBOuter = convInfo.inChannels;\n    const dimInner =\n        convInfo.filterHeight * convInfo.filterWidth * convInfo.outChannels;\n    dimensions.push(\n        {type: 'uint32', data: [dimAOuter]},\n        {type: 'uint32', data: [dimBOuter]},\n        {type: 'uint32', data: [dimInner]});\n  }\n  return backend.runWebGPUProgram(program, [dy, filter], 'float32', dimensions);\n}\n\nexport const conv2DBackpropInputConfig: KernelConfig = {\n  kernelName: Conv2DBackpropInput,\n  backendName: 'webgpu',\n  kernelFunc: conv2DBackpropInput as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport enum CumOpType {\n  Prod = '*',\n  Sum = '+',\n}\n\nexport class CumProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  workGroupSize: [number, number, number];\n  // pow(i32, i32) is not supported, use pow(f32, f32) instead.\n  uniforms = 'index : f32,';\n  size = true;\n  exclusive: boolean;\n  reverse: boolean;\n  op: CumOpType;\n\n  constructor(\n      op: CumOpType, shape: number[], exclusive: boolean, reverse: boolean) {\n    const workGroupSizeX = 128;\n    this.workGroupSize = [workGroupSizeX, 1, 1];\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.exclusive = exclusive;\n    this.reverse = reverse;\n    this.op = op;\n    this.shaderKey = `cum_${this.op}_${this.exclusive}_${this.reverse}`;\n  }\n\n  getUserCode(): string {\n    const rank = this.outputShape.length;\n    const initVal = this.op === CumOpType.Prod ? '1.0' : '0.0';\n    const val = this.exclusive ? initVal :\n                                 `getX(${getCoords(rank, 'coords', this.op)})`;\n    const length = this.outputShape[this.outputShape.length - 1];\n    let condition = '';\n    let idxString = '';\n    // When exclusive is set, the cum op becomes roll op that copies the\n    // value from the previous index based on the direction specified by the\n    // reverse flag.\n    if (this.exclusive) {\n      condition = this.reverse ? `end != ${length - 1}` : 'end != 0';\n      idxString = this.reverse ? 'end + 1' : 'end - 1';\n    } else {\n      condition = this.reverse ? `end + pow2 < ${length}` : 'end >= pow2';\n      idxString = (this.reverse ? 'end + pow2' : 'end - pow2');\n    }\n    return `\n      ${main('index')} {\n       if (index < uniforms.size) {\n         var coords = getCoordsFromIndex(index);\n\n         let end = ${getFinalCoord(rank, 'coords', this.op)};\n         var val = ${val};\n         let pow2 = i32(pow(2.0, uniforms.index));\n         if (${condition}) {\n           let idx = ${idxString};\n           ${getFinalCoord(rank, 'coords', this.op)} = idx;\n           val ${this.op}= getX(${getCoords(rank, 'coords', this.op)});\n         }\n         setOutputAtIndex(index, val);\n       }\n      }\n    `;\n  }\n}\n\nfunction getCoords(rank: number, name: string, op: CumOpType): string {\n  if (rank === 1) {\n    return `${name}`;\n  } else if (rank === 2) {\n    return `${name}.x, ${name}.y`;\n  } else if (rank === 3) {\n    return `${name}.x, ${name}.y, ${name}.z`;\n  } else if (rank === 4) {\n    return `${name}.x, ${name}.y, ${name}.z, ${name}.w`;\n  } else {\n    throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);\n  }\n}\n\nfunction getFinalCoord(rank: number, name: string, op: CumOpType): string {\n  if (rank === 1) {\n    return `${name}`;\n  } else if (rank === 2) {\n    return `${name}.y`;\n  } else if (rank === 3) {\n    return `${name}.z`;\n  } else if (rank === 4) {\n    return `${name}.w`;\n  } else {\n    throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cos, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const cos = unaryKernelFunc({opType: UnaryOpType.COS});\n\nexport const cosConfig: KernelConfig = {\n  kernelName: Cos,\n  backendName: 'webgpu',\n  kernelFunc: cos\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cosh, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const cosh = unaryKernelFunc({opType: UnaryOpType.COSH});\n\nexport const coshConfig: KernelConfig = {\n  kernelName: Cosh,\n  backendName: 'webgpu',\n  kernelFunc: cosh\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class CropAndResizeProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['Image', 'Boxes', 'BoxInd'];\n  uniforms = 'extrapolationValue : f32,';\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  methodId: number;\n  cropHeightBiggerThan1: boolean;\n  cropWidthBiggerThan1: boolean;\n  size = true;\n\n  constructor(\n      channnel: number, boxShape: [number, number], cropSize: [number, number],\n      method: 'bilinear'|'nearest') {\n    const [numBoxes, ] = boxShape;\n    this.outputShape = [numBoxes, cropSize[0], cropSize[1], channnel];\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.methodId = method === 'bilinear' ? 1 : 0;\n    this.cropHeightBiggerThan1 = this.outputShape[1] > 1;\n    this.cropWidthBiggerThan1 = this.outputShape[2] > 1;\n    this.shaderKey = `cropAndResize_${this.methodId}_${\n        this.cropHeightBiggerThan1}_${this.cropWidthBiggerThan1}`;\n  }\n\n  getUserCode(): string {\n    const [inputHeightFloat, inputWidthFloat] =\n        [`f32(uniforms.imageShape[1] - 1)`, `f32(uniforms.imageShape[2] - 1)`];\n\n    const [heightRatio, heightScale, inY] = this.cropHeightBiggerThan1 ?\n        [\n          `(${inputHeightFloat} / f32(uniforms.outShape[1] - 1))`,\n          '(y2-y1) * height_ratio',\n          `y1*${inputHeightFloat} + f32(y)*(height_scale)`,\n        ] :\n        [\n          '0.0',\n          '0.0',\n          `0.5 * (y1+y2) * ${inputHeightFloat}`,\n        ];\n    const [widthRatio, widthScale, inX] = this.cropWidthBiggerThan1 ?\n        [\n          `(${inputWidthFloat} / f32(uniforms.outShape[2] - 1))`,\n          '(x2-x1) * width_ratio',\n          `x1*${inputWidthFloat} + f32(x)*(width_scale)`,\n        ] :\n        [\n          '0.0',\n          '0.0',\n          `0.5 * (x1+x2) * ${inputWidthFloat}`,\n        ];\n\n    // Reference implementation\n    // tslint:disable-next-line:max-line-length\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc\n    const userCode = `\n    ${main('index')} {\n      if (index < uniforms.size) {\n        let coords = getCoordsFromIndex(index);\n        let height_ratio = f32(${heightRatio});\n        let width_ratio = f32(${widthRatio});\n        let b = coords[0];\n        let y = coords[1];\n        let x = coords[2];\n        let d = coords[3];\n        // get box vals\n        let y1 = getBoxes(b, 0);\n        let x1 = getBoxes(b, 1);\n        let y2 = getBoxes(b, 2);\n        let x2 = getBoxes(b, 3);\n        // get image in batch index\n        let bInd = i32(round(getBoxInd(b)));\n        if(bInd < 0 || bInd >= uniforms.outShape[0]) {\n          return;\n        }\n        let height_scale = ${heightScale};\n        let width_scale = ${widthScale};\n        let in_y = ${inY};\n        if( in_y < 0.0 || in_y > ${inputHeightFloat} ) {\n          setOutputAtIndex(index, uniforms.extrapolationValue);\n          return;\n        }\n        let in_x = ${inX};\n        if( in_x < 0.0 || in_x > ${inputWidthFloat} ) {\n          setOutputAtIndex(index, uniforms.extrapolationValue);\n          return;\n        }\n        let sourceFracIndexCR = vec2<f32>(in_x,in_y);\n        if(${this.methodId} == 1) {\n          // Compute the four integer indices.\n          let sourceFloorCR = vec2<i32>(sourceFracIndexCR);\n          let sourceCeilCR = vec2<i32>(ceil(sourceFracIndexCR));\n          let topLeft = getImage(bInd, sourceFloorCR.y, sourceFloorCR.x, d);\n          let bottomLeft = getImage(bInd, sourceCeilCR.y, sourceFloorCR.x, d);\n          let topRight = getImage(bInd, sourceFloorCR.y, sourceCeilCR.x, d);\n          let bottomRight = getImage(bInd, sourceCeilCR.y, sourceCeilCR.x, d);\n          let fracCR = sourceFracIndexCR - vec2<f32>(sourceFloorCR);\n          let top = topLeft + (topRight - topLeft) * fracCR.x;\n          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracCR.x;\n          let newValue = top + (bottom - top) * fracCR.y;\n          setOutputAtIndex(index, newValue);\n        } else {\n          // Compute the coordinators of nearest neighbor point.\n          let sourceNearestCR = vec2<i32>(floor(\n            sourceFracIndexCR + vec2<f32>(0.5,0.5)));\n          let newValue = getImage(\n            bInd, sourceNearestCR.y, sourceNearestCR.x, d);\n          setOutputAtIndex(index, newValue);\n        }\n      }\n    }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {CropAndResize, CropAndResizeAttrs, CropAndResizeInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {CropAndResizeProgram} from '../crop_and_resize_webgpu';\n\nexport const cropAndResize = (args: {\n  inputs: CropAndResizeInputs,\n  backend: WebGPUBackend,\n  attrs: CropAndResizeAttrs\n}): TensorInfo => {\n  const {inputs, backend, attrs} = args;\n  const {image, boxes, boxInd} = inputs;\n  const {cropSize, method, extrapolationValue} = attrs;\n\n  const program = new CropAndResizeProgram(\n      image.shape[3], boxes.shape as [number, number], cropSize, method);\n  const uniformData = [{type: 'float32', data: [extrapolationValue]}];\n  return backend.runWebGPUProgram(\n      program, [image, boxes, boxInd], 'float32', uniformData);\n};\n\nexport const cropAndResizeConfig: KernelConfig = {\n  kernelName: CropAndResize,\n  backendName: 'webgpu',\n  kernelFunc: cropAndResize as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {CumOpType, CumProgram} from '../cum_webgpu';\n\nimport {identity} from './Identity';\nimport {transpose} from './Transpose';\n\nexport function cumImpl(\n    op: CumOpType, x: TensorInfo, backend: WebGPUBackend, axis: number,\n    exclusive: boolean, reverse: boolean): TensorInfo {\n  const xRank = x.shape.length;\n  const permutation = backend_util.getAxesPermutation([axis], xRank);\n  let permutedX = x;\n  if (permutation != null) {\n    permutedX = transpose({inputs: {x}, backend, attrs: {perm: permutation}});\n  }\n  const permutedAxis = backend_util.getInnerMostAxes(1, xRank)[0];\n\n  if (permutedAxis !== xRank - 1) {\n    throw new Error(\n        `WebGPU cumprod shader expects an inner-most axis=${\n            x.shape.length - 1} ` +\n        `but got axis=${axis}`);\n  }\n  const size = permutedX.shape[permutedAxis];\n  let result = identity({inputs: {x: permutedX}, backend});\n  // Use cum parallel algorithm, inspired by:\n  // https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda\n  // Note: although the algorithm is called sum, it works for any associtative\n  // operator with an identity.\n\n  for (let i = 0; i <= Math.ceil(Math.log2(size)) - 1; i++) {\n    const program = new CumProgram(op, permutedX.shape, false, reverse);\n    const prevResult = result;\n    const uniformData = [{type: 'float32', data: [i]}];\n    result =\n        backend.runWebGPUProgram(program, [result], result.dtype, uniformData);\n    backend.disposeData(prevResult.dataId);\n  }\n  // For exclusive cum, shift the end result in the direction of product or sum\n  // and add 1 for product or 0 for sum to the front index.\n  if (exclusive) {\n    const program = new CumProgram(op, permutedX.shape, exclusive, reverse);\n    const prevResult = result;\n    const uniformData = [{type: 'float32', data: [0]}];\n    result =\n        backend.runWebGPUProgram(program, [result], result.dtype, uniformData);\n    backend.disposeData(prevResult.dataId);\n  }\n\n  if (permutation != null) {\n    const reversePermutation = backend_util.getUndoAxesPermutation(permutation);\n    const reverseTransposedResult = transpose(\n        {inputs: {x: result}, backend, attrs: {perm: reversePermutation}});\n\n    backend.disposeData(result.dataId);\n    backend.disposeData(permutedX.dataId);\n\n    return reverseTransposedResult;\n  }\n\n  return result;\n}\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cumprod, CumprodAttrs, CumprodInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {CumOpType} from '../cum_webgpu';\nimport {cumImpl} from './Cum_impl';\n\nexport function cumprod(\n    args: {inputs: CumprodInputs, backend: WebGPUBackend, attrs: CumprodAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, exclusive, reverse} = attrs;\n  return cumImpl(CumOpType.Prod, x, backend, axis, exclusive, reverse);\n}\n\nexport const cumprodConfig: KernelConfig = {\n  kernelName: Cumprod,\n  backendName: 'webgpu',\n  kernelFunc: cumprod as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Cumsum, CumsumAttrs, CumsumInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {CumOpType} from '../cum_webgpu';\nimport {cumImpl} from './Cum_impl';\n\nexport function cumsum(\n    args: {inputs: CumsumInputs, backend: WebGPUBackend, attrs: CumsumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, exclusive, reverse} = attrs;\n  return cumImpl(CumOpType.Sum, x, backend, axis, exclusive, reverse);\n}\n\nexport const cumsumConfig: KernelConfig = {\n  kernelName: Cumsum,\n  backendName: 'webgpu',\n  kernelFunc: cumsum as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class DepthToSpaceProgram implements WebGPUProgram {\n  variableNames = ['x'];\n  outputShape: number[];\n  dataFormat: string;\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n  uniforms = 'blockSize : i32,';\n\n  constructor(outputShape: number[], dataFormat: 'NHWC'|'NCHW') {\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.shaderKey = `depthToSpace_${dataFormat}`;\n    this.dataFormat = dataFormat;\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getCoordsFromIndex(index);\n          let b = coords[0];\n          let h = ${this.getHeightCoordString()};\n          let w = ${this.getWidthCoordString()};\n          let d = ${this.getDepthCoordString()};\n\n          let in_h = h / uniforms.blockSize;\n          let offset_h = h % uniforms.blockSize;\n          let in_w = w / uniforms.blockSize;\n          let offset_w = w % uniforms.blockSize;\n          let offset_d = (offset_h * uniforms.blockSize + offset_w) *\n            ${this.getOutputDepthSize()};\n          let in_d = d + offset_d;\n\n          let rlt = ${this.getInputSamplingString()};\n          setOutputAtIndex(index, rlt);\n        }\n      }`;\n    return userCode;\n  }\n\n  private getHeightCoordString(): string {\n    if (this.dataFormat === 'NHWC') {\n      return `coords[1]`;\n    } else {\n      return `coords[2]`;\n    }\n  }\n\n  private getWidthCoordString(): string {\n    if (this.dataFormat === 'NHWC') {\n      return `coords[2]`;\n    } else {\n      return `coords[3]`;\n    }\n  }\n\n  private getDepthCoordString(): string {\n    if (this.dataFormat === 'NHWC') {\n      return `coords[3]`;\n    } else {\n      return `coords[1]`;\n    }\n  }\n\n  private getOutputDepthSize(): string {\n    if (this.dataFormat === 'NHWC') {\n      return `uniforms.outShape[3]`;\n    } else {\n      return `uniforms.outShape[1]`;\n    }\n  }\n\n  private getInputSamplingString(): string {\n    if (this.dataFormat === 'NHWC') {\n      return `getX(b, in_h, in_w, in_d)`;\n    } else {\n      return `getX(b, in_d, in_h, in_w)`;\n    }\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DepthToSpace, DepthToSpaceAttrs, DepthToSpaceInputs, KernelConfig, KernelFunc, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {DepthToSpaceProgram} from '../depth_to_space_webgpu';\n\nexport function depthToSpace(args: {\n  inputs: DepthToSpaceInputs,\n  backend: WebGPUBackend,\n  attrs: DepthToSpaceAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockSize, dataFormat} = attrs;\n\n  const batchSize = x.shape[0];\n  const inputHeight = (dataFormat === 'NHWC') ? x.shape[1] : x.shape[2];\n  const inputWidth = (dataFormat === 'NHWC') ? x.shape[2] : x.shape[3];\n  const inputDepth = (dataFormat === 'NHWC') ? x.shape[3] : x.shape[1];\n\n  const outputHeight = inputHeight * blockSize;\n  const outputWidth = inputWidth * blockSize;\n  const outputDepth = inputDepth / (blockSize * blockSize);\n\n  const outputShape = (dataFormat === 'NHWC') ?\n      [batchSize, outputHeight, outputWidth, outputDepth] :\n      [batchSize, outputDepth, outputHeight, outputWidth];\n\n  const uniformData = [\n    {type: 'int32', data: [blockSize]},\n  ];\n\n  const program = new DepthToSpaceProgram(outputShape, dataFormat);\n  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n}\n\nexport const depthToSpaceConfig: KernelConfig = {\n  kernelName: DepthToSpace,\n  backendName: 'webgpu',\n  kernelFunc: depthToSpace as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet, biasActivationSnippet} from './activation_util';\nimport {getWorkGroupSizeString, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch} from './webgpu_util';\n\nexport class DepthwiseConv2DNCHWSharedProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  uniforms = `pad : vec2<i32>, inDims : vec2<i32>,`;\n  workGroupSize: [number, number, number] = [16, 16, 1];\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivation: boolean;\n  filterHeight: number;\n  filterWidth: number;\n\n  constructor(\n      outputShape: number[], filterHeight: number, filterWidth: number,\n      addBias = false, activation: backend_util.Activation = null,\n      hasPreluActivation = false) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = {x: [3], y: [2], z: [0, 1]};\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n    if (hasPreluActivation) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivation = hasPreluActivation;\n    this.filterHeight = filterHeight;\n    this.filterWidth = filterWidth;\n    this.shaderKey = `depthwiseNCHW_${this.activation}_${this.filterHeight}_${\n        this.filterWidth}`;\n  }\n\n  getUserCode(): string {\n    const filterSize = this.filterWidth * this.filterHeight;\n    const workGroupSize =\n        this.workGroupSize[0] * this.workGroupSize[1] * this.workGroupSize[2];\n    const tileAHeight = this.workGroupSize[1] + this.filterHeight - 1;\n    const tileAWidth = this.workGroupSize[0] + this.filterWidth - 1;\n\n    const userCode = `\n      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}\n\n      var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHeight}>;\n      var<workgroup> mm_Bsub : array<array<f32, ${this.filterWidth}>, ${\n        this.filterHeight}>;\n      fn readX(batch : i32, channel : i32, row : i32, col : i32) -> f32 {\n        var value = 0.0;\n        if (row >=0 && row < uniforms.inDims[0] && col >=0 && col < uniforms.inDims[1])\n        {\n          value = getX(batch, channel, row, col);\n        }\n        return value;\n      }\n\n      ${getWorkGroupSizeString()}\n      fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,\n                @builtin(global_invocation_id) GlobalId : vec3<u32>,\n                @builtin(local_invocation_index) LocalIndex: u32,\n                @builtin(num_workgroups) NumWorkgroups: vec3<u32>) {\n        localId = LocalId;\n        globalId = GlobalId;\n        let localIndex = i32(LocalIndex);\n        numWorkgroups = NumWorkgroups;\n        let coords = getOutputCoords();\n        let batch = coords[0];\n        let xRCCorner = vec2<i32>(coords.zw) - uniforms.pad;\n        let channelMul = uniforms.wShape[3];\n        let d1 = coords[1] / channelMul;\n        let q = coords[1] % channelMul;\n\n        let inputRowStart = xRCCorner.x;\n        let inputColStart = xRCCorner.y;\n\n        let localRow = i32(localId.y);\n        let localCol = i32(localId.x);\n\n        // Load one tile of X into local memory.\n        for (var inputRow = localRow; inputRow < ${\n        tileAHeight}; inputRow = inputRow + ${this.workGroupSize[1]}) {\n          for (var inputCol = localCol; inputCol < ${\n        tileAWidth}; inputCol = inputCol + ${this.workGroupSize[0]}) {\n            let rowOffset = inputRow - localRow;\n            let colOffset = inputCol - localCol;\n            mm_Asub[inputRow][inputCol] = readX(batch, d1, inputRowStart + rowOffset, inputColStart + colOffset);\n          }\n        }\n\n        // Load one tile of W into local memory.\n        var wIndex = localIndex;\n        ${\n        filterSize < workGroupSize ?\n            `if (wIndex < ${filterSize})` :\n            `for(; wIndex < ${filterSize}; wIndex = wIndex + ${workGroupSize})`}\n\n        {\n          let wRow = wIndex / ${this.filterWidth};\n          let wCol = wIndex % ${this.filterWidth};\n          mm_Bsub[wRow][wCol] = getW(wRow, wCol, d1, q);\n        }\n\n        workgroupBarrier();\n\n        var value = 0.0;\n        for (var wR = 0; wR < ${this.filterHeight}; wR = wR + 1) {\n          for (var wC = 0; wC < ${this.filterWidth}; wC = wC + 1) {\n            let xVal = mm_Asub[localRow + wR][localCol + wC];\n            let wVal = mm_Bsub[wR][wC];\n            value = fma(xVal, wVal, value);\n          }\n        }\n        ${biasActivationSnippet(this.addBias, this.activation)}\n        if (coordsInBounds4D(coords, uniforms.outShape)) {\n          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, util} from '@tensorflow/tfjs-core';\nimport {activationFnSnippet, biasActivationSnippet} from './activation_util';\nimport {getWorkGroupSizeString, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch} from './webgpu_util';\n\nexport class DepthwiseConv2DVec4Program implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y: number[], z: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  uniforms = 'pad : vec2<i32>, inDims : vec2<i32>,';\n  workGroupSize: [number, number, number] = [4, 4, 4];\n  workPerThread = 4;\n  convInfo: backend_util.Conv2DInfo;\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivation: boolean;\n  isVec4 = true;\n\n  constructor(\n      convInfo: backend_util.Conv2DInfo, addBias = false,\n      activation: backend_util.Activation = null, hasPreluActivation = false) {\n    this.outputShape = convInfo.outShape;\n    this.dispatchLayout = {x: [3], y: [2], z: [0, 1]};\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [4, this.workPerThread, 1]);\n\n    util.assert(\n        convInfo.dataFormat === 'channelsLast',\n        () => 'TODO: NCHW is unimplemented');\n\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n    if (hasPreluActivation) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.convInfo = convInfo;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivation = hasPreluActivation;\n\n    this.shaderKey =\n        `depthwiseVec4_${activation}_${this.convInfo.filterHeight}_${\n            this.convInfo.filterWidth}_${this.convInfo.strideHeight}_${\n            this.convInfo.strideWidth}_${this.workPerThread}`;\n  }\n\n  getUserCode(): string {\n    const xNumber = (this.workPerThread - 1) * this.convInfo.strideWidth +\n        this.convInfo.filterWidth;\n\n    const userCode = `\n      ${activationFnSnippet(this.activation, this.hasPreluActivation, true, 4)}\n      fn readX(batch : i32, row : i32, col : i32, channel : i32) -> vec4<f32> {\n        var value = vec4<f32>(0.0);\n        if (col >=0 && col < uniforms.inDims[1]) {\n          value = getX(batch, row, col, channel);\n        }\n        return value;\n      }\n\n      const strideHeight = ${this.convInfo.strideHeight};\n      const strideWidth = ${this.convInfo.strideWidth};\n      ${getWorkGroupSizeString()}\n      fn _start(@builtin(global_invocation_id) globalId: vec3<u32>) {\n        let batch = i32(globalId.z) / uniforms.outShape[1];\n        let r = i32(globalId.z) % uniforms.outShape[1];\n        let c = i32(globalId.y) * ${this.workPerThread};\n        let d1 = i32(globalId.x) * 4;\n        let xRCCorner = vec2<i32>(r, c) * vec2<i32>(strideHeight, strideWidth) - uniforms.pad;\n\n        let xRCorner = xRCCorner.x;\n        let xCCorner = xRCCorner.y;\n        var xVals : array<vec4<f32>, ${xNumber}>;\n        var dotProd : array<vec4<f32>, ${this.workPerThread}>;\n        for (var i = 0; i < ${this.workPerThread}; i++) {\n          dotProd[i] = vec4<f32>(0.0);\n        }\n\n        // Use constant instead of uniform can give better performance.\n        for (var wR = 0; wR < ${this.convInfo.filterHeight}; wR = wR + 1) {\n          let xR = xRCorner + wR;\n          if (xR >=0 && xR < uniforms.inDims[0]) {\n            for (var i = 0; i < ${xNumber}; i++) {\n              xVals[i] = readX(batch, xR, xCCorner + i, d1);\n            }\n            for (var wC = 0; wC < ${this.convInfo.filterWidth}; wC = wC + 1) {\n              let wValue = getW(wR, wC, d1, 0);\n              for (var i = 0; i < ${this.workPerThread}; i++) {\n                dotProd[i] = fma(xVals[i * strideWidth + wC], wValue, dotProd[i]);\n              }\n            }\n          }\n        }\n\n        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {\n          let coords = vec4<i32>(batch, r, c + i, d1);\n          if (coordsInBounds4D(coords, uniforms.outShape)) {\n            var value = dotProd[i];\n            ${biasActivationSnippet(this.addBias, this.activation)}\n            setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\n\nimport {activationFnSnippet, biasActivationSnippet} from './activation_util';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class DepthwiseConv2DProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y?: number[], z?: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'W'];\n  uniforms = `pad : vec2<i32>, inDims : vec2<i32>, filterHeight : i32,\n      filterWidth : i32, stride : vec2<i32>, dilation : vec2<i32>,`;\n  // This is an experimental value.\n  workGroupSize: [number, number, number] = [256, 1, 1];\n  convInfo: backend_util.Conv2DInfo;\n  addBias: boolean;\n  activation: backend_util.Activation;\n  hasPreluActivation: boolean;\n  isChannelsLast: boolean;\n\n  constructor(\n      convInfo: backend_util.Conv2DInfo, addBias = false,\n      activation: backend_util.Activation = null, hasPreluActivation = false) {\n    this.outputShape = convInfo.outShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.isChannelsLast = convInfo.dataFormat === 'channelsLast';\n\n    if (addBias) {\n      this.variableNames.push('bias');\n    }\n    if (hasPreluActivation) {\n      this.variableNames.push('preluActivationWeights');\n    }\n\n    this.convInfo = convInfo;\n    this.addBias = addBias;\n    this.activation = activation;\n    this.hasPreluActivation = hasPreluActivation;\n    this.shaderKey = `depthwise_${this.activation}_${this.isChannelsLast}`;\n  }\n\n  getUserCode(): string {\n    const getXSnippet = this.isChannelsLast ? 'getX(batch, xR, xC, d1);' :\n                                              'getX(batch, d1, xR, xC);';\n\n    const userCode = `\n      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}\n\n      ${main()} {\n        let coords = getOutputCoords();\n        let batch = coords[0];\n        let xRCCorner = vec2<i32>(coords.${\n        this.isChannelsLast ? 'yz' : 'zw'}) * uniforms.stride - uniforms.pad;\n        let d2 = coords[${this.isChannelsLast ? 3 : 1}];\n        let channelMul = uniforms.wShape[3];\n        let d1 = d2 / channelMul;\n        let q = d2 % channelMul;\n\n        let inputRowStart = xRCCorner.x;\n        let inputColStart = xRCCorner.y;\n        let inputRowEnd = inputRowStart + uniforms.filterHeight *\n            uniforms.dilation[0];\n        let inputColEnd = inputColStart + uniforms.filterWidth *\n            uniforms.dilation[1];\n\n        // Convolve x(?, ?, d1)|x(d1, ?, ?) with w(:, :, d1, q) to get\n        // y(yR, yC, d2)|y(d2, yR, yC). ? = to be determined. : = across all\n        // values in that axis. x(?, ?, d1) and y(yR, yC, d2) is for NHWC.\n        // x(d1, ?, ?) and y(d2, yR, yC) is for NCHW.\n        var value = 0.0;\n\n        // Extract if checking out of for loop for performance.\n        if (inputRowStart >= 0 && inputColStart >= 0 &&\n          inputRowEnd < uniforms.inDims[0] &&\n              inputColEnd < uniforms.inDims[1]) {\n            for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {\n              let xR = inputRowStart + wR * uniforms.dilation[0];\n\n              for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {\n                let xC = inputColStart + wC * uniforms.dilation[1];\n\n                let xVal = ${getXSnippet};\n                let wVal = getW(wR, wC, d1, q);\n                value = value + xVal * wVal;\n              }\n            }\n          } else {\n            for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {\n              let xR = inputRowStart + wR * uniforms.dilation[0];\n\n              if (xR < 0 || xR >= uniforms.inDims[0]) {\n                continue;\n              }\n\n              for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {\n                let xC = inputColStart + wC * uniforms.dilation[1];\n\n                if (xC < 0 || xC >= uniforms.inDims[1]) {\n                  continue;\n                }\n\n                let xVal = ${getXSnippet};\n                let wVal = getW(wR, wC, d1, q);\n                value = value + xVal * wVal;\n              }\n            }\n          }\n          ${biasActivationSnippet(this.addBias, this.activation)}\n        if (coordsInBounds4D(coords, uniforms.outShape)) {\n          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DepthwiseConv2dNative, DepthwiseConv2dNativeAttrs, DepthwiseConv2dNativeInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {DepthwiseConv2DNCHWSharedProgram} from '../depthwise_conv2d_nchw_shared_webgpu';\nimport {DepthwiseConv2DVec4Program} from '../depthwise_conv2d_vec4_webgpu';\nimport {DepthwiseConv2DProgram} from '../depthwise_conv2d_webgpu';\n\nexport function depthwiseConv2dNative(args: {\n  inputs: DepthwiseConv2dNativeInputs,\n  attrs: DepthwiseConv2dNativeAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {x, filter} = inputs;\n  const {strides, pad, dataFormat, dilations, dimRoundingMode} = attrs;\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  let $dilations = dilations;\n  if ($dilations == null) {\n    $dilations = [1, 1];\n  }\n\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, $dilations,\n      pad, dimRoundingMode, true /* depthwise */, $dataFormat);\n  const dimensions = [\n    {type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left]},\n    {type: 'int32', data: [convInfo.inHeight, convInfo.inWidth]},\n  ];\n\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  let program: DepthwiseConv2DProgram|DepthwiseConv2DVec4Program|\n      DepthwiseConv2DNCHWSharedProgram;\n  if (!isChannelsLast && convInfo.inHeight > 16 && convInfo.inWidth > 16 &&\n      convInfo.strideHeight === 1 && convInfo.strideWidth === 1 &&\n      convInfo.dilationWidth === 1 && convInfo.dilationHeight === 1 &&\n      convInfo.inChannels === convInfo.outChannels) {\n    program = new DepthwiseConv2DNCHWSharedProgram(\n        convInfo.outShape, convInfo.filterHeight, convInfo.filterWidth);\n  } else if (\n      isChannelsLast && convInfo.inHeight > 4 && convInfo.inWidth > 4 &&\n      convInfo.strideWidth <= 2 &&\n      convInfo.inChannels === convInfo.outChannels &&\n      convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&\n      convInfo.inChannels % 4 === 0) {\n    program = new DepthwiseConv2DVec4Program(convInfo);\n  } else {\n    program = new DepthwiseConv2DProgram(convInfo);\n    dimensions.push(\n        {type: 'int32', data: [convInfo.filterHeight]},\n        {type: 'int32', data: [convInfo.filterWidth]},\n        {type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]}, {\n          type: 'int32',\n          data: [convInfo.dilationHeight, convInfo.dilationWidth]\n        });\n  }\n\n  return backend.runWebGPUProgram(program, [x, filter], x.dtype, dimensions);\n}\n\nexport const depthwiseConv2dNativeConfig: KernelConfig = {\n  kernelName: DepthwiseConv2dNative,\n  backendName: 'webgpu',\n  kernelFunc: depthwiseConv2dNative as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Multiply} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {multiplyImplCPU as cpuMultiply} from '../kernel_utils/shared';\n\nexport const multiplyKernelFunc = binaryKernelFunc({\n  opType: BinaryOpType.MUL,\n  cpuKernelImpl: cpuMultiply,\n  supportsComplex: true\n});\n\nexport const multiplyConfig: KernelConfig = {\n  kernelName: Multiply,\n  backendName: 'webgpu',\n  kernelFunc: multiplyKernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Sum, SumAttrs, SumInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reduce} from '../kernel_utils/reduce';\n\nexport function sum(\n    args: {inputs: SumInputs, backend: WebGPUBackend, attrs: SumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  return reduce(x, axis, keepDims, 'sum', backend);\n}\n\nexport const sumConfig: KernelConfig = {\n  kernelName: Sum,\n  backendName: 'webgpu',\n  kernelFunc: sum as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Einsum, EinsumAttrs, EinsumInputs, KernelConfig, KernelFunc, Tensor, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {multiplyKernelFunc} from './Multiply';\nimport {reshape} from './Reshape';\nimport {sum} from './Sum';\nimport {transpose} from './Transpose';\n\nexport function einsum(\n    args: {inputs: EinsumInputs, backend: WebGPUBackend, attrs: EinsumAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {equation} = attrs;\n  const tensors = inputs as Tensor[];\n\n  const {allDims, summedDims, idDims} =\n      backend_util.decodeEinsumEquation(equation, tensors.length);\n  backend_util.checkEinsumDimSizes(allDims.length, idDims, tensors);\n  const {path, steps} = backend_util.getEinsumComputePath(summedDims, idDims);\n\n  const nSteps = steps.length;\n  let out: TensorInfo|null = null;\n  let numDimsRemaining = allDims.length;\n  const tensorsToDispose: TensorInfo[] = [];\n  for (let i = 0; i < nSteps; ++i) {\n    for (const idTerm of steps[i]) {\n      const {permutationIndices: perm, expandDims: dimsToExpand} =\n          backend_util.getEinsumPermutation(numDimsRemaining, idDims[idTerm]);\n      let x: TensorInfo;\n      if (backend_util.isIdentityPermutation(perm)) {\n        x = tensors[idTerm];\n      } else {\n        x = transpose({inputs: {x: tensors[idTerm]}, backend, attrs: {perm}});\n        tensorsToDispose.push(x);\n      }\n      const targetShape: number[] = x.shape.slice();\n      for (let k = 0; k < dimsToExpand.length; ++k) {\n        targetShape.splice(dimsToExpand[k], 0, 1);\n      }\n\n      if (!util.arraysEqual(x.shape, targetShape)) {\n        x = reshape({inputs: {x}, backend, attrs: {shape: targetShape}});\n        tensorsToDispose.push(x);\n      }\n      if (out === null) {\n        out = x;\n      } else {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        out =\n            multiplyKernelFunc({inputs: {a: x, b: out}, backend}) as TensorInfo;\n        tensorsToDispose.push(out);\n      }\n    }\n    if (i < nSteps - 1) {\n      if (path[i] >= 0) {\n        out = sum({\n          inputs: {x: out},\n          backend,\n          attrs: {\n            axis: path[i] - (allDims.length - numDimsRemaining),\n            keepDims: false\n          }\n        });\n        tensorsToDispose.push(out);\n      }\n      numDimsRemaining--;\n    }\n  }\n\n  // Clean up intermediate tensors.\n  for (const tensorInfo of tensorsToDispose) {\n    if (tensorInfo === out) {\n      continue;\n    }\n    backend.disposeData(tensorInfo.dataId);\n  }\n\n  return out;\n}\n\nexport const einsumConfig: KernelConfig = {\n  kernelName: Einsum,\n  backendName: 'webgpu',\n  kernelFunc: einsum as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Elu, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const elu = unaryKernelFunc({opType: UnaryOpType.ELU});\n\nexport const eluConfig: KernelConfig = {\n  kernelName: Elu,\n  backendName: 'webgpu',\n  kernelFunc: elu\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Equal, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {equalImplCPU as cpuEqual} from '../kernel_utils/shared';\n\nexport const equal = binaryKernelFunc(\n    {opType: BinaryOpType.EQUAL, dtype: 'bool', cpuKernelImpl: cpuEqual});\n\nexport const equalConfig: KernelConfig = {\n  kernelName: Equal,\n  backendName: 'webgpu',\n  kernelFunc: equal\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Exp, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {expImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const exp = unaryKernelFunc({\n  opType: UnaryOpType.EXP,\n  cpuKernelImpl: expImplCPU,\n  dtype: 'float32',\n});\n\nexport const expConfig: KernelConfig = {\n  kernelName: Exp,\n  backendName: 'webgpu',\n  kernelFunc: exp\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the License);\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an AS IS BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ExpandDims, ExpandDimsAttrs, ExpandDimsInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reshape} from './Reshape';\n\nexport function expandDims(args: {\n  inputs: ExpandDimsInputs,\n  attrs: ExpandDimsAttrs,\n  backend: WebGPUBackend\n}): TensorInfo {\n  const {inputs, attrs, backend} = args;\n  const {dim} = attrs;\n  const {input} = inputs;\n\n  const inputRank = input.shape.length;\n  const newShape = input.shape.slice();\n  let $dim = dim;\n  if (dim < 0) {\n    // Negative value is counted from the tail of rank.\n    util.assert(\n        -(inputRank + 1) <= dim,\n        () => `Axis must be in the interval [${- (inputRank + 1)}, ${\n            inputRank}]`);\n    $dim = inputRank + dim + 1;\n  }\n  newShape.splice($dim, 0, 1);\n\n  return reshape({inputs: {x: input}, backend, attrs: {shape: newShape}});\n}\n\nexport const expandDimsConfig: KernelConfig = {\n  kernelName: ExpandDims,\n  backendName: 'webgpu',\n  kernelFunc: expandDims as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use backend file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {env, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\nimport {FromPixels, FromPixelsAttrs, FromPixelsInputs, util} from '@tensorflow/tfjs-core';\nimport {backend_util, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {TextureInfo, WebGPUBackend} from '../backend_webgpu';\nimport {FromPixelsProgram} from '../from_pixels_webgpu';\n\nexport const fromPixelsConfig: KernelConfig = {\n  kernelName: FromPixels,\n  backendName: 'webgpu',\n  kernelFunc: fromPixels as {} as KernelFunc,\n};\n\nlet fromPixels2DContext: CanvasRenderingContext2D;\nlet willReadFrequently = env().getBool('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU');\nconst videoToTextureMap = new Map<object, object>();\n\nexport function fromPixels(args: {\n  inputs: FromPixelsInputs,\n  backend: WebGPUBackend,\n  attrs: FromPixelsAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  let {pixels} = inputs;\n  const {numChannels} = attrs;\n\n  if (pixels == null) {\n    throw new Error('pixels passed to tf.browser.fromPixels() can not be null');\n  }\n\n  const isVideo = typeof (HTMLVideoElement) !== 'undefined' &&\n      pixels instanceof HTMLVideoElement;\n  const isImage = typeof (HTMLImageElement) !== 'undefined' &&\n      pixels instanceof HTMLImageElement;\n  const isCanvas = (typeof (HTMLCanvasElement) !== 'undefined' &&\n                    pixels instanceof HTMLCanvasElement) ||\n      (typeof (OffscreenCanvas) !== 'undefined' &&\n       pixels instanceof OffscreenCanvas);\n  const isImageBitmap =\n      typeof (ImageBitmap) !== 'undefined' && pixels instanceof ImageBitmap;\n\n  const [width, height] = isVideo ?\n      [\n        (pixels as HTMLVideoElement).videoWidth,\n        (pixels as HTMLVideoElement).videoHeight\n      ] :\n      [pixels.width, pixels.height];\n  const outputShape = [height, width, numChannels];\n\n  // Disable importExternalTexture temporarily as it has problem in spec and\n  // browser impl\n  const importVideo =\n      false && env().getBool('WEBGPU_IMPORT_EXTERNAL_TEXTURE') && isVideo;\n  const isVideoOrImage = isVideo || isImage;\n  if (isImageBitmap || isCanvas || isVideoOrImage) {\n    let textureInfo: TextureInfo;\n    if (importVideo) {\n      const videoElement = pixels as HTMLVideoElement;\n      if (!(videoToTextureMap.has(videoElement)) ||\n          (videoToTextureMap.get(videoElement) as GPUExternalTexture).expired) {\n        const externalTextureDescriptor = {source: videoElement};\n        videoToTextureMap.set(\n            videoElement,\n            backend.device.importExternalTexture(externalTextureDescriptor));\n      }\n\n      textureInfo = {\n        width,\n        height,\n        format: null,\n        usage: null,\n        texture: videoToTextureMap.get(videoElement) as GPUExternalTexture\n      };\n    } else {\n      if (isVideoOrImage) {\n        const newWillReadFrequently =\n            env().getBool('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU');\n        if (fromPixels2DContext == null ||\n            newWillReadFrequently !== willReadFrequently) {\n          willReadFrequently = newWillReadFrequently;\n          fromPixels2DContext =\n              document.createElement('canvas').getContext(\n                  '2d', {willReadFrequently}) as CanvasRenderingContext2D;\n        }\n        fromPixels2DContext.canvas.width = width;\n        fromPixels2DContext.canvas.height = height;\n        fromPixels2DContext.drawImage(\n            pixels as HTMLVideoElement | HTMLImageElement, 0, 0, width, height);\n        pixels = fromPixels2DContext.canvas;\n      }\n\n      const usage = GPUTextureUsage.COPY_DST |\n          GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING;\n      const format = 'rgba8unorm' as GPUTextureFormat;\n      const texture = backend.textureManager.acquireTexture(\n          outputShape[1], outputShape[0], format, usage);\n      backend.queue.copyExternalImageToTexture(\n          {source: pixels as HTMLCanvasElement | ImageBitmap}, {texture},\n          [outputShape[1], outputShape[0]]);\n      textureInfo = {width, height, format, usage, texture};\n    }\n\n    const size = util.sizeFromShape(outputShape);\n    const strides = util.computeStrides(outputShape);\n    const program =\n        new FromPixelsProgram(outputShape, numChannels, importVideo);\n\n    const uniformData = [\n      {type: 'uint32', data: [size]}, {type: 'uint32', data: [numChannels]},\n      {type: 'uint32', data: [...strides]}\n    ];\n    const input = backend.makeTensorInfo([height, width], 'int32');\n    const info = backend.tensorMap.get(input.dataId);\n    info.resourceInfo = textureInfo;\n\n    const result =\n        backend.runWebGPUProgram(program, [input], 'int32', uniformData);\n    backend.disposeData(input.dataId);\n    return result;\n  }\n\n  // TODO: Encoding should happen on GPU once we no longer have to download\n  // image data to the CPU.\n  const imageData = (pixels as ImageData | backend_util.PixelData).data;\n  let pixelArray = imageData;\n  if (numChannels != null && numChannels !== 4) {\n    pixelArray = new Uint8Array(pixels.width * pixels.height * numChannels);\n\n    const dataLength = imageData.length;\n    let j = 0;\n    for (let i = 0; i < dataLength; i++) {\n      if (i % 4 < numChannels) {\n        pixelArray[j++] = imageData[i];\n      }\n    }\n  }\n\n  const output =\n      backend.makeTensorInfo(outputShape, 'int32', new Int32Array(pixelArray));\n  backend.uploadToGPU(output.dataId);\n  return output;\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Expm1, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {expm1ImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const expm1 =\n    unaryKernelFunc({opType: UnaryOpType.EXPM1, cpuKernelImpl: expm1ImplCPU});\n\nexport const expm1Config: KernelConfig = {\n  kernelName: Expm1,\n  backendName: 'webgpu',\n  kernelFunc: expm1\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class FlipLeftRightProgram implements WebGPUProgram {\n  outputShape: number[] = [];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(imageShape: [number, number, number, number]) {\n    this.outputShape = imageShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.shaderKey = 'flipLeftRight';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getCoordsFromIndex(index);\n          let coordX = uniforms.xShape[2] - coords[2] - 1;\n          let outputValue = getX(coords[0], coords[1], coordX, coords[3]);\n          setOutputAtIndex(index, outputValue);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Tensor4D} from '@tensorflow/tfjs-core';\nimport {FlipLeftRight, FlipLeftRightInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {FlipLeftRightProgram} from '../flip_left_right_webgpu';\n\nexport const flipLeftRightConfig: KernelConfig = {\n    kernelName: FlipLeftRight,\n    backendName: 'webgpu',\n    kernelFunc: ({inputs, backend}) => {\n      const {image} = inputs as FlipLeftRightInputs;\n      const webgpuBackend = backend as WebGPUBackend;\n\n      const program = new FlipLeftRightProgram((image as Tensor4D).shape);\n      const output =\n          webgpuBackend.runWebGPUProgram(program, [image], image.dtype);\n      return output;\n  }\n};\n","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Floor, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {floorImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const floor =\n    unaryKernelFunc({opType: UnaryOpType.FLOOR, cpuKernelImpl: floorImplCPU});\n\nexport const floorConfig: KernelConfig = {\n  kernelName: Floor,\n  backendName: 'webgpu',\n  kernelFunc: floor\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FloorDiv, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const floorDiv =\n    binaryKernelFunc({opType: BinaryOpType.INT_DIV, dtype: 'int32'});\n\nexport const floorDivConfig: KernelConfig = {\n  kernelName: FloorDiv,\n  backendName: 'webgpu',\n  kernelFunc: floorDiv\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class FromPixelsProgram implements WebGPUProgram {\n  dispatch: [number, number, number];\n  dispatchLayout: {x: number[]};\n  isFromPixels = true;\n  outputShape: number[] = [0];\n  shaderKey: string;\n  importVideo: boolean;\n  variableNames: string[] = [];\n  workGroupSize: [number, number, number] =\n      [256, 1, 1];  // The empirical value.\n\n  constructor(outputShape: number[], numChannels: number, importVideo = false) {\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [numChannels, 1, 1]);\n\n    this.importVideo = importVideo;\n    this.shaderKey = `fromPixels_${this.importVideo}`;\n  }\n\n  getUserCode(): string {\n    const textureLoad = this.importVideo ?\n        'textureLoad(src, vec2<i32>(coords.yx));' :\n        'textureLoad(src, vec2<i32>(coords.yx), 0)';\n    const textureType =\n        this.importVideo ? 'texture_external' : 'texture_2d<f32>';\n    return `\n      @binding(1) @group(0) var src: ${textureType};\n      ${main('index')} {\n        let flatIndex = index * uniforms.numChannels;\n        if (flatIndex < uniforms.size) {\n          let coords = getCoordsFromIndex(flatIndex);\n          let values = ${textureLoad};\n          for (var i = 0; i < uniforms.numChannels; i = i + 1) {\n            result[flatIndex + i] = i32(floor(255.0 * values[i]));\n          }\n        }\n      }\n  `;\n  }\n}\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '@tensorflow/tfjs-core';\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class BatchNormProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[], y?: number[], z?: number[]};\n  dispatch: [number, number, number];\n  variableNames: string[];\n  uniforms = 'varianceEpsilon : f32,';\n  // This is an experimental value.\n  workGroupSize: [number, number, number] = [128, 1, 1];\n  offsetShape: number[]|null;\n  scaleShape: number[]|null;\n  varianceEpsilon: number;\n  size = true;\n\n  constructor(\n      xShape: number[], meanShape: number[], varianceShape: number[],\n      offsetShape: number[]|null, scaleShape: number[]|null) {\n    this.variableNames = ['x', 'mean', 'variance'];\n    backend_util.assertAndGetBroadcastShape(xShape, meanShape);\n    backend_util.assertAndGetBroadcastShape(xShape, varianceShape);\n    this.outputShape = xShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    if (offsetShape != null) {\n      backend_util.assertAndGetBroadcastShape(xShape, offsetShape);\n      this.variableNames.push('offset');\n    }\n    if (scaleShape != null) {\n      backend_util.assertAndGetBroadcastShape(xShape, scaleShape);\n      this.variableNames.push('scale');\n    }\n    this.offsetShape = offsetShape;\n    this.scaleShape = scaleShape;\n    this.shaderKey = 'batchNorm';\n  }\n\n  getUserCode(): string {\n    let offsetSnippet = '0.0';\n    if (this.offsetShape != null) {\n      offsetSnippet = 'getOffsetByOutputIndex(index)';\n    }\n\n    let scaleSnippet = '1.0';\n    if (this.scaleShape != null) {\n      scaleSnippet = 'getScaleByOutputIndex(index)';\n    }\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size)\n        {\n          let xValue = getXByOutputIndex(index);\n          let meanValue = getMeanByOutputIndex(index);\n          let varianValue = getVarianceByOutputIndex(index);\n          let offsetValue = ${offsetSnippet};\n          let scaleValue = ${scaleSnippet};\n          let inv = scaleValue * inverseSqrt(varianValue + f32(uniforms.varianceEpsilon));\n          setOutputAtIndex(index,dot(vec3<f32>(xValue, -meanValue, offsetValue), vec3<f32>(inv, inv, 1.0)));\n        }\n      }\n  `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs, KernelConfig, Tensor} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {BatchNormProgram} from '../batchnorm_webgpu';\n\nexport const fusedBatchNormConfig: KernelConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'webgpu',\n  kernelFunc: ({inputs, attrs, backend}) => {\n    const {x, scale, offset, mean, variance} = inputs as FusedBatchNormInputs;\n    const {varianceEpsilon} = attrs as unknown as FusedBatchNormAttrs;\n    const webGPUBackend = backend as WebGPUBackend;\n    const batchNormInputs = [x as Tensor, mean as Tensor, variance as Tensor];\n    let offsetShape = null;\n    if (offset != null) {\n      offsetShape = offset.shape;\n      batchNormInputs.push(offset as Tensor);\n    }\n    let scaleShape = null;\n    if (scale != null) {\n      scaleShape = scale.shape;\n      batchNormInputs.push(scale as Tensor);\n    }\n    const program = new BatchNormProgram(\n        x.shape, mean.shape, variance.shape, offsetShape, scaleShape);\n    const uniformData = [{type: 'float32', data: [varianceEpsilon]}];\n    return webGPUBackend.runWebGPUProgram(\n        program, batchNormInputs, x.dtype, uniformData);\n  }\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, FusedConv2D, FusedConv2DAttrs, FusedConv2DInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {conv2DImpl} from './Conv2D_impl';\n\nexport function fusedConv2d(args: {\n  inputs: FusedConv2DInputs,\n  attrs: FusedConv2DAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {x, filter, bias, preluActivationWeights} = inputs;\n  const {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  } = attrs;\n\n  const $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, dilations, pad,\n      dimRoundingMode, false /* depthwise */, $dataFormat);\n\n  return conv2DImpl({\n    x,\n    filter,\n    convInfo,\n    backend,\n    bias,\n    preluActivationWeights,\n    leakyreluAlpha,\n    activation\n  });\n}\n\nexport const fusedConv2DConfig: KernelConfig = {\n  kernelName: FusedConv2D,\n  backendName: 'webgpu',\n  kernelFunc: fusedConv2d as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, FusedDepthwiseConv2D, FusedDepthwiseConv2DAttrs, FusedDepthwiseConv2DInputs, KernelConfig, KernelFunc, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {DepthwiseConv2DVec4Program} from '../depthwise_conv2d_vec4_webgpu';\nimport {DepthwiseConv2DProgram} from '../depthwise_conv2d_webgpu';\n\nexport function fusedDepthwiseConv2D(args: {\n  inputs: FusedDepthwiseConv2DInputs,\n  attrs: FusedDepthwiseConv2DAttrs,\n  backend: WebGPUBackend\n}) {\n  const {inputs, backend, attrs} = args;\n  const {x, filter, bias, preluActivationWeights} = inputs;\n  const {strides, pad, dilations, dimRoundingMode, activation, leakyreluAlpha} =\n      attrs;\n\n  let $dilations = dilations;\n  if ($dilations == null) {\n    $dilations = [1, 1];\n  }\n\n  util.assert(\n      backend_util.eitherStridesOrDilationsAreOne(strides, $dilations),\n      () => 'Error in depthwiseConv2d: Either strides or dilations must be ' +\n          `1. Got strides ${strides} and dilations '${$dilations}'`);\n\n  const convInfo = backend_util.computeConv2DInfo(\n      x.shape as [number, number, number, number],\n      filter.shape as [number, number, number, number], strides, $dilations,\n      pad, dimRoundingMode, true /* depthwise */);\n\n  const programInputs: TensorInfo[] = [x, filter];\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n\n  if (hasBias) {\n    programInputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    programInputs.push(preluActivationWeights);\n  }\n\n  const dimensions = [\n    {type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left]},\n    {type: 'int32', data: [convInfo.inHeight, convInfo.inWidth]},\n  ];\n\n  let program: DepthwiseConv2DProgram|DepthwiseConv2DVec4Program;\n  if (convInfo.inHeight > 4 && convInfo.inWidth > 4 &&\n      convInfo.strideWidth <= 2 &&\n      convInfo.inChannels === convInfo.outChannels &&\n      convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&\n      convInfo.inChannels % 4 === 0) {\n    program = new DepthwiseConv2DVec4Program(\n        convInfo, hasBias, activation, hasPreluActivationWeights);\n  } else {\n    program = new DepthwiseConv2DProgram(\n        convInfo, hasBias, activation, hasPreluActivationWeights);\n    dimensions.push(\n        {type: 'int32', data: [convInfo.filterHeight]},\n        {type: 'int32', data: [convInfo.filterWidth]},\n        {type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]}, {\n          type: 'int32',\n          data: [convInfo.dilationHeight, convInfo.dilationWidth]\n        });\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({type: 'float32', data: [leakyreluAlpha]});\n    program.uniforms += ' alpha : f32,';\n  }\n  const result =\n      backend.runWebGPUProgram(program, programInputs, 'float32', dimensions);\n\n  return result;\n}\n\nexport const fusedDepthwiseConv2DConfig: KernelConfig = {\n  kernelName: FusedDepthwiseConv2D,\n  backendName: 'webgpu',\n  kernelFunc: fusedDepthwiseConv2D as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class GatherNDProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames: string[] = ['A', 'indices'];\n  uniforms: string;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n  sliceDim: number;\n  constructor(sliceDim: number, shape: number[]) {\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.shaderKey = `gathernd_${sliceDim}`;\n    this.sliceDim = sliceDim;\n    this.uniforms = `sliceDim : i32, strides : ${getCoordsDataType(sliceDim)},`;\n  }\n\n  getUserCode(): string {\n    let strideString;\n    if (this.sliceDim > 1) {\n      strideString = 'uniforms.strides[j]';\n    } else {\n      strideString = 'uniforms.strides';\n    }\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getCoordsFromIndex(index);\n          var flattenIndex = 0;\n          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {\n            let indexTemp = i32(round(getIndices(coords[0], j)));\n            let strideNum = ${strideString};\n            flattenIndex = flattenIndex + indexTemp * strideNum;\n          }\n\n          setOutputAtIndex(index, getA(flattenIndex, coords[1]));\n        }\n      }\n      `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, GatherNd, GatherNdInputs, KernelConfig, KernelFunc, Rank, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {GatherNDProgram} from '../gather_nd_webgpu';\nimport {gatherNdImplCPU} from '../kernel_utils/shared';\n\nimport {reshape} from './Reshape';\n\nexport function gatherNd(\n    args: {inputs: GatherNdInputs, backend: WebGPUBackend}): TensorInfo {\n  const {inputs, backend} = args;\n  const {params, indices} = inputs;\n\n  const indicesShape = indices.shape;\n  const sliceRank = indicesShape[indicesShape.length - 1];\n  const paramsSize = util.sizeFromShape(params.shape);\n\n  const [resultShape, numSlices, sliceSize, strides] =\n      backend_util.prepareAndValidate(params, indices);\n\n  const flattenIndices = reshape(\n      {inputs: {x: indices}, backend, attrs: {shape: [numSlices, sliceRank]}});\n  const flattenX = reshape({\n    inputs: {x: params},\n    backend,\n    attrs: {shape: [(util.sizeFromShape(params.shape) / sliceSize), sliceSize]}\n  });\n  if (backend.shouldExecuteOnCPU([params, indices]) ||\n      params.dtype === 'string') {\n    const indicesData = backend.readSync(indices.dataId) as TypedArray;\n    const paramsBuf = backend.bufferSync<Rank, 'float32'>(params);\n    const outValue = gatherNdImplCPU(\n        indicesData, paramsBuf, params.dtype, numSlices, sliceRank, sliceSize,\n        strides, params.shape, paramsSize);\n\n    return backend.makeTensorInfo(resultShape, params.dtype, outValue.values);\n  }\n  const program = new GatherNDProgram(sliceRank, [numSlices, sliceSize]);\n  const uniformData =\n      [{type: 'int32', data: [sliceRank]}, {type: 'int32', data: strides}];\n  const res = backend.runWebGPUProgram(\n      program, [flattenX, flattenIndices], flattenX.dtype, uniformData);\n\n  const reshaped =\n      reshape({inputs: {x: res}, backend, attrs: {shape: resultShape}});\n\n  backend.disposeData(flattenIndices.dataId);\n  backend.disposeData(flattenX.dataId);\n  backend.disposeData(res.dataId);\n\n  return reshaped;\n}\n\nexport const gatherNdConfig: KernelConfig = {\n  kernelName: GatherNd,\n  backendName: 'webgpu',\n  kernelFunc: gatherNd as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class GatherProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames: string[] = ['A', 'indices'];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  aShape: number[];\n  size = true;\n\n  constructor(aShape: number[], outputShape: number[]) {\n    this.outputShape = aShape.slice();\n    this.aShape = aShape;\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.shaderKey = `gather`;\n  }\n\n  getUserCode(): string {\n    const sourceCoords = getSourceCoords(this.aShape);\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let resRC = getCoordsFromIndex(index);\n          let indexZ = i32(getIndices(resRC.x, resRC.z));\n          let inBounds = select(0.0, 1.0, indexZ >= 0 && indexZ < uniforms.aShape[2]);\n          setOutputAtIndex(index, inBounds * getA(${sourceCoords}));\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n\n// The input and output are always flattened into rank 4 tensors.\nfunction getSourceCoords(aShape: number[]): string {\n  const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];\n  const sourceCoords = [];\n  for (let i = 0; i < aShape.length; i++) {\n    if (i === 2) {\n      sourceCoords.push('indexZ');\n    } else {\n      sourceCoords.push(`${currentCoords[i]}`);\n    }\n  }\n  return sourceCoords.join();\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, buffer, GatherV2, GatherV2Attrs, GatherV2Inputs, KernelConfig, KernelFunc, Rank, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {gatherV2ImplCPU} from '../kernel_utils/shared';\n\nimport {GatherProgram} from '../gather_webgpu';\nimport {reshape} from './Reshape';\n\nexport function gatherV2(\n    args:\n        {inputs: GatherV2Inputs, backend: WebGPUBackend, attrs: GatherV2Attrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, indices} = inputs;\n  const {axis, batchDims} = attrs;\n\n  // Unlike WebGL, WebGPU won't check if index is out of bound by calling\n  // backend.readSync() function in debug mode.\n  const parsedAxis = util.parseAxisParam(axis, x.shape)[0];\n\n  const shapeInfo = backend_util.segment_util.collectGatherOpShapeInfo(\n      x, indices, parsedAxis, batchDims);\n\n  const indicesSize = util.sizeFromShape(indices.shape);\n\n  const toDispose = [];\n\n  const flattenX = reshape({\n    inputs: {x},\n    backend,\n    attrs: {\n      shape: [\n        shapeInfo.batchSize, shapeInfo.outerSize, shapeInfo.dimSize,\n        shapeInfo.sliceSize\n      ]\n    }\n  });\n\n  const flattenIndex = reshape({\n    inputs: {x: indices},\n    backend,\n    attrs: {shape: [shapeInfo.batchSize, indicesSize / shapeInfo.batchSize]}\n  });\n\n  toDispose.push(flattenX);\n  toDispose.push(flattenIndex);\n\n  const flattenOutputShape = [\n    shapeInfo.batchSize, shapeInfo.outerSize, indicesSize / shapeInfo.batchSize,\n    shapeInfo.sliceSize\n  ];\n\n  if (backend.shouldExecuteOnCPU([x, indices])) {\n    const indicesBufferInfo = backend.tensorMap.get(flattenIndex.dataId);\n    const indicesValues = indicesBufferInfo.values as TypedArray;\n    const indicesBuf =\n        buffer(flattenIndex.shape, flattenIndex.dtype, indicesValues) as\n        TensorBuffer<Rank>;\n    const xBufferInfo = backend.tensorMap.get(flattenX.dataId);\n    const xValues = xBufferInfo.values as TypedArray;\n    const xBuf =\n        buffer(flattenX.shape, flattenX.dtype, xValues) as TensorBuffer<Rank>;\n    const outBuf = gatherV2ImplCPU(xBuf, indicesBuf, flattenOutputShape);\n\n    toDispose.forEach(t => backend.disposeData(t.dataId));\n\n    return backend.makeTensorInfo(\n        shapeInfo.outputShape, outBuf.dtype, outBuf.values as TypedArray);\n  }\n\n  const program = new GatherProgram(flattenX.shape, flattenOutputShape);\n  const res = backend.runWebGPUProgram(\n      program, [flattenX, flattenIndex], flattenX.dtype);\n  toDispose.push(res);\n\n  const reshaped = reshape(\n      {inputs: {x: res}, backend, attrs: {shape: shapeInfo.outputShape}});\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n  return reshaped;\n}\n\nexport const gatherV2Config: KernelConfig = {\n  kernelName: GatherV2,\n  backendName: 'webgpu',\n  kernelFunc: gatherV2 as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Greater, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {greaterImplCPU as cpuGreater} from '../kernel_utils/shared';\n\nexport const greater = binaryKernelFunc({\n  opType: BinaryOpType.GREATER,\n  cpuKernelImpl: cpuGreater,\n  dtype: 'bool',\n});\n\nexport const greaterConfig: KernelConfig = {\n  kernelName: Greater,\n  backendName: 'webgpu',\n  kernelFunc: greater\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {GreaterEqual, KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {greaterEqualImplCPU as cpuGreaterEqual} from '../kernel_utils/shared';\n\nexport const greaterEqual = binaryKernelFunc({\n  opType: BinaryOpType.GREATER_EQUAL,\n  dtype: 'bool',\n  cpuKernelImpl: cpuGreaterEqual\n});\n\nexport const greaterEqualConfig: KernelConfig = {\n  kernelName: GreaterEqual,\n  backendName: 'webgpu',\n  kernelFunc: greaterEqual\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {IsNan, KernelConfig} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const isNaN =\n    unaryKernelFunc({opType: UnaryOpType.IS_NAN, dtype: 'bool'});\n\nexport const isNaNConfig: KernelConfig = {\n  kernelName: IsNan,\n  backendName: 'webgpu',\n  kernelFunc: isNaN\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LeakyRelu, LeakyReluAttrs, LeakyReluInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {UnaryOpType} from '../unary_op_util';\nimport {UnaryOpProgram} from '../unary_op_webgpu';\n\nexport function leakyRelu(args: {\n  inputs: LeakyReluInputs,\n  backend: WebGPUBackend,\n  attrs: LeakyReluAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {alpha} = attrs;\n  const uniformData = [{type: 'float32', data: [alpha]}];\n  const program = new UnaryOpProgram(x.shape, UnaryOpType.LEAKYRELU);\n  program.uniforms = 'alpha : f32,';\n  return backend.runWebGPUProgram(program, [x], 'float32', uniformData);\n}\n\nexport const leakyReluConfig: KernelConfig = {\n  kernelName: LeakyRelu,\n  backendName: 'webgpu',\n  kernelFunc: leakyRelu as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Less} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {lessImplCPU as cpuLess} from '../kernel_utils/shared';\n\nexport const less = binaryKernelFunc(\n    {opType: BinaryOpType.LESS, dtype: 'bool', cpuKernelImpl: cpuLess});\n\nexport const lessConfig: KernelConfig = {\n  kernelName: Less,\n  backendName: 'webgpu',\n  kernelFunc: less\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LessEqual} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {lessEqualImplCPU as cpuLessEqual} from '../kernel_utils/shared';\n\nexport const lessEqual = binaryKernelFunc({\n  opType: BinaryOpType.LESS_EQUAL,\n  dtype: 'bool',\n  cpuKernelImpl: cpuLessEqual\n});\n\nexport const lessEqualConfig: KernelConfig = {\n  kernelName: LessEqual,\n  backendName: 'webgpu',\n  kernelFunc: lessEqual\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Log} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {logImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const log =\n    unaryKernelFunc({opType: UnaryOpType.LOG, cpuKernelImpl: logImplCPU});\n\nexport const logConfig: KernelConfig = {\n  kernelName: Log,\n  backendName: 'webgpu',\n  kernelFunc: log\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LogicalAnd} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const logicalAnd =\n    binaryKernelFunc({opType: BinaryOpType.LOGICAL_AND, dtype: 'bool'});\n\nexport const logicalAndConfig: KernelConfig = {\n  kernelName: LogicalAnd,\n  backendName: 'webgpu',\n  kernelFunc: logicalAnd\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, LogicalNot} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const logicalNot = unaryKernelFunc({opType: UnaryOpType.LOGICAL_NOT});\n\nexport const logicalNotConfig: KernelConfig = {\n  kernelName: LogicalNot,\n  backendName: 'webgpu',\n  kernelFunc: logicalNot\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Maximum} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {maximumImplCPU as cpuMaximum} from '../kernel_utils/shared';\n\nexport const maximum = binaryKernelFunc({\n  opType: BinaryOpType.MAX,\n  cpuKernelImpl: cpuMaximum,\n});\n\nexport const maximumConfig: KernelConfig = {\n  kernelName: Maximum,\n  backendName: 'webgpu',\n  kernelFunc: maximum\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {backend_util, KernelConfig, KernelFunc, MaxPool, MaxPoolAttrs, MaxPoolInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {poolImpl} from './Pool_impl';\n\nexport function maxPool(\n    args: {inputs: MaxPoolInputs, backend: WebGPUBackend, attrs: MaxPoolAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {filterSize, strides, pad, dimRoundingMode} = attrs;\n  const dilations = 1;\n  const convInfo = backend_util.computePool2DInfo(\n      x.shape as [number, number, number, number], filterSize, strides,\n      dilations, pad, dimRoundingMode);\n\n  return poolImpl(x, convInfo, 'max', backend);\n}\n\nexport const maxPoolConfig: KernelConfig = {\n  kernelName: MaxPool,\n  backendName: 'webgpu',\n  kernelFunc: maxPool as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Min, MinAttrs, MinInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reduce} from '../kernel_utils/reduce';\n\nexport function min(\n    args: {inputs: MinInputs, backend: WebGPUBackend, attrs: MinAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  return reduce(x, axis, keepDims, 'min', backend);\n}\n\nexport const minConfig: KernelConfig = {\n  kernelName: Min,\n  backendName: 'webgpu',\n  kernelFunc: min as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Minimum} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {minimumImplCPU as cpuMinimum} from '../kernel_utils/shared';\n\nexport const minimum = binaryKernelFunc({\n  opType: BinaryOpType.MIN,\n  cpuKernelImpl: cpuMinimum,\n});\n\nexport const minimumConfig: KernelConfig = {\n  kernelName: Minimum,\n  backendName: 'webgpu',\n  kernelFunc: minimum\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class MirrorPadProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  uniforms = '';\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  xShape: number[];\n  offset: number;\n  size = true;\n\n  constructor(\n      xShape: number[], paddings: Array<[number, number]>,\n      mode: 'reflect'|'symmetric') {\n    this.outputShape = paddings.map(\n        (p, i) => p[0] /* beforePad */ + xShape[i] + p[1] /* afterPad */);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.xShape = xShape;\n    paddings.map((_, i) => {\n      this.uniforms += ` pad${i} : vec2<i32>,`;\n    });\n    this.offset = mode === 'reflect' ? 0 : 1;\n    this.shaderKey = `mirrorPad_${mode}`;\n  }\n\n  getUserCode(): string {\n    const rank = this.xShape.length;\n    // The length of paddings are same with the rank of the input tensor.\n    const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(',');\n    const end = this.xShape\n                    .map(\n                        (_, i) => `uniforms.pad${i}[0] + uniforms.xShape${\n                            rank > 1 ? `[${i}]` : ''}`)\n                    .join(',');\n\n    const shaderStart = rank === 1 ? 'start' : 'start[i]';\n    const shaderEnd = rank === 1 ? 'end' : 'end[i]';\n    const shaderOutC = rank === 1 ? 'outC' : 'outC[i]';\n    const dtype = getCoordsDataType(rank);\n    const unpackedCoords = rank > 1 ?\n        ['coords[0]', 'coords[1]', 'coords[2]', 'coords[3]'].slice(0, rank) :\n        'coords';\n\n    return `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let start = ${dtype}(${start});\n          let end = ${dtype}(${end});\n          var outC = getCoordsFromIndex(index);\n          for (var i = 0; i < ${rank}; i = i + 1) {\n            if (${shaderOutC} < ${shaderStart}) {\n              ${shaderOutC} = ${shaderStart} * 2 - ${shaderOutC} - ${\n        this.offset};\n            } else if(${shaderOutC} >= ${shaderEnd}) {\n              ${shaderOutC} = (${shaderEnd} - 1) * 2 - ${shaderOutC} + ${\n        this.offset};\n            }\n          }\n          let coords = outC - start;\n          setOutputAtIndex(index, getX(${unpackedCoords}));\n        }\n      }\n    `;\n  }\n}\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, MirrorPad, MirrorPadAttrs, MirrorPadInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {MirrorPadProgram} from '../mirror_pad_webgpu';\n\nexport const mirrorPadConfig: KernelConfig = {\n  kernelName: MirrorPad,\n  backendName: 'webgpu',\n  kernelFunc: ({inputs, attrs, backend}) => {\n    const {x} = inputs as MirrorPadInputs;\n    const {paddings, mode} = attrs as unknown as MirrorPadAttrs;\n    const webGPUBackend = backend as WebGPUBackend;\n\n    const uniformData = paddings.map(p => {\n      return {type: 'int32', data: [p[0], p[1]]};\n    });\n    const program = new MirrorPadProgram(x.shape, paddings, mode);\n    const output =\n        webGPUBackend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n\n    return output;\n  }\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Neg, NegInputs, TensorInfo, TypedArray} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {negImplCPU} from '../kernel_utils/shared';\n\nimport {UnaryOpType} from '../unary_op_util';\nimport {UnaryOpProgram} from '../unary_op_webgpu';\n\n// This doesn't use unaryKernelFunc because negImplCPU is not of type\n// SimpleUnaryKernelImplCPU.\nexport function neg(args: {inputs: NegInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  if (backend.shouldExecuteOnCPU([x])) {\n    const xData = backend.tensorMap.get(x.dataId);\n    const [outValues, newShape] =\n        negImplCPU(xData.values as TypedArray, x.shape, x.dtype);\n    return backend.makeTensorInfo(newShape, x.dtype, outValues);\n  }\n\n  const program = new UnaryOpProgram(x.shape, UnaryOpType.NEG);\n\n  return backend.runWebGPUProgram(program, [x], x.dtype);\n}\n\nexport const negConfig: KernelConfig = {\n  kernelName: Neg,\n  backendName: 'webgpu',\n  kernelFunc: neg as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {kernel_impls, KernelConfig, KernelFunc, NonMaxSuppressionV3, NonMaxSuppressionV3Attrs, NonMaxSuppressionV3Inputs, TypedArray} from '@tensorflow/tfjs-core';\nimport {WebGPUBackend} from '../backend_webgpu';\n\nexport function nonMaxSuppressionV3(args: {\n  inputs: NonMaxSuppressionV3Inputs,\n  backend: WebGPUBackend,\n  attrs: NonMaxSuppressionV3Attrs\n}) {\n  console.warn(\n      'tf.nonMaxSuppression() in webgpu locks the UI thread. ' +\n      'Call tf.nonMaxSuppressionAsync() instead');\n\n  const {inputs, backend, attrs} = args;\n  const {boxes, scores} = inputs;\n  const {maxOutputSize, iouThreshold, scoreThreshold} = attrs;\n\n  const boxesVals = backend.readSync(boxes.dataId) as TypedArray;\n  const scoresVals = backend.readSync(scores.dataId) as TypedArray;\n\n  const {selectedIndices} = kernel_impls.nonMaxSuppressionV3Impl(\n      boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n\n  return backend.makeTensorInfo(\n      [selectedIndices.length], 'int32', new Int32Array(selectedIndices));\n}\n\nexport const nonMaxSuppressionV3Config: KernelConfig = {\n  kernelName: NonMaxSuppressionV3,\n  backendName: 'webgpu',\n  kernelFunc: nonMaxSuppressionV3 as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {kernel_impls, KernelConfig, KernelFunc, NonMaxSuppressionV5, NonMaxSuppressionV5Attrs, NonMaxSuppressionV5Inputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nexport type TypedArray = Float32Array|Int32Array|Uint8Array;\n\nexport function nonMaxSuppressionV5(args: {\n  inputs: NonMaxSuppressionV5Inputs,\n  backend: WebGPUBackend,\n  attrs: NonMaxSuppressionV5Attrs\n}): [TensorInfo, TensorInfo] {\n  console.warn(\n      'tf.nonMaxSuppression() in webgpu locks the UI thread. ' +\n      'Call tf.nonMaxSuppressionAsync() instead');\n\n  const {inputs, backend, attrs} = args;\n  const {boxes, scores} = inputs;\n  const {maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma} = attrs;\n\n  const boxesVals = backend.readSync(boxes.dataId) as TypedArray;\n  const scoresVals = backend.readSync(scores.dataId) as TypedArray;\n\n  const maxOutputSizeVal = maxOutputSize;\n  const iouThresholdVal = iouThreshold;\n  const scoreThresholdVal = scoreThreshold;\n  const softNmsSigmaVal = softNmsSigma;\n\n  const {selectedIndices, selectedScores} =\n      kernel_impls.nonMaxSuppressionV5Impl(\n          boxesVals, scoresVals, maxOutputSizeVal, iouThresholdVal,\n          scoreThresholdVal, softNmsSigmaVal);\n\n  return [\n    backend.makeTensorInfo(\n        [selectedIndices.length], 'int32', new Int32Array(selectedIndices)),\n    backend.makeTensorInfo(\n        [selectedScores.length], 'float32', new Float32Array(selectedScores))\n  ];\n}\n\nexport const nonMaxSuppressionV5Config: KernelConfig = {\n  kernelName: NonMaxSuppressionV5,\n  backendName: 'webgpu',\n  kernelFunc: nonMaxSuppressionV5 as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, ZerosLike, ZerosLikeInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {complex} from './Complex';\nimport {fill} from './Fill';\nimport {imag} from './Imag';\nimport {real} from './Real';\n\nexport function zerosLike(\n    args: {inputs: ZerosLikeInputs, backend: WebGPUBackend}): TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n  if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const r = zerosLike({inputs: {x: realPart}, backend});\n    const imagPart = imag({inputs: {input: x}, backend});\n    const i = zerosLike({inputs: {x: imagPart}, backend});\n\n    const result = complex({inputs: {real: r, imag: i}, backend});\n\n    backend.disposeData(realPart.dataId);\n    backend.disposeData(r.dataId);\n    backend.disposeData(imagPart.dataId);\n    backend.disposeData(i.dataId);\n\n    return result;\n  } else {\n    return fill({\n      attrs: {\n        shape: x.shape,\n        dtype: x.dtype,\n        value: x.dtype === 'string' ? '' : 0\n      },\n      backend\n    });\n  }\n}\n\nexport const zerosLikeConfig: KernelConfig = {\n  kernelName: ZerosLike,\n  backendName: 'webgpu',\n  kernelFunc: zerosLike as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, OnesLike, OnesLikeInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {complex} from './Complex';\nimport {fill} from './Fill';\nimport {imag} from './Imag';\nimport {real} from './Real';\nimport {zerosLike} from './ZerosLike';\n\nexport function onesLike(\n    args: {inputs: OnesLikeInputs, backend: WebGPUBackend}): TensorInfo {\n  const {inputs, backend} = args;\n  const {x} = inputs;\n\n  if (x.dtype === 'string') {\n    throw new Error('onesLike is not supported under string dtype');\n  } else if (x.dtype === 'complex64') {\n    const realPart = real({inputs: {input: x}, backend});\n    const r = onesLike({inputs: {x: realPart}, backend});\n    const imagPart = imag({inputs: {input: x}, backend});\n    const i = zerosLike({inputs: {x: imagPart}, backend});\n\n    const result = complex({inputs: {real: r, imag: i}, backend});\n\n    backend.disposeData(realPart.dataId);\n    backend.disposeData(r.dataId);\n    backend.disposeData(imagPart.dataId);\n    backend.disposeData(i.dataId);\n\n    return result;\n  } else {\n    return fill({attrs: {shape: x.shape, dtype: x.dtype, value: 1}, backend});\n  }\n}\n\nexport const onesLikeConfig: KernelConfig = {\n  kernelName: OnesLike,\n  backendName: 'webgpu',\n  kernelFunc: onesLike as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Pack, PackAttrs, PackInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {concat} from './Concat';\nimport {expandDims} from './ExpandDims';\n\nexport function pack(\n    args: {inputs: PackInputs, backend: WebGPUBackend, attrs: PackAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {axis} = attrs;\n\n  if (inputs.length === 1) {\n    return expandDims(\n        {inputs: {input: inputs[0]}, backend, attrs: {dim: axis}});\n  }\n\n  const shape = inputs[0].shape;\n  const dtype = inputs[0].dtype;\n\n  inputs.forEach(t => {\n    util.assertShapesMatch(\n        shape, t.shape,\n        'All tensors passed to stack must have matching shapes');\n    util.assert(\n        dtype === t.dtype,\n        () => 'All tensors passed to stack must have matching dtypes');\n  });\n\n  const intermediateTensorInfos: TensorInfo[] = [];\n  const expandedTensors = inputs.map(t => {\n    const expandedT =\n        expandDims({inputs: {input: t}, backend, attrs: {dim: axis}});\n    intermediateTensorInfos.push(expandedT);\n    return expandedT;\n  });\n\n  const result = concat({inputs: expandedTensors, backend, attrs: {axis}});\n\n  intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));\n\n  return result;\n}\n\nexport const packConfig: KernelConfig = {\n  kernelName: Pack,\n  backendName: 'webgpu',\n  kernelFunc: pack as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class PadProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms = 'constantValue : f32,';\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  xShape: number[];\n  size = true;\n\n  constructor(xShape: number[], paddings: Array<[number, number]>) {\n    this.outputShape = paddings.map(\n        (p, i) => p[0] /* beforePad */ + xShape[i] + p[1] /* afterPad */);\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    paddings.map((_, i) => {\n      this.uniforms += ` pad${i} : vec2<i32>,`;\n    });\n    this.xShape = xShape;\n    this.shaderKey = 'pad';\n  }\n\n  getUserCode(): string {\n    const rank = this.xShape.length;\n    const type = getCoordsDataType(rank);\n    // The length of paddings are same with the rank of the input tensor.\n    const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(',');\n    const end = this.xShape\n                    .map(\n                        (_, i) => `uniforms.pad${i}[0] + uniforms.xShape${\n                            rank > 1 ? `[${i}]` : ''}`)\n                    .join(',');\n    const startValue = rank > 1 ? `${type}(${start})` : `${start}`;\n    const endValue = rank > 1 ? `${type}(${end})` : `${end}`;\n\n    const leftPadCondition = rank > 1 ? `any(outC < start)` : `outC < start`;\n    const rightPadCondition = rank > 1 ? `any(outC >= end)` : `outC >= end`;\n\n    const unpackedCoords = rank > 1 ?\n        ['coords[0]', 'coords[1]', 'coords[2]', 'coords[3]'].slice(0, rank) :\n        'coords';\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let start = ${startValue};\n          let end = ${endValue};\n          let outC = getCoordsFromIndex(index);\n\n          if (${leftPadCondition} || ${rightPadCondition}) {\n            setOutputAtIndex(index, uniforms.constantValue);\n          } else {\n            let coords = outC - start;\n            setOutputAtIndex(index, getX(${unpackedCoords}));\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, PadV2, PadV2Attrs, PadV2Inputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {identity} from './Identity';\nimport {PadProgram} from '../pad_webgpu';\nimport {fill} from './Fill';\n\nexport const padV2 =\n    (args: {inputs: PadV2Inputs,\n            backend: WebGPUBackend,\n            attrs: PadV2Attrs}): TensorInfo => {\n      const {inputs, backend, attrs} = args;\n      const {x} = inputs;\n      const {paddings, constantValue} = attrs;\n      if (paddings.every(p => util.arraysEqual(p, [0, 0]))) {\n        return identity({inputs: {x}, backend});\n      }\n      if (util.sizeFromShape(x.shape) === 0) {\n        // Short-circuit the computation, since x doesn't have value, only\n        // the shape is used to compute output shape to pad.\n        const outputShape = paddings.map(\n            (p, i) =>\n                p[0] /* beforePad */ + x.shape[i] + p[1] /* afterPad */);\n        return fill({\n          backend,\n          attrs: {shape: outputShape, value: constantValue, dtype: x.dtype}\n        });\n      }\n      const uniformData = [{type: 'float32', data: [constantValue]}];\n      paddings.map(p => uniformData.push({type: 'int32', data: [p[0], p[1]]}));\n      const program = new PadProgram(x.shape, paddings);\n      return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n    };\n\nexport const padV2Config: KernelConfig = {\n  kernelName: PadV2,\n  backendName: 'webgpu',\n  kernelFunc: padV2 as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Pow} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const pow = binaryKernelFunc({\n  opType: BinaryOpType.POW,\n});\n\nexport const powConfig: KernelConfig = {\n  kernelName: Pow,\n  backendName: 'webgpu',\n  kernelFunc: pow\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Prelu, PreluInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {BinaryOpProgram} from '../binary_op_webgpu';\n\nexport function prelu(args: {inputs: PreluInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {x, alpha} = inputs;\n\n  const program = new BinaryOpProgram(BinaryOpType.PRELU, x.shape, alpha.shape);\n  return backend.runWebGPUProgram(program, [x, alpha], 'float32');\n}\n\nexport const preluConfig: KernelConfig = {\n  kernelName: Prelu,\n  backendName: 'webgpu',\n  kernelFunc: prelu as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Prod, ProdAttrs, ProdInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reduce} from '../kernel_utils/reduce';\n\nexport function prod(\n    args: {inputs: ProdInputs, backend: WebGPUBackend, attrs: ProdAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {axis, keepDims} = attrs;\n\n  return reduce(x, axis, keepDims, 'prod', backend);\n}\n\nexport const prodConfig: KernelConfig = {\n  kernelName: Prod,\n  backendName: 'webgpu',\n  kernelFunc: prod as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Range, RangeAttrs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {rangeImplCPU} from '../kernel_utils/shared';\n\nexport const range =\n    (args: {backend: WebGPUBackend, attrs: RangeAttrs}): TensorInfo => {\n      const {backend, attrs} = args;\n      const {start, stop, step, dtype} = attrs;\n      const values = rangeImplCPU(start, stop, step, dtype);\n      return backend.makeTensorInfo([values.length], dtype, values);\n    };\n\nexport const rangeConfig: KernelConfig = {\n  kernelName: Range,\n  backendName: 'webgpu',\n  kernelFunc: range as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, RealDiv} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const realDiv = binaryKernelFunc({opType: BinaryOpType.DIV});\n\nexport const realDivConfig: KernelConfig = {\n  kernelName: RealDiv,\n  backendName: 'webgpu',\n  kernelFunc: realDiv as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Reciprocal} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const reciprocal = unaryKernelFunc({opType: UnaryOpType.RECIPROCAL});\n\nexport const reciprocalConfig: KernelConfig = {\n  kernelName: Reciprocal,\n  backendName: 'webgpu',\n  kernelFunc: reciprocal\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Relu} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const relu = unaryKernelFunc({opType: UnaryOpType.RELU});\n\nexport const reluConfig: KernelConfig = {\n  kernelName: Relu,\n  backendName: 'webgpu',\n  kernelFunc: relu\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Relu6} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const relu6 = unaryKernelFunc({opType: UnaryOpType.RELU6});\n\nexport const relu6Config: KernelConfig = {\n  kernelName: Relu6,\n  backendName: 'webgpu',\n  kernelFunc: relu6\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ResizeBilinearProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms = 'adjustHeightWidth : vec2<f32>, halfPixelCenters : f32,';\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(\n      inputShape: [number, number, number, number], newHeight: number,\n      newWidth: number) {\n    this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];\n\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.shaderKey = `resizeBilinear`;\n  }\n\n  getUserCode(): string {\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n        let coords = getCoordsFromIndex(index);\n          let b = coords[0];\n          let d = coords[3];\n          let rc = coords.yz;\n\n          let effectiveInSize = vec2<f32>(\n            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],\n            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);\n\n          let effectiveOutSize = vec2<f32>(\n            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],\n            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);\n\n          let effectiveInputOverOutputRatioRC =\n              effectiveInSize / effectiveOutSize;\n\n          // Fractional source index\n          let sourceFracIndexRC =\n            (vec2<f32>(rc) + vec2<f32>(uniforms.halfPixelCenters)) *\n            effectiveInputOverOutputRatioRC - vec2<f32>(uniforms.halfPixelCenters);\n\n          // Compute the four integer indices.\n          let sourceFloorRC = vec2<i32>(sourceFracIndexRC);\n          let sourceCeilRC = vec2<i32>(\n            min(vec2<f32>(uniforms.xShape.yz) - vec2<f32>(1.0), ceil(sourceFracIndexRC)));\n\n          let topLeft = getX(b, sourceFloorRC.x, sourceFloorRC.y, d);\n          let bottomLeft = getX(b, sourceCeilRC.x, sourceFloorRC.y, d);\n          let topRight = getX(b, sourceFloorRC.x, sourceCeilRC.y, d);\n          let bottomRight = getX(b, sourceCeilRC.x, sourceCeilRC.y, d);\n\n          let fracRC = sourceFracIndexRC - vec2<f32>(sourceFloorRC);\n\n          let top = topLeft + (topRight - topLeft) * fracRC.y;\n          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracRC.y;\n          let newValue = top + (bottom - top) * fracRC.x;\n\n          setOutputAtIndex(index, newValue);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeBilinear, ResizeBilinearAttrs, ResizeBilinearInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {ResizeBilinearProgram} from '../resize_bilinear_webgpu';\n\nexport function resizeBilinear(args: {\n  inputs: ResizeBilinearInputs,\n  backend: WebGPUBackend,\n  attrs: ResizeBilinearAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images} = inputs;\n  const {alignCorners, size, halfPixelCenters} = attrs;\n\n  const [newHeight, newWidth] = size;\n  const adjustHeight = alignCorners && newHeight > 1 ? 1.0 : 0.0;\n  const adjustWidth = alignCorners && newWidth > 1 ? 1.0 : 0.0;\n  const halfPixelCentersValue = halfPixelCenters ? 0.5 : 0.0;\n  const uniformData = [\n    {type: 'float32', data: [adjustHeight, adjustWidth]},\n    {type: 'float32', data: [halfPixelCentersValue]}\n  ];\n\n  const program = new ResizeBilinearProgram(\n      images.shape as [number, number, number, number], newHeight, newWidth);\n\n  return backend.runWebGPUProgram(program, [images], 'float32', uniformData);\n}\n\nexport const resizeBilinearConfig: KernelConfig = {\n  kernelName: ResizeBilinear,\n  backendName: 'webgpu',\n  kernelFunc: resizeBilinear as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ResizeNearestNeighborProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms = 'adjustHeightWidth : vec2<f32>, roundBase : f32,';\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  halfPixelCenters: boolean;\n  size = true;\n\n  constructor(\n      inputShape: [number, number, number, number], newHeight: number,\n      newWidth: number, halfPixelCenters: boolean) {\n    this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];\n\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.halfPixelCenters = halfPixelCenters;\n    this.shaderKey = `resizeNearest_${halfPixelCenters}`;\n  }\n\n  getUserCode(): string {\n    let sourceFracIndexRC: string;\n    if (this.halfPixelCenters) {\n      sourceFracIndexRC =\n          `max((vec2<f32>(rc) + vec2<f32>(0.5)) * effectiveInputOverOutputRatioRC` +\n          `, vec2<f32>(0.0))`;\n    } else {\n      sourceFracIndexRC = `vec2<f32>(rc) * effectiveInputOverOutputRatioRC`;\n    }\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getCoordsFromIndex(index);\n          let b = coords[0];\n          let d = coords[3];\n          let rc = coords.yz;\n\n          let effectiveInSize = vec2<f32>(\n            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],\n            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);\n\n          let effectiveOutSize = vec2<f32>(\n            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],\n            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);\n\n          let effectiveInputOverOutputRatioRC =\n              effectiveInSize / effectiveOutSize;\n\n          // Fractional source index\n          let sourceFracIndexRC = ${sourceFracIndexRC};\n\n          // Compute the coordinators of nearest neighbor point.\n          let inputShapeRC = vec2<f32>(f32(uniforms.xShape.y), f32(uniforms.xShape.z));\n          let sourceNearestRC = vec2<i32>(\n            min(inputShapeRC - 1.0, floor(sourceFracIndexRC + uniforms.roundBase)));\n          let newValue = getX(b, sourceNearestRC.x, sourceNearestRC.y, d);\n\n          setOutputAtIndex(index, newValue);\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, ResizeNearestNeighbor, ResizeNearestNeighborAttrs, ResizeNearestNeighborInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {ResizeNearestNeighborProgram} from '../resize_nearest_neighbor_webgpu';\n\nexport function resizeNearestNeighbor(args: {\n  inputs: ResizeNearestNeighborInputs,\n  backend: WebGPUBackend,\n  attrs: ResizeNearestNeighborAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {images} = inputs;\n  const {alignCorners, halfPixelCenters, size} = attrs;\n\n  const [newHeight, newWidth] = size;\n  const adjustHeight = alignCorners && newHeight > 1 ? 1.0 : 0.0;\n  const adjustWidth = alignCorners && newWidth > 1 ? 1.0 : 0.0;\n  // When align corners is false, we rounds the value with floor.\n  const roundBase = alignCorners ? 0.5 : 0.0;\n  const uniformData = [\n    {type: 'float32', data: [adjustHeight, adjustWidth]},\n    {type: 'float32', data: [roundBase]}\n  ];\n\n  const program = new ResizeNearestNeighborProgram(\n      images.shape as [number, number, number, number], newHeight, newWidth,\n      halfPixelCenters);\n  return backend.runWebGPUProgram(program, [images], images.dtype, uniformData);\n}\n\nexport const resizeNearestNeighborConfig: KernelConfig = {\n  kernelName: ResizeNearestNeighbor,\n  backendName: 'webgpu',\n  kernelFunc: resizeNearestNeighbor as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class RotateProgram implements WebGPUProgram {\n  outputShape: number[] = [];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x'];\n  uniforms: string;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  fillSnippet: string;\n  size = true;\n\n  constructor(\n      imageShape: [number, number, number, number],\n      fillValue: number|[number, number, number]) {\n    this.outputShape = imageShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.uniforms = `centerX : f32, centerY : f32, sinRadians : f32,\n          cosRadians : f32,`;\n    this.shaderKey = 'rotate';\n    this.outputShape = imageShape;\n\n    if (typeof fillValue === 'number') {\n      this.uniforms += ` fillValue : f32,`;\n      this.fillSnippet = `var outputValue = uniforms.fillValue;`;\n      this.shaderKey += '_float';\n    } else {\n      this.uniforms += ` fillValue : vec3<f32>,`;\n      this.fillSnippet = `var outputValue = uniforms.fillValue[coords[3]];`;\n      this.shaderKey += '_vec3';\n    }\n  }\n\n  getUserCode(): string {\n    const userCode = `\n        ${main('index')} {\n          if (index < uniforms.size) {\n            let coords = getCoordsFromIndex(index);\n            let coordXFloat = (f32(coords[2]) - uniforms.centerX) *\n                uniforms.cosRadians - (f32(coords[1]) - uniforms.centerY) *\n                uniforms.sinRadians;\n            let coordYFloat = (f32(coords[2]) - uniforms.centerX) *\n                uniforms.sinRadians + (f32(coords[1]) - uniforms.centerY) *\n                uniforms.cosRadians;\n            let coordX = i32(round(coordXFloat + uniforms.centerX));\n            let coordY = i32(round(coordYFloat + uniforms.centerY));\n            ${this.fillSnippet}\n            if(coordX >= 0 && coordX < uniforms.xShape[2] && coordY >= 0 &&\n                coordY < uniforms.xShape[1]) {\n              outputValue = getX(coords[0], coordY, coordX, coords[3]);\n            }\n            setOutputAtIndex(index, outputValue);\n          }\n        }\n      `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, Tensor4D} from '@tensorflow/tfjs-core';\nimport {RotateWithOffset, RotateWithOffsetAttrs, RotateWithOffsetInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {RotateProgram} from '../rotate_webgpu';\n\nexport const rotateWithOffsetConfig: KernelConfig = {\n    kernelName: RotateWithOffset,\n    backendName: 'webgpu',\n    kernelFunc: ({inputs, attrs, backend}) => {\n      const {image} = inputs as RotateWithOffsetInputs;\n      const {radians, fillValue, center} = attrs as {} as RotateWithOffsetAttrs;\n      const webgpuBackend = backend as WebGPUBackend;\n\n      const program = new RotateProgram((image as Tensor4D).shape, fillValue);\n      const [centerX, centerY] =\n          backend_util.getImageCenter(center, image.shape[1], image.shape[2]);\n      const uniformData = [\n            {type: 'float32', data: [centerX]},\n            {type: 'float32', data: [centerY]},\n            {type: 'float32', data: [Math.sin(radians)]},\n            {type: 'float32', data: [Math.cos(radians)]}\n          ];\n\n      if (typeof fillValue === 'number') {\n        uniformData.push(\n            {type: 'float32', data: [Number.parseFloat(fillValue.toFixed(2))]});\n      } else {\n        uniformData.push({type: 'float32', data: fillValue});\n      }\n\n      const output = webgpuBackend.runWebGPUProgram(\n          program, [image], image.dtype, uniformData);\n      return output;\n   }\n };\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Rsqrt} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {rsqrtImplCPU} from '../kernel_utils/shared';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const rsqrt =\n    unaryKernelFunc({opType: UnaryOpType.RSQRT, cpuKernelImpl: rsqrtImplCPU});\n\nexport const rsqrtConfig: KernelConfig = {\n  kernelName: Rsqrt,\n  backendName: 'webgpu',\n  kernelFunc: rsqrt\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType} from '@tensorflow/tfjs-core';\nimport {getCoordsDataType, getMainHeaderString as main, mapToWgslTypes, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class ScatterProgram implements WebGPUProgram {\n  variableNames = ['updates', 'indices'];\n  uniforms: string;\n  outputShape: number[];\n  sumDupeIndices: boolean;\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  updatesRank: number;\n  indicesRank: number;\n  sliceDimGreaterThanOne: boolean;\n  atomic = true;\n  type: DataType;\n\n  constructor(\n      flattenXShape: number[], sliceDim: number, indicesRank: number,\n      updatesRank: number, strides: number[], shape: number[],\n      outputDtype: DataType, sumDupeIndices = true) {\n    this.outputShape = shape;\n    this.type = outputDtype;\n    this.sumDupeIndices = sumDupeIndices;\n    this.dispatchLayout = flatDispatchLayout(flattenXShape);\n    // Dispatching based on |updates| shape instead of output shape.\n    this.dispatch =\n        computeDispatch(this.dispatchLayout, flattenXShape, this.workGroupSize);\n    this.sliceDimGreaterThanOne = sliceDim > 1;\n    this.shaderKey = `scatter_${indicesRank}_${updatesRank}_${\n        this.sliceDimGreaterThanOne}_${outputDtype}_${sumDupeIndices}`;\n    const stridesType = getCoordsDataType(strides.length);\n    this.uniforms = `sliceDim : i32, strides: ${stridesType}, size: i32,`;\n    this.updatesRank = updatesRank;\n    this.indicesRank = indicesRank;\n  }\n\n  getUserCode(): string {\n    let indicesString = '';\n    if (this.indicesRank === 1) {\n      indicesString = 'coords[0]';\n    } else if (this.indicesRank === 2) {\n      indicesString = 'coords[0], j';\n    }\n    const indicesSnippet = `getIndices(${indicesString})`;\n\n    const strideString = this.sliceDimGreaterThanOne ? 'uniforms.strides[j]' :\n                                                       'uniforms.strides';\n\n    let outCoordsString = '';\n    let getUpdatesCoordsFromFlatIndex = '';\n    if (this.dispatchLayout.x.length === 1) {\n      outCoordsString = 'flattenedIndex';\n      getUpdatesCoordsFromFlatIndex = `\n      fn getUpdatesCoordsFromFlatIndex(index : i32) -> i32 {\n        return index;\n      }\n      `;\n    } else if (this.dispatchLayout.x.length === 2) {\n      outCoordsString = 'vec2<i32>(flattenedIndex, coords[1])';\n      getUpdatesCoordsFromFlatIndex = `\n      fn getUpdatesCoordsFromFlatIndex(index : i32) -> vec2<i32> {\n        // N.B. |updates| could be a scalar tensor, conceptually representing a\n        // 2D tensor with all values equal to that. By design, its size must be\n        // the same as |outShape[1]| in one dimension, and |indicesShape[0]|\n        // gives the other.\n        let sliceSize = uniforms.outShape[1];\n        let d0 = index / sliceSize;\n        let d1 = index - d0 * sliceSize;\n        return vec2<i32>(d0, d1);\n      }\n      `;\n    }\n    const updatesString =\n        Array.from({length: this.updatesRank}, (_, idx) => `coords[${idx}]`);\n    const updatesSnippet = `getUpdates(${updatesString.join(', ')})`;\n\n    const atomicRMW = (ptr: string, val: string) => {\n      let atomicAddSnippet = `atomicAdd(${ptr}, bitcast<i32>(${val}))`;\n      if (this.type === 'float32') {\n        atomicAddSnippet = `\n          {\n            var oldBits = 0;\n            var newBits = bitcast<i32>(${val});\n            loop {\n              let info = atomicCompareExchangeWeak(${ptr}, oldBits, newBits);\n              if (info.exchanged) {\n                break;\n              }\n              oldBits = info.old_value;\n              let oldValue = bitcast<f32>(oldBits);\n              let newValue = oldValue + (${val});\n              newBits = bitcast<i32>(newValue);\n            }\n          }\n        `;\n      }\n      const atomicStoreSnippet = `atomicStore(${ptr}, bitcast<i32>(${val}));`;\n      return this.sumDupeIndices ? atomicAddSnippet : atomicStoreSnippet;\n    };\n\n    const userCode = `\n    ${getUpdatesCoordsFromFlatIndex}\n\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let coords = getUpdatesCoordsFromFlatIndex(index);\n          var flattenedIndex = 0;\n          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {\n            let indexInside = i32(round(${indicesSnippet}));\n            flattenedIndex = flattenedIndex + indexInside * ${strideString};\n          }\n          let updateValue =\n              ${mapToWgslTypes(this.type, false)}(${updatesSnippet});\n          let flatIndex = getOutputIndexFromCoords(${outCoordsString});\n\n          ${atomicRMW('&result[flatIndex]', 'updateValue')};\n        }\n      }`;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, ScatterNd, ScatterNdAttrs, ScatterNdInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {ScatterProgram} from '../scatter_webgpu';\n\nimport {fill} from './Fill';\nimport {reshape} from './Reshape';\n\nexport function scatterNd(args: {\n  inputs: ScatterNdInputs,\n  backend: WebGPUBackend,\n  attrs: ScatterNdAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {indices, updates} = inputs;\n  const {shape} = attrs;\n\n  const {sliceRank, numUpdates, sliceSize, strides, outputSize} =\n      backend_util.calculateShapes(updates, indices, shape);\n\n  const flattenShape = [outputSize / sliceSize, sliceSize];\n\n  if (outputSize === 0) {\n    return backend.makeTensorInfo(shape, indices.dtype);\n  }\n\n  const flattenIndices = reshape(\n      {inputs: {x: indices}, backend, attrs: {shape: [numUpdates, sliceRank]}});\n  const flattenX = reshape(\n      {inputs: {x: updates}, backend, attrs: {shape: [numUpdates, sliceSize]}});\n\n  const type = flattenX.dtype;\n  const output =\n      fill({backend, attrs: {shape: flattenShape, value: 0, dtype: type}});\n  const size = util.sizeFromShape(flattenX.shape);\n  const uniformData = [\n    {type: 'int32', data: [sliceRank]}, {type: 'int32', data: strides},\n    {type: 'int32', data: [size]}\n  ];\n  const program = new ScatterProgram(\n      flattenX.shape, sliceRank, flattenIndices.shape.length,\n      flattenX.shape.length, strides, flattenShape, type);\n  const res = backend.runWebGPUProgram(\n      program, [flattenX, flattenIndices], type, uniformData, output);\n\n  const reshaped = reshape({inputs: {x: res}, backend, attrs: {shape}});\n\n  backend.disposeData(flattenIndices.dataId);\n  backend.disposeData(flattenX.dataId);\n  backend.disposeData(res.dataId);\n\n  return reshaped;\n}\n\nexport const scatterNdConfig: KernelConfig = {\n  kernelName: ScatterNd,\n  backendName: 'webgpu',\n  kernelFunc: scatterNd as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class SelectProgram implements WebGPUProgram {\n  variableNames = ['c', 'a', 'b'];\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  cRank: number;\n  rank: number;\n  size = true;\n\n  constructor(cRank: number, shape: number[], rank: number) {\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n\n    this.cRank = cRank;\n    this.rank = rank;\n    this.shaderKey = 'select';\n  }\n\n  getUserCode(): string {\n    // TODO(WGSL): below code can be merged with getUserCode.\n    let cCoords;\n    let abCoords;\n    if (this.rank > 4) {\n      throw Error(`Where for rank ${this.rank} is not yet supported`);\n    }\n\n    if (this.rank === 1) {\n      abCoords = `resRC`;\n      cCoords = `resRC`;\n    } else {\n      const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];\n      const cCoordVars = [];\n      const abCoordVars = [];\n      for (let i = 0; i < this.outputShape.length; i++) {\n        abCoordVars.push(`${currentCoords[i]}`);\n        if (i < this.cRank) {\n          cCoordVars.push(`${currentCoords[i]}`);\n        }\n      }\n      cCoords = cCoordVars.join();\n      abCoords = abCoordVars.join();\n    }\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let resRC = getCoordsFromIndex(index);\n          let cVal = getC(${cCoords});\n          if (cVal >= 1.0) {\n            setOutputAtIndex(index, getA(${abCoords}));\n          } else {\n            setOutputAtIndex(index, getB(${abCoords}));\n          }\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, Select, SelectInputs, TensorInfo, upcastType} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {SelectProgram} from '../select_webgpu';\n\nexport function select(args: {inputs: SelectInputs, backend: WebGPUBackend}):\n    TensorInfo {\n  const {inputs, backend} = args;\n  const {condition, t, e} = inputs;\n\n  const program =\n      new SelectProgram(condition.shape.length, t.shape, t.shape.length);\n  return backend.runWebGPUProgram(\n      program, [condition, t, e], upcastType(t.dtype, e.dtype));\n}\n\nexport const selectConfig: KernelConfig = {\n  kernelName: Select,\n  backendName: 'webgpu',\n  kernelFunc: select as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sigmoid} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const sigmoid = unaryKernelFunc({opType: UnaryOpType.SIGMOID});\n\nexport const sigmoidConfig: KernelConfig = {\n  kernelName: Sigmoid,\n  backendName: 'webgpu',\n  kernelFunc: sigmoid,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sin} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const sin = unaryKernelFunc({opType: UnaryOpType.SIN});\n\nexport const sinConfig: KernelConfig = {\n  kernelName: Sin,\n  backendName: 'webgpu',\n  kernelFunc: sin\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sinh} from '@tensorflow/tfjs-core';\n\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const sinh = unaryKernelFunc({opType: UnaryOpType.SINH});\n\nexport const sinhConfig: KernelConfig = {\n  kernelName: Sinh,\n  backendName: 'webgpu',\n  kernelFunc: sinh\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sub} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {subImplCPU as cpuSub} from '../kernel_utils/shared';\n\nexport const sub = binaryKernelFunc(\n    {opType: BinaryOpType.SUB, cpuKernelImpl: cpuSub, supportsComplex: true});\n\nexport const subConfig: KernelConfig = {\n  kernelName: Sub,\n  backendName: 'webgpu',\n  kernelFunc: sub\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Softmax, SoftmaxAttrs, SoftmaxInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {exp} from './Exp';\nimport {max} from './Max';\nimport {realDiv} from './RealDiv';\nimport {reshape} from './Reshape';\nimport {sub} from './Sub';\nimport {sum} from './Sum';\n\nexport function softmax(\n    args: {inputs: SoftmaxInputs, backend: WebGPUBackend, attrs: SoftmaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {logits} = inputs;\n  const {dim} = attrs;\n\n  const axes = util.parseAxisParam([dim], logits.shape);\n\n  const maxLogit = max({\n    inputs: {x: logits},\n    backend,\n    attrs: {reductionIndices: axes, keepDims: false}\n  });\n\n  const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n\n  const maxLogitsReshaped =\n      reshape({inputs: {x: maxLogit}, backend, attrs: {shape: expandedShape}});\n  const a =\n      sub({inputs: {a: logits, b: maxLogitsReshaped}, backend}) as TensorInfo;\n  const b = exp({inputs: {x: a}, backend}) as TensorInfo;\n  const sumExp =\n      sum({inputs: {x: b}, backend, attrs: {axis: axes, keepDims: false}});\n  const sumExpReshaped =\n      reshape({inputs: {x: sumExp}, backend, attrs: {shape: expandedShape}});\n  const res =\n      realDiv({inputs: {a: b, b: sumExpReshaped}, backend}) as TensorInfo;\n\n  backend.disposeData(maxLogit.dataId);\n  backend.disposeData(maxLogitsReshaped.dataId);\n  backend.disposeData(a.dataId);\n  backend.disposeData(b.dataId);\n  backend.disposeData(sumExp.dataId);\n  backend.disposeData(sumExpReshaped.dataId);\n\n  return res;\n}\n\nexport const softmaxConfig: KernelConfig = {\n  kernelName: Softmax,\n  backendName: 'webgpu',\n  kernelFunc: softmax as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, SpaceToBatchND, SpaceToBatchNDAttrs, SpaceToBatchNDInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {padV2} from './PadV2';\nimport {reshape} from './Reshape';\nimport {transpose} from './Transpose';\n\nexport const spaceToBatchND = (args: {\n  inputs: SpaceToBatchNDInputs,\n  backend: WebGPUBackend,\n  attrs: SpaceToBatchNDAttrs\n}): TensorInfo => {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {blockShape, paddings} = attrs;\n\n  util.assert(\n      x.shape.length <= 4,\n      () => 'spaceToBatchND for rank > 4 with a WebGPU backend not ' +\n          'implemented yet');\n\n  const prod = blockShape.reduce((a, b) => a * b);\n\n  const completePaddings: Array<[number, number]> = [[0, 0]];\n  completePaddings.push(...paddings as Array<[number, number]>);\n  for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {\n    completePaddings.push([0, 0]);\n  }\n\n  const toDispose = [];\n\n  const paddedX = padV2({\n    inputs: {x},\n    backend,\n    attrs: {paddings: completePaddings, constantValue: 0}\n  });\n\n  const reshapedPaddedShape =\n      backend_util.getReshaped(paddedX.shape, blockShape, prod, false);\n\n  const permutedReshapedPaddedPermutation = backend_util.getPermuted(\n      reshapedPaddedShape.length, blockShape.length, false);\n\n  const flattenShape =\n      backend_util.getReshapedPermuted(paddedX.shape, blockShape, prod, false);\n\n  const reshapedPaddedX = reshape(\n      {inputs: {x: paddedX}, backend, attrs: {shape: reshapedPaddedShape}});\n\n  const paddedXT = transpose({\n    inputs: {x: reshapedPaddedX},\n    backend,\n    attrs: {perm: permutedReshapedPaddedPermutation}\n  });\n\n  const result =\n      reshape({inputs: {x: paddedXT}, backend, attrs: {shape: flattenShape}});\n\n  toDispose.push(paddedX);\n  toDispose.push(reshapedPaddedX);\n  toDispose.push(paddedXT);\n\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n\n  return result;\n};\n\nexport const spaceToBatchNDConfig: KernelConfig = {\n  kernelName: SpaceToBatchND,\n  backendName: 'webgpu',\n  kernelFunc: spaceToBatchND as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class TileProgram implements WebGPUProgram {\n  variableNames = ['A'];\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n  rank: number;\n\n  constructor(aShape: number[], reps: number[]) {\n    const outputShape: number[] = new Array(aShape.length);\n    for (let i = 0; i < outputShape.length; i++) {\n      outputShape[i] = aShape[i] * reps[i];\n    }\n    this.outputShape = outputShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.rank = this.outputShape.length;\n    this.shaderKey = 'tile';\n  }\n\n  getUserCode(): string {\n    const sourceCoords = getSourceCoords(this.rank, 'uniforms.');\n\n    const userCode = `\n      ${main('index')} {\n        if (index < uniforms.size) {\n          let resRC = getCoordsFromIndex(index);\n          setOutputAtIndex(index, getA(${sourceCoords}));\n        }\n      }\n    `;\n    return userCode;\n  }\n}\n\nfunction getSourceCoords(rank: number, uniformPrefix = ''): string {\n  if (rank >= 5) {\n    throw Error(`Tile for rank ${rank} is not yet supported`);\n  }\n  if (rank === 1) {\n    return `(resRC % ${uniformPrefix}aShape)`;\n  }\n\n  const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];\n  const sourceCoords = [];\n  for (let i = 0; i < rank; i++) {\n    sourceCoords.push(`(${currentCoords[i]} % ${uniformPrefix}aShape[${i}])`);\n  }\n  return sourceCoords.join();\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, KernelConfig, KernelFunc, TensorInfo, Tile, TileAttrs, TileInputs, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {tileImplCPU} from '../kernel_utils/shared';\nimport {TileProgram} from '../tile_webgpu';\n\nexport function tile(\n    params: {inputs: TileInputs, backend: WebGPUBackend, attrs: TileAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = params;\n  const {x} = inputs;\n  const {reps} = attrs;\n\n  // tile gpu program cannot handle rank >= 5 case.\n  if (backend.shouldExecuteOnCPU([x]) || x.dtype === 'string' ||\n      x.shape.length >= 5) {\n    // Even thought string tensor is always on CPU, just to be consistent on how\n    // to access tensor data.\n    const data = backend.readSync(x.dataId);\n    const value = x.dtype === 'string' ?\n        (data as Uint8Array[]).map(d => util.decodeString(d)) :\n        data as TypedArray;\n    const buf = buffer(x.shape, x.dtype, value);\n    const outBuf = tileImplCPU(buf, reps);\n    return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);\n  }\n\n  const program = new TileProgram(x.shape, reps);\n  const output = backend.runWebGPUProgram(program, [x], x.dtype);\n\n  return output;\n}\n\nexport const tileConfig: KernelConfig = {\n  kernelName: Tile,\n  backendName: 'webgpu',\n  kernelFunc: tile as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Rank, SparseToDense, SparseToDenseAttrs, SparseToDenseInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {scatterImplCPU} from '../kernel_utils/shared';\nimport {ScatterProgram} from '../scatter_webgpu';\n\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\nimport {tile} from './Tile';\n\nexport function sparseToDense(args: {\n  inputs: SparseToDenseInputs,\n  backend: WebGPUBackend,\n  attrs: SparseToDenseAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {sparseIndices, sparseValues, defaultValue} = inputs;\n  const {outputShape} = attrs;\n\n  const {sliceRank, numUpdates, sliceSize, strides, outputSize} =\n      backend_util.calculateShapes(sparseValues, sparseIndices, outputShape);\n\n  const sumDupeIndices = false;\n  if (sparseValues.dtype === 'string') {\n    const indicesBuf = backend.bufferSync<Rank, 'int32'>(sparseIndices);\n    const updatesBuf = backend.bufferSync<Rank, 'string'>(sparseValues);\n    const $defaultValue = util.decodeString(\n        backend.readSync(defaultValue.dataId)[0] as Uint8Array);\n    const outBuf = scatterImplCPU(\n        indicesBuf, updatesBuf, outputShape, outputSize, sliceSize, numUpdates,\n        sliceRank, strides, $defaultValue, sumDupeIndices);\n    return backend.makeTensorInfo(outputShape, outBuf.dtype, outBuf.values);\n  }\n\n  const flattenShape = [outputSize / sliceSize, sliceSize];\n\n  const $sparseIndices = reshape({\n    inputs: {x: sparseIndices},\n    backend,\n    attrs: {shape: [numUpdates, sliceRank]}\n  });\n  const $sparseValues = sparseValues.shape.length ?\n      reshape({\n        inputs: {x: sparseValues},\n        backend,\n        attrs: {shape: [numUpdates, sliceSize]}\n      }) :\n      identity({inputs: {x: sparseValues}, backend});\n\n  const type = $sparseValues.dtype;\n  const zero =\n      backend.makeTensorInfo([], type, util.makeZerosTypedArray(1, type));\n\n  // Fill output tensor with the default value.\n  const $defaultValue = reshape({\n    inputs: {x: defaultValue},\n    backend,\n    attrs: {shape: Array(flattenShape.length).fill(1)}\n  });\n  const $denseValues =\n      tile({inputs: {x: $defaultValue}, backend, attrs: {reps: flattenShape}});\n\n  const size = util.sizeFromShape([numUpdates, sliceSize]);\n  const uniformData = [\n    {type: 'int32', data: [sliceRank]},\n    {type: 'int32', data: strides},\n    {type: 'int32', data: [size]},\n  ];\n\n  switch (numUpdates) {\n    case 0:\n      break;\n    case 1:\n      if (true) {\n        const program = new ScatterProgram(\n            [numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length,\n            $sparseValues.shape.length, strides, flattenShape, type,\n            sumDupeIndices);\n        backend.runWebGPUProgram(\n            program, [$sparseValues, $sparseIndices], type, uniformData,\n            $denseValues);\n      }\n      break;\n    default:\n      if (true) {\n        // First replace the default value with 0 at indices.\n        const program = new ScatterProgram(\n            [numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length,\n            zero.shape.length, strides, flattenShape, type, sumDupeIndices);\n        backend.runWebGPUProgram(\n            program, [zero, $sparseIndices], type, uniformData, $denseValues);\n      }\n      {\n        // Then replace 0 with the (sum of) sparse value(s) at indices.\n        const program = new ScatterProgram(\n            [numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length,\n            $sparseValues.shape.length, strides, flattenShape, type);\n        backend.runWebGPUProgram(\n            program, [$sparseValues, $sparseIndices], type, uniformData,\n            $denseValues);\n      }\n  }\n\n  const denseValues = reshape(\n      {inputs: {x: $denseValues}, backend, attrs: {shape: outputShape}});\n\n  backend.disposeData($sparseIndices.dataId);\n  backend.disposeData($sparseValues.dataId);\n  backend.disposeData($defaultValue.dataId);\n  backend.disposeData(zero.dataId);\n  backend.disposeData($denseValues.dataId);\n  return denseValues;\n}\n\nexport const sparseToDenseConfig: KernelConfig = {\n  kernelName: SparseToDense,\n  backendName: 'webgpu',\n  kernelFunc: sparseToDense as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, SplitV, SplitVAttrs, SplitVInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {slice} from './Slice';\n\nexport function splitV(\n    args: {inputs: SplitVInputs, backend: WebGPUBackend, attrs: SplitVAttrs}):\n    TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {numOrSizeSplits, axis} = attrs;\n\n  const $axis = util.parseAxisParam(axis, x.shape)[0];\n  const splitSizes = backend_util.prepareSplitSize(x, numOrSizeSplits, $axis);\n\n  const xRank = x.shape.length;\n  const begin = new Array(xRank).fill(0);\n  const size = x.shape.slice();\n\n  return splitSizes.map(s => {\n    const sliceSize = [...size];\n    sliceSize[$axis] = s;\n    const sliceT =\n        slice({inputs: {x}, backend, attrs: {begin, size: sliceSize}});\n    begin[$axis] += s;\n    return sliceT;\n  });\n}\n\nexport const splitVConfig: KernelConfig = {\n  kernelName: SplitV,\n  backendName: 'webgpu',\n  kernelFunc: splitV as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Sqrt} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const sqrt = unaryKernelFunc({opType: UnaryOpType.SQRT});\n\nexport const sqrtConfig: KernelConfig = {\n  kernelName: Sqrt,\n  backendName: 'webgpu',\n  kernelFunc: sqrt\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Square, SquareInputs} from '@tensorflow/tfjs-core';\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {UnaryOpProgram} from '../unary_op_webgpu';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const squareConfig: KernelConfig = {\n  kernelName: Square,\n  backendName: 'webgpu',\n  kernelFunc: ({inputs, backend}) => {\n    const {x} = inputs as SquareInputs;\n    const webGPUBackend = backend as WebGPUBackend;\n    const program = new UnaryOpProgram(x.shape, UnaryOpType.SQUARE);\n    return webGPUBackend.runWebGPUProgram(program, [x], x.dtype);\n  }\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, SquaredDifference} from '@tensorflow/tfjs-core';\n\nimport {BinaryOpType} from '../binary_op_util';\nimport {binaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\n\nexport const squaredDifference = binaryKernelFunc({\n  opType: BinaryOpType.SQUARED_DIFFERENCE,\n});\n\nexport const squaredDifferenceConfig: KernelConfig = {\n  kernelName: SquaredDifference,\n  backendName: 'webgpu',\n  kernelFunc: squaredDifference\n};\n","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getCoordsDataType, getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class StridedSliceProgram implements WebGPUProgram {\n  variableNames = ['x'];\n  uniforms: string;\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  // TODO(xing.xu): Increase the workPerThread.\n  workPerThread = 1;\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(destSize: number[]) {\n    this.outputShape = destSize;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize,\n        [this.workPerThread, 1, 1]);\n\n    const dtype = getCoordsDataType(this.outputShape.length);\n    this.uniforms = `begin : ${dtype},  strides : ${dtype}, `;\n    this.shaderKey = 'stridedSlice';\n  }\n\n  getUserCode(): string {\n    const rank = this.outputShape.length;\n    let newCoords = '';\n    if (rank === 1) {\n      newCoords = 'coords * uniforms.strides + uniforms.begin';\n    } else {\n      let outputAxis = 0;\n      newCoords =\n          this.outputShape\n              .map((_, i) => {\n                outputAxis++;\n                return this.outputShape.length === 1 ?\n                    `coords * uniforms.strides[${i}] + uniforms.begin[${i}]` :\n                    `coords[${outputAxis - 1}] * uniforms.strides[${\n                        i}] + uniforms.begin[${i}]`;\n              })\n              .join(',');\n    }\n\n    const userCode = `\n       ${main('index')} {\n         if (index < uniforms.size) {\n           let coords = getCoordsFromIndex(index);\n           setOutputAtIndex(index, getX(${newCoords}));\n         }\n       }\n     `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {buffer, KernelConfig, KernelFunc, Rank, slice_util, StridedSlice, StridedSliceAttrs, StridedSliceInputs, TensorBuffer, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {stridedSliceImplCPU} from '../kernel_utils/shared';\n\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\nimport {StridedSliceProgram} from '../strided_slice_webgpu';\n\nexport function stridedSlice(args: {\n  inputs: StridedSliceInputs,\n  backend: WebGPUBackend,\n  attrs: StridedSliceAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {\n    begin,\n    end,\n    strides,\n    beginMask,\n    endMask,\n    ellipsisMask,\n    newAxisMask,\n    shrinkAxisMask\n  } = attrs;\n\n  const {\n    finalShapeSparse,\n    finalShape,\n    isIdentity,\n    sliceDim0,\n    isSimpleSlice,\n    begin: $begin,\n    end: $end,\n    strides: $strides\n  } =\n      slice_util.sliceInfo(\n          x.shape, begin, end, strides, beginMask, endMask, ellipsisMask,\n          newAxisMask, shrinkAxisMask);\n\n  let result;\n\n  if (isIdentity) {\n    // Optimization #1, slice is a no-op plus reshape\n    result = reshape({inputs: {x}, backend, attrs: {shape: finalShape}});\n  } else if (sliceDim0 || isSimpleSlice) {\n    // Optimization #2, slice is memory contiguous (only occurs in dim 0)\n    util.assert(\n        x.shape.length >= 1,\n        () => `Input must have rank at least 1, got: ${x.shape.length}`);\n\n    const size = slice_util.computeOutShape($begin, $end, $strides);\n    // To tolerate begin[0] > end[0] (a 0-output slice), we min(begin, end).\n    const sliced = slice({inputs: {x}, backend, attrs: {begin: $begin, size}});\n    result =\n        reshape({inputs: {x: sliced}, backend, attrs: {shape: finalShape}});\n    backend.disposeData(sliced.dataId);\n  } else {\n    const shouldExecuteOnCPU = backend.shouldExecuteOnCPU([x]);\n    if (shouldExecuteOnCPU) {\n      const values = backend.readSync(x.dataId) as TypedArray;\n      const xBuf = buffer(x.shape, x.dtype, values) as TensorBuffer<Rank>;\n      const resultValues =\n          stridedSliceImplCPU(finalShapeSparse, xBuf, $strides, $begin);\n      result = backend.makeTensorInfo(finalShape, x.dtype, resultValues.values);\n    } else {\n      const program = new StridedSliceProgram(finalShapeSparse);\n      const uniformData =\n          [{type: 'int32', data: $begin}, {type: 'int32', data: $strides}];\n      const resultValues =\n          backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n      result = reshape(\n          {inputs: {x: resultValues}, backend, attrs: {shape: finalShape}});\n      backend.disposeData(resultValues.dataId);\n    }\n  }\n\n  return result;\n}\n\nexport const stridedSliceConfig: KernelConfig = {\n  kernelName: StridedSlice,\n  backendName: 'webgpu',\n  kernelFunc: stridedSlice as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, StringNGrams, StringNGramsAttrs, StringNGramsInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {stringNGramsImplCPU} from '../kernel_utils/shared';\n\nexport function stringNGrams(args: {\n  inputs: StringNGramsInputs,\n  backend: WebGPUBackend,\n  attrs: StringNGramsAttrs\n}): [TensorInfo, TensorInfo] {\n  const {inputs, backend, attrs} = args;\n  const {\n    separator,\n    nGramWidths,\n    leftPad,\n    rightPad,\n    padWidth,\n    preserveShortSequences\n  } = attrs;\n  const {data, dataSplits} = inputs;\n  const $data = backend.readSync(data.dataId) as Uint8Array[];\n  const $dataSplits = backend.readSync(dataSplits.dataId) as Int32Array;\n\n  const [nGrams, nGramsSplits] = stringNGramsImplCPU(\n      $data, $dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth,\n      preserveShortSequences);\n  return [\n    backend.makeTensorInfo([nGrams.length], 'string', nGrams),\n    backend.makeTensorInfo(dataSplits.shape, 'int32', nGramsSplits),\n  ];\n}\n\nexport const stringNGramsConfig: KernelConfig = {\n  kernelName: StringNGrams,\n  backendName: 'webgpu',\n  kernelFunc: stringNGrams as {} as KernelFunc,\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, Tanh} from '@tensorflow/tfjs-core';\nimport {unaryKernelFunc} from '../kernel_utils/kernel_funcs_utils';\nimport {UnaryOpType} from '../unary_op_util';\n\nexport const tanh = unaryKernelFunc({opType: UnaryOpType.TANH});\n\nexport const tanhConfig: KernelConfig = {\n  kernelName: Tanh,\n  backendName: 'webgpu',\n  kernelFunc: tanh\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\n// Based on Algorithm 2 of Bitonic Top K, ref:\n// https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf\n// The original algorithm is based on computing the top K only, however\n// since for TFJS we require the indices of the top K values as well then the\n// algorithm found here is a bit modified. Rather than producing the values\n// at each step, the indices containing the top K are generated instead.\n// The output values are not generated to reduce the number of outputs in the\n// GPU, the values can easily be retrieved from the indices using a gather\n// op.\n\nexport class SwapProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'indices'];\n  uniforms: string;\n  workGroupSize: [number, number, number] = [256, 1, 1];\n  size = true;\n\n  constructor(shape: number[]) {\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.uniforms = `inputSize : i32, firstPass : i32, negativeInf : f32,\n        dir : i32, inc : i32,`;\n    this.shaderKey = 'swap';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n        ${main('index')} {\n          if (index < uniforms.size) {\n            let outC = getCoordsFromIndex(index);\n            let batch = outC[0];\n            let elemIdx = outC[1];\n            // We compare elements pair-wise within a group of size 2 * inc.\n            // The comparing rule for each group alternates between ascending\n            // and descending. Within each group, we compare each pair at\n            // positions i and i+inc. To decide whether an element at position i\n            // is x0 or x1, we mod it by 2 * inc, if the result is smaller than\n            // inc, it is in the first half of the group, we denote it as x0,\n            // otherwise we denote it as x1.\n            // For example, as shown in the Bitonic top K paper referenced\n            // above, Figure5(a) shows that element[1] is in the second half of\n            // the group when group size is 2, but it is in the first half of\n            // the group when group size is 4.\n            let isFirstInPair = elemIdx % (2 * uniforms.inc) < uniforms.inc;\n            var i = 0;\n            if (isFirstInPair) {\n              i = elemIdx;\n            } else {\n              i = elemIdx - uniforms.inc;\n            }\n\n            var i0 = 0;\n            if (uniforms.firstPass == 1) {\n              i0 = i;\n            } else {\n              i0 = i32(getIndices(batch, i));\n            }\n\n            var i1 = 0;\n            if (uniforms.firstPass == 1) {\n              i1 = i + uniforms.inc;\n            } else {\n              i1 = i32(getIndices(batch, i + uniforms.inc));\n            }\n\n            var x0 = f32(0.0);\n            var x1 = f32(0.0);\n            if (i0 < uniforms.inputSize) {\n              x0 = getX(batch, i0);\n            } else {\n              x0 = uniforms.negativeInf;\n            }\n            if (i1 < uniforms.inputSize) {\n              x1 = getX(batch, i1);\n            } else {\n              x1 = uniforms.negativeInf;\n            }\n\n            let reverse = elemIdx % (2 * uniforms.dir) >= uniforms.dir;\n            let isGreater = x0 > x1 || (x0 == x1 && i1 > i0);\n            if (reverse == isGreater) {\n              // Elements in opposite order of direction\n              let iTemp = i0;\n              i0 = i1;\n              i1 = iTemp;\n            }\n            if (isFirstInPair) {\n              setOutputAtIndex(index, f32(i0));\n            } else {\n              setOutputAtIndex(index, f32(i1));\n            }\n          }\n        }\n      `;\n    return userCode;\n  }\n}\n\nexport class MergeProgram implements WebGPUProgram {\n  outputShape: number[];\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  variableNames = ['x', 'indices'];\n  uniforms: string;\n  workGroupSize: [number, number, number] = [256, 1, 1];\n  size = true;\n\n  constructor(shape: number[]) {\n    this.outputShape = shape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    // |n| Size of the original input of TopK\n    // |firstPass| indicates if this is the first time swap is being used which\n    // means no indices input containing the top K is present yet.\n    // |k| Top k elements desired\n    this.uniforms = `inputSize : i32, firstPass : i32, k : i32,`;\n    this.shaderKey = 'merge';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n        ${main('index')} {\n          if (index < uniforms.size) {\n            let outC = getCoordsFromIndex(index);\n            let batch = outC[0];\n            let elemIdx = outC[1];\n            // The output size is half of the previous size.\n            // If the previous sequence is | | | | _ _ _ _  | | | |  _ _ _ _\n            // (k=4), we only need to output the indices at positions |, the\n            // indices at positions _ can be thrown away, see Figure5(b) After\n            // Phase 2 (Merge phase) in the Bitonic Top K paper referenced\n            // above.\n            // For example, the paper shows we only need to output the orange\n            // bars. The output sequence should look like this | | | | | | | |.\n            // Because the sequence is halved, to map the output index back to\n            // the previous sequence to find the corresponding value, we need\n            // to double the index. When we double the index, we basically\n            // interpolate a position, so 2i looks like\n            // | _ | _ | _ | _ | _ | _ | _. We move the | to the first k\n            // position of each 2k positions by - elemIdx % k. E.g. for output\n            // at index 4,5,6,7, we want to get the corresponding element at\n            // original index 8,9,10,11, for output at index 8,9,10,11,\n            // we want to get the corresponding element at original index\n            // 16,17,18,19, so on and so forth.\n\n            var i = 0;\n            if (elemIdx < uniforms.k) {\n              i = elemIdx;\n            } else {\n              i = elemIdx * 2 - elemIdx % uniforms.k;\n            }\n            var i0 = 0;\n            if (uniforms.firstPass == 1) {\n              i0 = i;\n            } else {\n              i0 = i32(getIndices(batch, i));\n            }\n            var i1 = 0;\n            if (uniforms.firstPass == 1) {\n              i1 = i + uniforms.k;\n            } else {\n              i1 = i32(getIndices(batch, i + uniforms.k));\n            }\n\n            let x0 = getX(batch, i0);\n            var x1 = f32(0.0);\n            if (i1 < uniforms.inputSize) {\n              x1 = getX(batch, i1);\n            } else {\n              x1 = x0;\n            }\n\n            if (x0 >= x1) {\n              setOutputAtIndex(index, f32(i0));\n            } else {\n              setOutputAtIndex(index, f32(i1));\n            }\n          }\n        }\n      `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, NumericDataType, TensorInfo, TopK, TopKAttrs, TopKInputs, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {topKImplCPU} from '../kernel_utils/shared';\nimport {MergeProgram, SwapProgram} from '../top_k_webgpu';\nimport {fill} from './Fill';\nimport {gatherV2} from './GatherV2';\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\n\nfunction disposeIntermediateTensorInfoOrNull(\n    backend: WebGPUBackend, tensorInfo: TensorInfo) {\n  if (tensorInfo !== null) {\n    backend.disposeData(tensorInfo.dataId);\n  }\n}\n\nfunction roundUpToPow2(num: number) {\n  let pow2 = 1;\n  while (pow2 < num) {\n    pow2 *= 2;\n  }\n  return pow2;\n}\n\n// Based on Algorithm 2 of Bitonic Top K, ref:\n// https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf\nexport function topK(\n    args: {inputs: TopKInputs, backend: WebGPUBackend, attrs: TopKAttrs}):\n    TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {k, sorted}= attrs;\n\n  const xShape = x.shape;\n  const lastDim = xShape[xShape.length - 1];\n\n  if (backend.shouldExecuteOnCPU([x])) {\n    const xVals = backend.readSync(x.dataId) as TypedArray;\n    const [allTopKVals, allTopKIndices] =\n        topKImplCPU(xVals, xShape, x.dtype as NumericDataType, k, sorted);\n\n    return [\n      backend.makeTensorInfo(\n          allTopKVals.shape, allTopKVals.dtype, allTopKVals.values),\n      backend.makeTensorInfo(\n          allTopKIndices.shape, allTopKIndices.dtype, allTopKIndices.values)\n    ];\n  }\n\n  if (k === 0) {\n    xShape[xShape.length - 1] = 0;\n    return [\n      backend.makeTensorInfo(xShape, x.dtype, []),\n      backend.makeTensorInfo(xShape, 'int32', [])\n    ];\n  }\n\n  if (lastDim === 1 /* firstPass */) {\n    return [\n      x, fill({attrs: {shape: xShape, dtype: 'int32', value: 0}, backend})\n    ];\n  }\n\n  // Reshape into a 2d tensor [batch, lastDim] and compute topk along lastDim.\n  const xSize = util.sizeFromShape(xShape);\n  const batch = xSize / lastDim;\n  const x2D = reshape({inputs: {x}, attrs: {shape: [batch, lastDim]}, backend});\n\n  const kPow2 = roundUpToPow2(k);\n  const lastDimPow2 = roundUpToPow2(lastDim);\n\n  // Only the indices containing the top K are kept at every step to reduce\n  // number of outputs in the GPU algorithms, so once the final set of indices\n  // is computed then gather is used to grab the corresponding values\n  // from the original input.\n  let indices: TensorInfo = null;\n\n  // GPU algorithm always takes in an indices input but this input is not used\n  // on the first run of a GPU algorithm, therefore if indices is null we simply\n  // pass in x2D instead of it but the value will not actually be used\n  const getInputs = () => indices === null ? [x2D, x2D] : [x2D, indices];\n\n  const runSwap = (dir: number, inc: number, shape: number[]) => {\n    const inputs = getInputs();\n    const program = new SwapProgram(shape);\n    const firstPass = indices === null ? 1 : 0;\n    const uniformDataSwap = [\n        {type: 'int32', data: [lastDim]},\n        {type: 'int32', data: [firstPass]},\n        {type: 'float32', data: [Number.NEGATIVE_INFINITY]},\n        {type: 'int32', data: [dir]},\n        {type: 'int32', data: [inc]}\n    ];\n    const prevIndices = indices;\n    indices = backend.runWebGPUProgram(\n        program, inputs, 'int32', uniformDataSwap);\n    disposeIntermediateTensorInfoOrNull(backend, prevIndices);\n  };\n\n  // Step 1: local sort\n  for (let len = 1; len < kPow2; len *= 2) {\n    const dir = len * 2;\n    for (let inc = len; inc >= 1; inc /= 2) {\n      runSwap(dir, inc, [batch, lastDimPow2]);\n    }\n  }\n\n  // Step 2: merge\n  for (let indicesSize = lastDimPow2; indicesSize > kPow2; indicesSize /= 2) {\n    const inputs = getInputs();\n    const mergeProgram = new MergeProgram([batch, indicesSize / 2]);\n    const firstPass = indices === null ? 1 : 0;\n    const uniformDataMerge = [\n        {type: 'int32', data: [lastDim]},\n        {type: 'int32', data: [firstPass]},\n        {type: 'int32', data: [kPow2]}\n    ];\n    const prevIndices = indices;\n    indices = backend.runWebGPUProgram(\n        mergeProgram, inputs, 'int32', uniformDataMerge);\n    disposeIntermediateTensorInfoOrNull(backend, prevIndices);\n\n    // Step 3: rebuild\n    const len = kPow2 / 2;\n    const dir = len * 2;\n    for (let inc = len; inc >= 1; inc /= 2) {\n      runSwap(dir, inc, indices.shape);\n    }\n  }\n\n  // Keep only the requested top K results instead of kPow2\n  let prevIndices = indices;\n  indices = slice(\n      {inputs: {x: indices}, backend, attrs: {begin: 0, size: [batch, k]}});\n  disposeIntermediateTensorInfoOrNull(backend, prevIndices);\n\n  // Gather values on last dimension\n  let values = gatherV2(\n      {inputs: {x: x2D, indices}, backend, attrs: {axis: 1, batchDims: 1}});\n  disposeIntermediateTensorInfoOrNull(backend, x2D);\n\n  // Reshape back to the original input shape, except that the last\n  // dimension is k.\n  const newShape = xShape.slice(0, -1);\n  newShape.push(k);\n\n  prevIndices = indices;\n  indices = reshape({inputs: {x: indices}, attrs: {shape: newShape}, backend});\n  disposeIntermediateTensorInfoOrNull(backend, prevIndices);\n\n  const prevValues = values;\n  values = reshape({inputs: {x: values}, attrs: {shape: newShape}, backend});\n  disposeIntermediateTensorInfoOrNull(backend, prevValues);\n\n  return [values, indices];\n}\n\nexport const topKConfig: KernelConfig = {\n  kernelName: TopK,\n  backendName: 'webgpu',\n  kernelFunc: topK as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {getMainHeaderString as main, WebGPUProgram} from './webgpu_program';\nimport {computeDispatch, flatDispatchLayout} from './webgpu_util';\n\nexport class TransformProgram implements WebGPUProgram {\n  variableNames = ['Image', 'Transforms'];\n  outputShape: number[];\n  uniforms = 'interpolationModeId : i32, fillModeId : i32, fillValue : f32,';\n  shaderKey: string;\n  dispatchLayout: {x: number[]};\n  dispatch: [number, number, number];\n  workGroupSize: [number, number, number] = [64, 1, 1];\n  size = true;\n\n  constructor(outShape: [number, number, number, number]) {\n    this.outputShape = outShape;\n    this.dispatchLayout = flatDispatchLayout(this.outputShape);\n    this.dispatch = computeDispatch(\n        this.dispatchLayout, this.outputShape, this.workGroupSize);\n    this.shaderKey = 'transform';\n  }\n\n  getUserCode(): string {\n    const userCode = `\n          fn mapCoord(outCoord : f32, len : f32) -> f32{\n            var inCoord = outCoord;\n            if(uniforms.fillModeId == 2) {\n              if (inCoord < 0.0) {\n                if (len <= 1.0) {\n                  inCoord = 0.0;\n                } else {\n                  let sz2 = 2.0 * len;\n                  if (inCoord < sz2) {\n                    inCoord = sz2 * f32(i32(f32(-inCoord / sz2))) +\n                    inCoord;\n                  }\n                  if (inCoord < -len) {\n                    inCoord = inCoord + sz2;\n                  } else {\n                    inCoord = -inCoord - 1.0;\n                  }\n                }\n              } else if (inCoord > len - 1.0) {\n                if (len <= 1.0) {\n                  inCoord = 0.0;\n                } else {\n                  let sz2 = 2.0 * len;\n                  inCoord = inCoord - sz2 * f32(i32(f32(inCoord / sz2)));\n                  if (inCoord >= len) {\n                    inCoord = sz2 - inCoord - 1.0;\n                  }\n                }\n              }\n              return clamp(inCoord, 0.0, len - 1.0);\n            } else if (uniforms.fillModeId == 3) {\n              if (inCoord < 0.0) {\n                if (len <= 1.0) {\n                  inCoord = 0.0;\n                } else {\n                  let sz = len - 1.0;\n                  inCoord = inCoord + len * (f32(i32(f32(-inCoord / sz))) + 1.0);\n                }\n              } else if (inCoord > len - 1.0) {\n                if (len <= 1.0) {\n                  inCoord = 0.0;\n                } else {\n                  let sz = len - 1.0;\n                  inCoord = inCoord - len * f32(i32(f32(inCoord / sz)));\n                }\n              }\n              return clamp(inCoord, 0.0, len - 1.0);\n            } else if (uniforms.fillModeId == 4) {\n              return clamp(outCoord, 0.0, len - 1.0);\n            }\n            return outCoord;\n          }\n          fn readWithFillValue(batch : i32, coordY : i32, coordX : i32,\n            channel : i32) -> f32 {\n            var outputValue : f32;\n            if (0 <= coordY && coordY < uniforms.imageShape[1] && 0 <= coordX && coordX < uniforms.imageShape[2]) {\n                outputValue = getImage(batch, coordY, coordX, channel);\n            } else {\n              outputValue = uniforms.fillValue;\n            }\n            return outputValue;\n          }\n\n          ${main('index')} {\n            if (index < uniforms.size) {\n              let coords = getCoordsFromIndex(index);\n              var outputValue : f32;\n              let batch = coords[0];\n              let x = coords[2];\n              let y = coords[1];\n              let channel = coords[3];\n              let xf = f32(x);\n              let yf = f32(y);\n              let a1 = getTransforms(batch, 0);\n              let a2 = getTransforms(batch, 1);\n              let a3 = getTransforms(batch, 2);\n              let b1 = getTransforms(batch, 3);\n              let b2 = getTransforms(batch, 4);\n              let b3 = getTransforms(batch, 5);\n              let c1 = getTransforms(batch, 6);\n              let c2 = getTransforms(batch, 7);\n              let projection = c1 * xf + c2 * yf + 1.0;\n              if (projection == 0.0) {\n                outputValue = uniforms.fillValue;\n              } else {\n                let inX = (a1 * xf + a2 * yf + a3) / projection;\n                let inY = (b1 * xf + b2 * yf + b3) / projection;\n                let mapX = mapCoord(inX, f32(uniforms.imageShape[2]));\n                let mapY = mapCoord(inY, f32(uniforms.imageShape[1]));\n\n                if (uniforms.interpolationModeId == 1) {\n                  let coordY = i32(round(mapY));\n                  let coordX = i32(round(mapX));\n                  outputValue = readWithFillValue(batch, coordY, coordX,\n                    channel);\n                } else {\n                  let yFloor = floor(mapY);\n                  let xFloor = floor(mapX);\n                  let yCeil = yFloor + 1.0;\n                  let xCeil = xFloor + 1.0;\n                  let valueYFloor = (xCeil - mapX) *\n                  readWithFillValue(batch, i32(yFloor), i32(xFloor), channel) +\n                  (mapX - xFloor) *\n                  readWithFillValue(batch, i32(yFloor), i32(xCeil), channel);\n                  let valueYCeil = (xCeil - mapX) *\n                  readWithFillValue(batch, i32(yCeil), i32(xFloor), channel) +\n                  (mapX - xFloor) *\n                  readWithFillValue(batch, i32(yCeil), i32(xCeil), channel);\n                  outputValue = (yCeil - mapY) * valueYFloor +\n                  (mapY - yFloor) * valueYCeil;\n                }\n              }\n              setOutputAtIndex(index, outputValue);\n            }\n          }\n        `;\n    return userCode;\n  }\n}\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, Transform, TransformAttrs, TransformInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {TransformProgram} from '../transform_webgpu';\n\nexport function transform(args: {\n  inputs: TransformInputs,\n  backend: WebGPUBackend,\n  attrs: TransformAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {image, transforms} = inputs;\n  const {interpolation, fillMode, fillValue, outputShape} = attrs;\n\n  const [batch, imageHeight, imageWidth, numChannels] = image.shape;\n  const [outHeight, outWidth] =\n      outputShape != null ? outputShape : [imageHeight, imageWidth];\n  const outShape =\n      [batch, outHeight, outWidth,\n       numChannels] as [number, number, number, number];\n\n  const program = new TransformProgram(outShape);\n  const interpolationModeId = interpolation === 'nearest' ? 1 : 2;\n  let fillModeId: number;\n  switch (fillMode) {\n    case 'constant':\n      fillModeId = 1;\n      break;\n    case 'reflect':\n      fillModeId = 2;\n      break;\n    case 'wrap':\n      fillModeId = 3;\n      break;\n    case 'nearest':\n      fillModeId = 4;\n      break;\n    default:\n      fillModeId = 1;\n      break;\n  }\n  const uniformData = [\n    {type: 'int32', data: [interpolationModeId]},\n    {type: 'int32', data: [fillModeId]}, {type: 'float32', data: [fillValue]}\n  ];\n  return backend.runWebGPUProgram(\n      program, [image, transforms], 'float32', uniformData);\n}\n\nexport const transformConfig: KernelConfig = {\n  kernelName: Transform,\n  backendName: 'webgpu',\n  kernelFunc: transform as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, TensorInfo, Unpack, UnpackAttrs, UnpackInputs} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\n\nimport {reshape} from './Reshape';\nimport {slice} from './Slice';\n\nexport function unpack(\n    args:\n        {inputs: UnpackInputs, backend: WebGPUBackend, attrs: UnpackAttrs}):\n    TensorInfo[] {\n  const {inputs, backend, attrs} = args;\n  const {value} = inputs;\n  let {axis} = attrs;\n\n  if (axis < 0) {\n    axis += value.shape.length;\n  }\n\n  const x = value;\n  const xRank = x.shape.length;\n\n  const num = value.shape[axis];\n  const outShape: number[] = new Array(xRank - 1);\n  let outIndex = 0;\n  for (let i = 0; i < xRank; i++) {\n    if (i !== axis) {\n      outShape[outIndex++] = x.shape[i];\n    }\n  }\n\n  const toDispose = [];\n\n  const begin = new Array(xRank).fill(0);\n  const size = x.shape.slice();\n  size[axis] = 1;\n  const res: TensorInfo[] = new Array(num);\n  for (let i = 0; i < res.length; i++) {\n    begin[axis] = i;\n    const sliced = slice({inputs: {x}, backend, attrs: {begin, size}});\n    const reshaped =\n        reshape({inputs: {x: sliced}, backend, attrs: {shape: outShape}});\n    res[i] = reshaped;\n\n    toDispose.push(sliced);\n  }\n\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n  return res;\n}\n\nexport const unpackConfig: KernelConfig = {\n  kernelName: Unpack,\n  backendName: 'webgpu',\n  kernelFunc: unpack as {} as KernelFunc\n};\n","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {KernelConfig, registerKernel} from '@tensorflow/tfjs-core';\n\nimport {_fusedMatMulConfig} from './kernels/_FusedMatMul';\nimport {absConfig} from './kernels/Abs';\nimport {addConfig} from './kernels/Add';\nimport {addNConfig} from './kernels/AddN';\nimport {argMaxConfig} from './kernels/ArgMax';\nimport {argMinConfig} from './kernels/ArgMin';\nimport {atan2Config} from './kernels/Atan2';\nimport {avgPoolConfig} from './kernels/AvgPool';\nimport {batchMatMulConfig} from './kernels/BatchMatMul';\nimport {batchToSpaceNDConfig} from './kernels/BatchToSpaceND';\nimport {castConfig} from './kernels/Cast';\nimport {ceilConfig} from './kernels/Ceil';\nimport {clipByValueConfig} from './kernels/ClipByValue';\nimport {complexConfig} from './kernels/Complex';\nimport {concatConfig} from './kernels/Concat';\nimport {conv2DConfig} from './kernels/Conv2D';\nimport {conv2DBackpropInputConfig} from './kernels/Conv2DBackpropInput';\nimport {cosConfig} from './kernels/Cos';\nimport {coshConfig} from './kernels/Cosh';\nimport {cropAndResizeConfig} from './kernels/CropAndResize';\nimport {cumprodConfig} from './kernels/Cumprod';\nimport {cumsumConfig} from './kernels/Cumsum';\nimport {depthToSpaceConfig} from './kernels/DepthToSpace';\nimport {depthwiseConv2dNativeConfig} from './kernels/DepthwiseConv2dNative';\nimport {einsumConfig} from './kernels/Einsum';\nimport {eluConfig} from './kernels/Elu';\nimport {equalConfig} from './kernels/Equal';\nimport {expConfig} from './kernels/Exp';\nimport {expandDimsConfig} from './kernels/ExpandDims';\nimport {expm1Config} from './kernels/Expm1';\nimport {fillConfig} from './kernels/Fill';\nimport {flipLeftRightConfig} from './kernels/FlipLeftRight';\nimport {floorConfig} from './kernels/Floor';\nimport {floorDivConfig} from './kernels/FloorDiv';\nimport {fromPixelsConfig} from './kernels/FromPixels';\nimport {fusedBatchNormConfig} from './kernels/FusedBatchNorm';\nimport {fusedConv2DConfig} from './kernels/FusedConv2D';\nimport {fusedDepthwiseConv2DConfig} from './kernels/FusedDepthwiseConv2D';\nimport {gatherNdConfig} from './kernels/GatherNd';\nimport {gatherV2Config} from './kernels/GatherV2';\nimport {greaterConfig} from './kernels/Greater';\nimport {greaterEqualConfig} from './kernels/GreaterEqual';\nimport {identityConfig} from './kernels/Identity';\nimport {imagConfig} from './kernels/Imag';\nimport {isNaNConfig} from './kernels/IsNaN';\nimport {leakyReluConfig} from './kernels/LeakyRelu';\nimport {lessConfig} from './kernels/Less';\nimport {lessEqualConfig} from './kernels/LessEqual';\nimport {logConfig} from './kernels/Log';\nimport {logicalAndConfig} from './kernels/LogicalAnd';\nimport {logicalNotConfig} from './kernels/LogicalNot';\nimport {maxConfig} from './kernels/Max';\nimport {maximumConfig} from './kernels/Maximum';\nimport {maxPoolConfig} from './kernels/MaxPool';\nimport {meanConfig} from './kernels/Mean';\nimport {minConfig} from './kernels/Min';\nimport {minimumConfig} from './kernels/Minimum';\nimport {mirrorPadConfig} from './kernels/MirrorPad';\nimport {multiplyConfig} from './kernels/Multiply';\nimport {negConfig} from './kernels/Neg';\nimport {nonMaxSuppressionV3Config} from './kernels/NonMaxSuppressionV3';\nimport {nonMaxSuppressionV5Config} from './kernels/NonMaxSuppressionV5';\nimport {notEqualConfig} from './kernels/NotEqual';\nimport {onesLikeConfig} from './kernels/OnesLike';\nimport {packConfig} from './kernels/Pack';\nimport {padV2Config} from './kernels/PadV2';\nimport {powConfig} from './kernels/Pow';\nimport {preluConfig} from './kernels/Prelu';\nimport {prodConfig} from './kernels/Prod';\nimport {rangeConfig} from './kernels/Range';\nimport {realConfig} from './kernels/Real';\nimport {realDivConfig} from './kernels/RealDiv';\nimport {reciprocalConfig} from './kernels/Reciprocal';\nimport {reluConfig} from './kernels/Relu';\nimport {relu6Config} from './kernels/Relu6';\nimport {reshapeConfig} from './kernels/Reshape';\nimport {resizeBilinearConfig} from './kernels/ResizeBilinear';\nimport {resizeNearestNeighborConfig} from './kernels/ResizeNearestNeighbor';\nimport {rotateWithOffsetConfig} from './kernels/RotateWithOffset';\nimport {rsqrtConfig} from './kernels/Rsqrt';\nimport {scatterNdConfig} from './kernels/ScatterNd';\nimport {selectConfig} from './kernels/Select';\nimport {sigmoidConfig} from './kernels/Sigmoid';\nimport {sinConfig} from './kernels/Sin';\nimport {sinhConfig} from './kernels/Sinh';\nimport {sliceConfig} from './kernels/Slice';\nimport {softmaxConfig} from './kernels/Softmax';\nimport {spaceToBatchNDConfig} from './kernels/SpaceToBatchND';\nimport {sparseToDenseConfig} from './kernels/SparseToDense';\nimport {splitVConfig} from './kernels/SplitV';\nimport {sqrtConfig} from './kernels/Sqrt';\nimport {squareConfig} from './kernels/Square';\nimport {squaredDifferenceConfig} from './kernels/SquaredDifference';\nimport {stridedSliceConfig} from './kernels/StridedSlice';\nimport {stringNGramsConfig} from './kernels/StringNGrams';\nimport {subConfig} from './kernels/Sub';\nimport {sumConfig} from './kernels/Sum';\nimport {tanhConfig} from './kernels/Tanh';\nimport {tileConfig} from './kernels/Tile';\nimport {topKConfig} from './kernels/TopK';\nimport {transformConfig} from './kernels/Transform';\nimport {transposeConfig} from './kernels/Transpose';\nimport {unpackConfig} from './kernels/Unpack';\nimport {zerosLikeConfig} from './kernels/ZerosLike';\n\n// List all kernel configs here\nconst kernelConfigs: KernelConfig[] = [\n  _fusedMatMulConfig,\n  absConfig,\n  addConfig,\n  addNConfig,\n  argMaxConfig,\n  argMinConfig,\n  atan2Config,\n  avgPoolConfig,\n  batchMatMulConfig,\n  batchToSpaceNDConfig,\n  castConfig,\n  ceilConfig,\n  clipByValueConfig,\n  complexConfig,\n  concatConfig,\n  conv2DConfig,\n  conv2DBackpropInputConfig,\n  cosConfig,\n  coshConfig,\n  cropAndResizeConfig,\n  cumprodConfig,\n  cumsumConfig,\n  depthToSpaceConfig,\n  depthwiseConv2dNativeConfig,\n  einsumConfig,\n  eluConfig,\n  equalConfig,\n  expConfig,\n  expandDimsConfig,\n  expm1Config,\n  fillConfig,\n  flipLeftRightConfig,\n  fromPixelsConfig,\n  floorConfig,\n  floorDivConfig,\n  fusedBatchNormConfig,\n  fusedConv2DConfig,\n  fusedDepthwiseConv2DConfig,\n  gatherNdConfig,\n  gatherV2Config,\n  greaterConfig,\n  greaterEqualConfig,\n  identityConfig,\n  imagConfig,\n  isNaNConfig,\n  leakyReluConfig,\n  lessConfig,\n  lessEqualConfig,\n  logConfig,\n  logicalAndConfig,\n  logicalNotConfig,\n  maxConfig,\n  maximumConfig,\n  maxPoolConfig,\n  meanConfig,\n  minConfig,\n  minimumConfig,\n  mirrorPadConfig,\n  multiplyConfig,\n  negConfig,\n  nonMaxSuppressionV3Config,\n  nonMaxSuppressionV5Config,\n  notEqualConfig,\n  onesLikeConfig,\n  packConfig,\n  padV2Config,\n  powConfig,\n  preluConfig,\n  prodConfig,\n  rangeConfig,\n  realConfig,\n  realDivConfig,\n  reciprocalConfig,\n  reluConfig,\n  relu6Config,\n  reshapeConfig,\n  resizeBilinearConfig,\n  resizeNearestNeighborConfig,\n  rotateWithOffsetConfig,\n  rsqrtConfig,\n  scatterNdConfig,\n  selectConfig,\n  sigmoidConfig,\n  sinConfig,\n  sinhConfig,\n  sliceConfig,\n  stridedSliceConfig,\n  stringNGramsConfig,\n  softmaxConfig,\n  spaceToBatchNDConfig,\n  sparseToDenseConfig,\n  splitVConfig,\n  sqrtConfig,\n  squareConfig,\n  squaredDifferenceConfig,\n  subConfig,\n  sumConfig,\n  tanhConfig,\n  tileConfig,\n  topKConfig,\n  transformConfig,\n  transposeConfig,\n  unpackConfig,\n  zerosLikeConfig\n];\n\nfor (const kernelConfig of kernelConfigs) {\n  registerKernel(kernelConfig);\n}\n"],"names":["extendStatics","d","b","Object","setPrototypeOf","__proto__","Array","p","prototype","hasOwnProperty","call","__awaiter","thisArg","_arguments","P","generator","Promise","resolve","reject","fulfilled","value","step","next","e","rejected","result","done","then","apply","__generator","body","f","y","t","g","_","label","sent","trys","ops","verb","throw","return","Symbol","iterator","this","n","v","op","TypeError","pop","length","push","__values","o","s","m","i","__read","r","ar","error","__spread","arguments","concat","ENV","env","registerFlag","adapterInfo","vendor","AdapterInfo","device","Map","BufferManager","size","usage","acquireBuffer","mappedAtCreation","key","getBufferKey","freeBuffers","has","set","usedBuffers","numBytesUsed","numUsedBuffers","get","numFreeBuffers","newBuffer_1","shift","numBytesAllocated","newBuffer","createBuffer","buffer","bufferList","bufferIndex","indexOf","Error","splice","mapAsync","GPUMapMode","WRITE","_this","releaseBuffer","err","forEach","buffers","destroy","TextureManager","width","height","format","byteSize","getBytesPerElement","getTextureKey","freeTextures","usedTextures","numUsedTextures","numFreeTextures","newTexture_1","newTexture","createTexture","texture","textureList","textureIndex","textures","symbolicallyComputeStrides","indicesArr","variableName","Math","max","numCoords","shape","map","strides","compileProgram","program","inputsData","output","source","inputInfo","outputData","prefixSnippets","workGroupSize","isFlatDispatch","isFromPixels","mapToWgslTypes","dtype","isVec4","commonSnippet","join","getCoordsFromIndexSnippet","getUserCode","uniformDeclaration","variableNames","x","perDataType","getCoordsDataType","charAt","toLowerCase","slice","outputDataType","stridesDataType","uniforms","uniformShader","curInsertRe","replace","match","preInsertRe","p1","p2","insertAlignment","atomic","variableTypes","coordsSnippet","outShape","dispatchLayout","_a","_b","z","outRank","rank","gatherDimensionsStr","dims","arr","j","dimensions","snippet","getOutputCoordsSnippet","sources","getOutputIndexFromCoordsSnippet","outBufferType","wgslType","type","setOutputSnippet","inputSnippet","isFlatDispatchLayout","res","texName","name","funcName","toUpperCase","inputs","shapeStr","rankStr","getInputAtCoordsSnippet","texFuncSnippet","inRank","util","arraysEqual","broadcastDims","backend_util","getBroadcastDims","rankDiff","getCoordsXYZ","unpackedCoordsSnippet","coordsType","coordsValues","getInputByOutputSnippet","getInputSnippet","makeShader","module","createShaderModule","code","constructor","createComputePipeline","compute","entryPoint","layout","index","getMainHeaderString","_i","params","computeStrides","coords","assert","dispatch","MatMulProgramType","arrayProduct","product","computeDispatch","outputShape","elementsPerThread","computeWorkGroupInfoForMatMul","dimAOuter","dimInner","dimBOuter","transposeA","computeWorkGroupSizeForConv2d","dim0","dim1","computeWorkPerThreadForConv2d","flatDispatchLayout","GPUBytesPerElement","ArrayBufferToTypedArray","data","Float32Array","Int32Array","Uint8Array","from","isWebGPUSupported","window","WorkerGlobalScope","navigator","gpu","tileSize","every","dim","dimIdx","CPU_HANDOFF_SIZE_THRESHOLD","getNumber","_super","WeakSet","webgpu_util.isWebGPUSupported","pipelineCache","queue","currentCommandEncoder","currentComputePass","supportTimeQuery","features","bufferManager","textureManager","tensorMap","DataStorage","engine","querySet","createQuerySet","count","getBool","dummyCanvas","document","createElement","dummyContext","getContext","configure","appendChild","String","__","create","tslib_1.__extends","WebGPUBackend","nextDataId","GPUBufferUsage","STORAGE","COPY_SRC","COPY_DST","dataId","force","tensorDataPendingDisposal","tensorData","decRef","refCount","commandQueueOwnedIds","complexTensorInfos","disposeData","real","imag","releaseResource","delete","numBytesInGPU","numBytesAllocatedInGPU","unreliable","resourceInfo","textureInfo","GPUTexture","releaseTexture","bufferInfo","values","id","ensureComputePassEnded","submit","finish","dispatchNumberInEncoder","uniformPendingDisposal","stagingPendingDisposal","releaseUploadBuffer","createCommandEncoder","end","beginComputePass","staging","MAP_READ","ensureCommandEncoderReady","copyBufferToBuffer","submitQueue","READ","getMappedRange","unmap","undefined","getCurrentTexture","convertAndCacheOnCPU","all","read","ps","realValues","imagValues","vals","mergeRealAndImagArrays","getBufferData","webgpu_util.ArrayBufferToTypedArray","srcTensorData","tensorInfo","makeTensorInfo","tensorRef","makeTensorFromTensorInfo","defaultGpuBufferUsage","bufSize","readSync","strings","decodeString","console","warn","oldActiveTimers","activeTimers","newActiveTimers","outerMostTime","programTimersStack","flattenedActiveTimerQueries","flatten","query","filter","flattenedActiveTimerNames","uploadWaitMs","downloadWaitMs","kernelMs","wallMs","sum","ms","isString","encodeString","write","tensor","info","GPUExternalTexture","createView","offset","getTimeFromQuerySet","webgpu_util.GPUBytesPerElement","sizeFromShape","stagingBuffer","acquireUploadBuffer","MAP_WRITE","arrayBuffer","stagingInfo","programUniform","currentOffset","preLength","offsets","baseAlignment","ceil","ArrayBuffer","Uint32Array","uniformBuffer","UNIFORM","writeBuffer","uniformInfo","outputDtype","programDefinedUniform","getTypedArrayFromDType","uploadToGPU","MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE","limits","maxComputeWorkgroupsPerDimension","dispatchAverage","sqrt","cbrt","reshapeDispatch","bufferShapes","NaN","uniformsType_1","pipeline","input","shapes","shaderKey","types","inputShapesEqualsOutShape","broadcastDimsKey","flatDispatchString","webgpu_program.makeShaderKey","webgpu_program.compileProgram","bindings","tensorToBinding","makeUniforms","bindGroup","createBindGroup","getBindGroupLayout","entries","binding","resource","pass","getComputePass","shouldTimeProgram","writeTimestamp","setPipeline","setBindGroup","dispatchWorkgroups","add","getQueryTime","queryBuffer","QUERY_RESOLVE","dst","resolveQuerySet","arrayBuf","BigUint64Array","timeElapsedNanos","Number","sizeThreshold","numDataIds","disposed","dispose","KernelBackend","BinaryOpType","registerBackend","gpuDescriptor","powerPreference","requestAdapter","adapter","adapterLimits","deviceDescriptor","requiredLimits","maxComputeWorkgroupStorageSize","maxStorageBufferBindingSize","requiredFeatures","requestDevice","requestAdapterInfo","UnaryOpType","CHECK_NAN_SNIPPET_VEC4_INNER","CHECK_NAN_SNIPPET_VEC4","getBinaryWithNanString","useVec4","valueForNaN","checkNanSnippet","getBinaryOpString","MUL","ADD","ATAN2","SUB","DIV","EQUAL","GREATER","GREATER_EQUAL","LESS","LESS_EQUAL","LOGICAL_AND","NOT_EQUAL","SQUARED_DIFFERENCE","INT_DIV","PRELU","MAX","MIN","POW","COMPLEX_MULTIPLY_REAL","COMPLEX_MULTIPLY_IMAG","getUnaryOpString","ABS","COS","COSH","CEIL","ELU","EXP","EXPM1","FLOOR","IS_NAN","LINEAR","LOG","LOGICAL_NOT","NEG","LEAKYRELU","RECIPROCAL","RELU","RELU6","RSQRT","SIGMOID","SIN","SINH","SQRT","SQUARE","TANH","TO_INT","typeSnippet","component","activationFnSnippet","activation","hasPreluActivationWeights","packed","coordsLength","activationOpSnippet","dataType","biasActivationSnippet","hasBias","matMulReadFnSource","batchAEqualOne","batchBEqualOne","transposeB","fitAOuter","fitBOuter","fitInner","sampleA","sampleB","matMulReadWriteFnSource","makeMatMulPackedVec4Source","workPerThread","tileInner","splitK","splitedDimInner","isVectorA","tileAOuter","tileBOuter","tileAWidth","tileAHight","innerElementSize","rowPerThreadB","transpose","writeDataToSubAVec4Snippet","calculateResultSnippet","writeDataToSubASnippet","makeMatMulPackedSource","sequentialAccessByThreads","rowPerThreadA","colPerThreadA","matmulSnippet","readDataFromSubASnippet","aShape","bias","preluActivationWeights","workGroupInfo","addBias","MatMulPackedProgram","userCode","main","readVectorASnippet","MatMulReduceProgram","bShape","MatMulSmallOutputSizeProgram","MatMulSplitKProgram","atomicAddSnippet","BiasActivationProgram","FillProgram","fill","args","backend","attrs","inferDtype","getArrayFromDType","uniformData","runWebGPUProgram","fillConfig","kernelName","Fill","backendName","kernelFunc","reshape","xSize","$shape","inferFromImplicitShape","$xSize","incRef","reshapeConfig","Reshape","batchMatMulImpl","a","_d","_e","_f","leakyreluAlpha","_g","aRank","bRank","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","outerDimsB","batchDimA","batchDimB","broadcast_util","assertAndGetBroadcastShape","out","a3dShape","b3dShape","a3d","b3d","intermediates","batchDim","matmulProgramType","biasActivationProgram","activationInputs","outActivated","outReshaped_1","intermediates_1","tslib_1.__values","isIntel","outReshaped","intermediates_2","_fusedMatMulConfig","_FusedMatMul","BinaryOpComplexProgram","useSharedMemoryWithA","useSharedMemoryWithB","lastDimensionSize","BinaryOpProgram","dType","opFnStr","sharedIndexSnippet","accessDataSnippet","identity","identityConfig","Identity","complex","complexInfo","realTensorInfo","imagTensorInfo","complexConfig","Complex","UnaryOpProgram","unaryKernelFunc","opType","cpuKernelImpl","webgpuBackend","$dtype","shouldExecuteOnCPU","xData","outValues","binaryKernelFunc","supportsComplex","aData","bData","realProgram","imagProgram","inputs_1","complexOutput","upcastType","decodedAVals","fromUint8ToStringArray","decodedBVals","_c","createSimpleBinaryKernelImpl","aVals","bVals","newShape","resultRank","resultStrides","resultSize","aStrides","bStrides","aBroadcastDims","bBroadcastDims","loc","indexToLoc","aLoc","aIndex","locToIndex","bLoc","bIndex","addImpl","createSimpleUnaryImpl","newValues","ceilImpl","xi","equalImpl","expImpl","exp","expm1Impl","expm1","floorImpl","floor","greaterImpl","greaterEqualImpl","lessImpl","lessEqualImpl","logImpl","log","maximumImpl","aValue","bValue","minimumImpl","min","multiplyImpl","notEqualImpl","RowPartitionType","rsqrtImpl","separator","nGramWidths","leftPad","rightPad","padWidth","preserveShortSequences","preserveShort","StringNGramsOp","nGramWidth","getPadWidth","splitIndex","outputStartIndex","numNGrams","nGramIndex","this_1","leftPadding","rightPadding","numTokens","dataStartIndex","nGramSize","nGram","nextNGramIndex","appendToNGram","str","splits","inputDataSize","splitsSize","prevSplit","validSplits","numBatchItems","nGramsSplits","empty","this_2","getNumNGrams","nGrams","outputStartIdx","this_3","createNGrams","dataLength","subImpl","comparePair","valueDiff","select","array","k","left","right","i_1","sd","sign","swap","addImplCPU","castImplCPU","inputType","zero","toTypedArray","resultData","ceilImplCPU","concatImplCPU","simplyConcat","outVals","offset_1","colOffset_1","decodedData","tIdx","row","resIdx","col","equalImplCPU","expImplCPU","expm1ImplCPU","floorImplCPU","gatherNdImplCPU","indicesData","paramsBuf","numSlices","sliceRank","sliceSize","paramsShape","paramsSize","outBuf","flattenIndex","gatherV2ImplCPU","xBuf","indicesBuf","flattenOutputShape","originalLoc","batchIdx","indicesIdx","indicesIndex","originalIndex","greaterEqualImplCPU","greaterImplCPU","lessEqualImplCPU","lessImplCPU","logImplCPU","maxImplCPU","reduceSize","isNaN","maximumImplCPU","minimumImplCPU","multiplyImplCPU","negImplCPU","xVals","xShape","xDtype","minusOne","createScalarValue","notEqualImplCPU","prodImplCPU","reductionAxes","reduceShape","outDtype","makeZerosTypedArray","prod_1","rangeImplCPU","start","stop","numElements","abs","rsqrtImplCPU","scatterImplCPU","indices","updates","outputSize","numUpdates","defaultValue","sumDupeIndices","flattenShape","updatesData","simpleAbsImplCPU","resultValues","sliceImplCPU","begin","isContinous","slice_util","isSliceContinous","xStrides","flatOffset","computeFlatOffset","subarray","inBuf","outLoc","inLoc","idx","fromStringArrayToUint8","stridedSliceImplCPU","newLoc","stringNGramsImplCPU","dataSplits","subImplCPU","tileImplCPU","reps","topKImplCPU","sorted","lastDim","batch","allTopKVals","allTopKIndices","valAndInd","sort","outOffset","topKVals","topKIndices","transposeImplCPU","perm","xRank","newStrides","absConfig","Abs","addKernelFunc","cpuAdd","addConfig","Add","AddNPackedProgram","snippets","variable","operation","addNConfig","AddN","tensors","reduce","d1","d2","inputShape","axis","reduceType","axes","ArgMinMaxProgram","getInputShapeLastDim","splitOutputCoords","newDim","TransposeSharedProgram","TransposeProgram","switched","switchedCoords","getSwitchedCoords","cpuTranspose","program_1","transposeConfig","Transpose","argMaxConfig","ArgMax","parseAxisParam","permutedAxes","getAxesPermutation","$x","intermediateTensorInfos","getInnerMostAxes","assertAxesAreInnerMostDims","NEGATIVE_INFINITY","argMinConfig","ArgMin","POSITIVE_INFINITY","atan2","atan2Config","Atan2","convInfo","poolType","Pool2DProgram","updateSnippet","returnValue","PoolWithFilterSizeEqualsOneProgram","reduceInfo","batchSize","inSize","ReduceProgram","reduceOp","initValue","outputSnippet","keepDims","toDispose","origAxes","reduceOutShape","resOutShape","expandShapeToKeepDim","windowSize","outSize","sumOutType","reduced","maxConfig","Max","mean","meanConfig","Mean","poolImpl","filterWidth","filterHeight","inShape","inWidth","inHeight","padInfo","reshapeX","reduceX","reductionIndices","strideHeight","strideWidth","top","dilationHeight","dilationWidth","effectiveFilterHeight","effectiveFilterWidth","avgPoolConfig","AvgPool","filterSize","pad","dimRoundingMode","computePool2DInfo","batchMatMulConfig","BatchMatMul","destSize","SliceProgram","coordSum","sourceCoords","coord","getCoords","$begin","$size","assertParamsValid","xBufferInfo","sliceConfig","Slice","batchToSpaceNDConfig","BatchToSpaceND","blockShape","crops","prod","reshaped","getReshaped","permuted","getPermuted","reshapedPermuted","getReshapedPermuted","sliceBeginCoords","getSliceBeginCoords","getSliceSize","reshapedIntermediate","transposedIntermediate","reshapedIntermediate2","sliced","notEqual","cpuNotEqual","notEqualConfig","NotEqual","realConfig","Real","castConfig","Cast","cast","zerosTensor","tf","zeros","floatX","realPart","hasEncodingLoss","resultShape","resultType","int","zerosTensorInfo","ceilConfig","Ceil","ClipVec4Program","ClipProgram","clipByValueConfig","ClipByValue","computeOutShape","offsetLength","ConcatProgram","lastIndex","lastShiftIndex","imagConfig","Imag","concatImpl","reals","imags","realConcated","imagConcated","runOnCpu","tensors2D_1","innerSize","inputsValShapes","outShape_1","finalOutShape","outInfo","maxInputNum","maxStorageBuffersPerShaderStage","reducedInputs","subArray","reducedInputs_1","tensors2D","reshapedResult","$axis","assertParamsConsistent","$inputs","concatConfig","Concat","isChannelsLast","dataFormat","inChannels","outWidth","outChannels","Conv2DMMProgram","matMulSource","elementsSize","innerElementSizeX","innerElementSizeW","coordASnippet","coordResSnippet","xHight","xWidth","readXSnippet","getXSnippet","sampleX","sampleW","getWSnippet","resType","aType","bType","conv2dCommonSnippet","Conv2DNaiveProgram","getShapeForBatchMatMul","conv2DImpl","sameSize","useNaiveConv2d","xReshaped","filterReshaped","targetShape","sharedDim","conv2dByMatMul","outHeight","inputVar","conv2DConfig","Conv2D","dilations","$dataFormat","convertConv2DDataFormat","computeConv2DInfo","Conv2DDerInputMMProgram","conv2dTransposeCommonSnippet","Conv2DDerInputProgram","rowDim","colDim","channelDim","CumOpType","conv2DBackpropInputConfig","Conv2DBackpropInput","dy","cos","cosConfig","Cos","cosh","coshConfig","Cosh","channnel","boxShape","cropSize","method","numBoxes","methodId","cropHeightBiggerThan1","cropWidthBiggerThan1","CropAndResizeProgram","inputHeightFloat","inputWidthFloat","heightRatio","heightScale","inY","widthRatio","widthScale","inX","cropAndResizeConfig","CropAndResize","image","boxes","boxInd","extrapolationValue","exclusive","reverse","CumProgram","initVal","Prod","val","condition","idxString","getFinalCoord","cumImpl","permutation","permutedX","permutedAxis","log2","prevResult","reverseTransposedResult","getUndoAxesPermutation","cumprodConfig","Cumprod","cumsumConfig","Cumsum","Sum","DepthToSpaceProgram","getHeightCoordString","getWidthCoordString","getDepthCoordString","getOutputDepthSize","getInputSamplingString","depthToSpaceConfig","DepthToSpace","blockSize","outputHeight","outputWidth","outputDepth","hasPreluActivation","DepthwiseConv2DNCHWSharedProgram","tileAHeight","DepthwiseConv2DVec4Program","xNumber","DepthwiseConv2DProgram","depthwiseConv2dNativeConfig","DepthwiseConv2dNative","$dilations","multiplyKernelFunc","cpuMultiply","multiplyConfig","Multiply","sumConfig","einsumConfig","Einsum","equation","allDims","summedDims","idDims","checkEinsumDimSizes","path","steps","nSteps","numDimsRemaining","tensorsToDispose","idTerm","dimsToExpand","isIdentityPermutation","tensorsToDispose_1","elu","eluConfig","Elu","equal","cpuEqual","equalConfig","Equal","expConfig","Exp","expandDims","inputRank","$dim","fromPixels2DContext","expandDimsConfig","ExpandDims","expm1Config","Expm1","imageShape","FlipLeftRightProgram","flipLeftRightConfig","FlipLeftRight","floorConfig","Floor","floorDiv","floorDivConfig","FloorDiv","numChannels","importVideo","FromPixelsProgram","textureLoad","fromPixelsConfig","FromPixels","pixels","isVideo","HTMLVideoElement","isImage","HTMLImageElement","isCanvas","HTMLCanvasElement","OffscreenCanvas","isImageBitmap","ImageBitmap","isVideoOrImage","newWillReadFrequently","willReadFrequently","canvas","drawImage","GPUTextureUsage","RENDER_ATTACHMENT","TEXTURE_BINDING","acquireTexture","copyExternalImageToTexture","imageData","pixelArray","meanShape","varianceShape","offsetShape","scaleShape","BatchNormProgram","offsetSnippet","scaleSnippet","fusedBatchNormConfig","FusedBatchNorm","scale","variance","varianceEpsilon","webGPUBackend","batchNormInputs","fusedConv2DConfig","FusedConv2D","fusedDepthwiseConv2DConfig","FusedDepthwiseConv2D","eitherStridesOrDilationsAreOne","programInputs","sliceDim","GatherNDProgram","strideString","gatherNdConfig","GatherNd","indicesShape","flattenIndices","flattenX","bufferSync","outValue","GatherProgram","currentCoords","getSourceCoords","gatherV2","batchDims","parsedAxis","shapeInfo","segment_util","collectGatherOpShapeInfo","indicesSize","outerSize","dimSize","indicesValues","xValues","gatherV2Config","GatherV2","greater","cpuGreater","greaterConfig","Greater","greaterEqual","cpuGreaterEqual","greaterEqualConfig","GreaterEqual","isNaNConfig","IsNan","leakyReluConfig","LeakyRelu","less","cpuLess","lessConfig","Less","lessEqual","cpuLessEqual","lessEqualConfig","LessEqual","logConfig","Log","logicalAnd","logicalAndConfig","LogicalAnd","logicalNot","logicalNotConfig","LogicalNot","maximum","cpuMaximum","maximumConfig","Maximum","maxPoolConfig","MaxPool","minConfig","Min","minimum","cpuMinimum","minimumConfig","Minimum","paddings","mode","MirrorPadProgram","shaderStart","shaderEnd","shaderOutC","unpackedCoords","mirrorPadConfig","MirrorPad","negConfig","Neg","nonMaxSuppressionV3Config","NonMaxSuppressionV3","scores","maxOutputSize","iouThreshold","scoreThreshold","boxesVals","scoresVals","selectedIndices","nonMaxSuppressionV5Config","NonMaxSuppressionV5","softNmsSigma","maxOutputSizeVal","iouThresholdVal","scoreThresholdVal","softNmsSigmaVal","selectedScores","zerosLike","imagPart","zerosLikeConfig","ZerosLike","onesLikeConfig","OnesLike","onesLike","packConfig","Pack","assertShapesMatch","expandedT","PadProgram","startValue","endValue","leftPadCondition","rightPadCondition","padV2","constantValue","padV2Config","PadV2","pow","powConfig","Pow","preluConfig","Prelu","alpha","prodConfig","rangeConfig","Range","realDiv","realDivConfig","RealDiv","reciprocal","reciprocalConfig","Reciprocal","relu","reluConfig","Relu","relu6","relu6Config","Relu6","newHeight","newWidth","ResizeBilinearProgram","resizeBilinearConfig","ResizeBilinear","images","alignCorners","halfPixelCenters","ResizeNearestNeighborProgram","sourceFracIndexRC","resizeNearestNeighborConfig","ResizeNearestNeighbor","fillValue","fillSnippet","RotateProgram","rotateWithOffsetConfig","RotateWithOffset","radians","center","sin","parseFloat","toFixed","rsqrt","rsqrtConfig","Rsqrt","flattenXShape","indicesRank","updatesRank","sliceDimGreaterThanOne","stridesType","ScatterProgram","indicesString","indicesSnippet","outCoordsString","getUpdatesCoordsFromFlatIndex","updatesSnippet","ptr","atomicStoreSnippet","atomicRMW","scatterNdConfig","ScatterNd","cRank","SelectProgram","cCoords","abCoords","cCoordVars","abCoordVars","selectConfig","Select","sigmoid","sigmoidConfig","Sigmoid","sinConfig","Sin","sinh","sinhConfig","Sinh","sub","cpuSub","subConfig","Sub","softmaxConfig","Softmax","logits","maxLogit","expandedShape","maxLogitsReshaped","sumExp","sumExpReshaped","spaceToBatchNDConfig","SpaceToBatchND","completePaddings","paddedX","reshapedPaddedShape","permutedReshapedPaddedPermutation","reshapedPaddedX","paddedXT","TileProgram","uniformPrefix","tile","buf","tileConfig","Tile","sparseToDenseConfig","SparseToDense","sparseIndices","sparseValues","updatesBuf","$defaultValue_1","$sparseIndices","$sparseValues","$defaultValue","$denseValues","denseValues","splitVConfig","SplitV","numOrSizeSplits","splitSizes","prepareSplitSize","sliceT","sqrtConfig","Sqrt","squareConfig","Square","squaredDifference","squaredDifferenceConfig","SquaredDifference","StridedSliceProgram","newCoords","outputAxis_1","stridedSliceConfig","StridedSlice","beginMask","endMask","ellipsisMask","newAxisMask","shrinkAxisMask","finalShapeSparse","finalShape","isIdentity","sliceDim0","isSimpleSlice","$end","$strides","stringNGramsConfig","StringNGrams","$data","$dataSplits","tanh","tanhConfig","Tanh","SwapProgram","MergeProgram","disposeIntermediateTensorInfoOrNull","roundUpToPow2","num","pow2","topKConfig","TopK","x2D","kPow2","lastDimPow2","getInputs","runSwap","dir","inc","uniformDataSwap","prevIndices","len","mergeProgram","uniformDataMerge","prevIndices_1","prevValues","TransformProgram","transformConfig","Transform","fillModeId","transforms","interpolation","fillMode","imageHeight","imageWidth","interpolationModeId","kernelConfigs","Unpack","outIndex","kernelConfigs_1","kernelConfig","registerKernel"],"mappings":";;;;;;;;;;;;;;;;2kBAgBIA,EAAgB,SAASC,EAAGC,GAI5B,OAHAF,EAAgBG,OAAOC,gBAClB,CAAEC,UAAW,cAAgBC,OAAS,SAAUL,EAAGC,GAAKD,EAAEI,UAAYH,IACvE,SAAUD,EAAGC,GAAK,IAAK,IAAIK,KAAKL,EAAOC,OAAOK,UAAUC,eAAeC,KAAKR,EAAGK,KAAIN,EAAEM,GAAKL,EAAEK,KACzFP,EAAcC,EAAGC,EAC5B,WAgDgBS,EAAUC,EAASC,EAAYC,EAAGC,GAE9C,OAAO,IAAKD,IAAMA,EAAIE,WAAU,SAAUC,EAASC,GAC/C,SAASC,EAAUC,GAAS,IAAMC,EAAKN,EAAUO,KAAKF,IAAW,MAAOG,GAAKL,EAAOK,IACpF,SAASC,EAASJ,GAAS,IAAMC,EAAKN,EAAiB,MAAEK,IAAW,MAAOG,GAAKL,EAAOK,IACvF,SAASF,EAAKI,GAJlB,IAAeL,EAIaK,EAAOC,KAAOT,EAAQQ,EAAOL,QAJ1CA,EAIyDK,EAAOL,MAJhDA,aAAiBN,EAAIM,EAAQ,IAAIN,GAAE,SAAUG,GAAWA,EAAQG,OAITO,KAAKR,EAAWK,GAClGH,GAAMN,EAAYA,EAAUa,MAAMhB,EAASC,GAAc,KAAKS,UAEtE,UAEgBO,EAAYjB,EAASkB,GACjC,IAAsGC,EAAGC,EAAGC,EAAGC,EAA3GC,EAAI,CAAEC,MAAO,EAAGC,KAAM,WAAa,GAAW,EAAPJ,EAAE,GAAQ,MAAMA,EAAE,GAAI,OAAOA,EAAE,IAAOK,KAAM,GAAIC,IAAK,IAChG,OAAOL,EAAI,CAAEZ,KAAMkB,EAAK,GAAIC,MAASD,EAAK,GAAIE,OAAUF,EAAK,IAAwB,mBAAXG,SAA0BT,EAAES,OAAOC,UAAY,WAAa,OAAOC,OAAUX,EACvJ,SAASM,EAAKM,GAAK,OAAO,SAAUC,GAAK,OACzC,SAAcC,GACV,GAAIjB,EAAG,MAAM,IAAIkB,UAAU,mCAC3B,KAAOd,OACH,GAAIJ,EAAI,EAAGC,IAAMC,EAAY,EAARe,EAAG,GAAShB,EAAU,OAAIgB,EAAG,GAAKhB,EAAS,SAAOC,EAAID,EAAU,SAAMC,EAAEvB,KAAKsB,GAAI,GAAKA,EAAEV,SAAWW,EAAIA,EAAEvB,KAAKsB,EAAGgB,EAAG,KAAKtB,KAAM,OAAOO,EAE3J,OADID,EAAI,EAAGC,IAAGe,EAAK,CAAS,EAARA,EAAG,GAAQf,EAAEb,QACzB4B,EAAG,IACP,KAAK,EAAG,KAAK,EAAGf,EAAIe,EAAI,MACxB,KAAK,EAAc,OAAXb,EAAEC,QAAgB,CAAEhB,MAAO4B,EAAG,GAAItB,MAAM,GAChD,KAAK,EAAGS,EAAEC,QAASJ,EAAIgB,EAAG,GAAIA,EAAK,CAAC,GAAI,SACxC,KAAK,EAAGA,EAAKb,EAAEI,IAAIW,MAAOf,EAAEG,KAAKY,MAAO,SACxC,QACI,KAAMjB,EAAIE,EAAEG,MAAML,EAAIA,EAAEkB,OAAS,GAAKlB,EAAEA,EAAEkB,OAAS,KAAkB,IAAVH,EAAG,IAAsB,IAAVA,EAAG,IAAW,CAAEb,EAAI,EAAG,SACjG,GAAc,IAAVa,EAAG,MAAcf,GAAMe,EAAG,GAAKf,EAAE,IAAMe,EAAG,GAAKf,EAAE,IAAM,CAAEE,EAAEC,MAAQY,EAAG,GAAI,MAC9E,GAAc,IAAVA,EAAG,IAAYb,EAAEC,MAAQH,EAAE,GAAI,CAAEE,EAAEC,MAAQH,EAAE,GAAIA,EAAIe,EAAI,MAC7D,GAAIf,GAAKE,EAAEC,MAAQH,EAAE,GAAI,CAAEE,EAAEC,MAAQH,EAAE,GAAIE,EAAEI,IAAIa,KAAKJ,GAAK,MACvDf,EAAE,IAAIE,EAAEI,IAAIW,MAChBf,EAAEG,KAAKY,MAAO,SAEtBF,EAAKlB,EAAKpB,KAAKE,EAASuB,GAC1B,MAAOZ,GAAKyB,EAAK,CAAC,EAAGzB,GAAIS,EAAI,UAAeD,EAAIE,EAAI,EACtD,GAAY,EAARe,EAAG,GAAQ,MAAMA,EAAG,GAAI,MAAO,CAAE5B,MAAO4B,EAAG,GAAKA,EAAG,QAAK,EAAQtB,MAAM,GArB9BL,CAAK,CAACyB,EAAGC,KAuB7D,UAkBgBM,EAASC,GACrB,IAAIC,EAAsB,mBAAXZ,QAAyBA,OAAOC,SAAUY,EAAID,GAAKD,EAAEC,GAAIE,EAAI,EAC5E,GAAID,EAAG,OAAOA,EAAE9C,KAAK4C,GACrB,GAAIA,GAAyB,iBAAbA,EAAEH,OAAqB,MAAO,CAC1C7B,KAAM,WAEF,OADIgC,GAAKG,GAAKH,EAAEH,SAAQG,OAAI,GACrB,CAAElC,MAAOkC,GAAKA,EAAEG,KAAM/B,MAAO4B,KAG5C,MAAM,IAAIL,UAAUM,EAAI,0BAA4B,kCACxD,UAEgBG,EAAOJ,EAAGR,GACtB,IAAIU,EAAsB,mBAAXb,QAAyBW,EAAEX,OAAOC,UACjD,IAAKY,EAAG,OAAOF,EACf,IAAmBK,EAAYpC,EAA3BkC,EAAID,EAAE9C,KAAK4C,GAAOM,EAAK,GAC3B,IACI,WAAc,IAANd,GAAgBA,KAAM,MAAQa,EAAIF,EAAEnC,QAAQI,MAAMkC,EAAGR,KAAKO,EAAEvC,OAExE,MAAOyC,GAAStC,EAAI,CAAEsC,MAAOA,WAEzB,IACQF,IAAMA,EAAEjC,OAAS8B,EAAIC,EAAU,SAAID,EAAE9C,KAAK+C,WAExC,GAAIlC,EAAG,MAAMA,EAAEsC,OAE7B,OAAOD,CACX,UAGgBE,IACZ,IAAK,IAAIF,EAAK,GAAIH,EAAI,EAAGA,EAAIM,UAAUZ,OAAQM,IAC3CG,EAAKA,EAAGI,OAAON,EAAOK,UAAUN,KACpC,OAAOG,CACX,CC1IA,IAAMK,EAAMC,QAGZD,EAAIE,aAAa,qCAAqC,WAAM,OAAA,MAM5DF,EAAIE,aAAa,sBAAsB,WAAM,OAAA,KAO7CF,EAAIE,aAAa,8BAA8B,WAAM,OAAC,KAMtDF,EAAIE,aAAa,qCAAqC,WAAM,OAAA,KAM5DF,EAAIE,aAAa,4BAA4B,WAAM,OAAA,KAQnDF,EAAIE,aAAa,qCAAqC,WAAM,OAAA,OAM5DF,EAAIE,aAAa,2BAA2B,WAAM,OAAA,KAKlDF,EAAIE,aAAa,kCAAkC,WAAM,OAAA,KAKzDF,EAAIE,aAAa,iCAAiC,WAAM,OAAA,KC/CxD,iBAGE,WAAYC,GACNA,IACFvB,KAAKwB,OAASD,EAAYC,eAI9BC,oBAAA,WACE,MAAuB,UAAhBzB,KAAKwB,0BCRd,WAAoBE,GAAA1B,YAAA0B,EARZ1B,oBAAiB,EACjBA,oBAAiB,EACjBA,iBAAwC,IAAI2B,IAC5C3B,iBAAwC,IAAI2B,IAE7C3B,kBAAe,EACfA,uBAAoB,SAI3B4B,gCAAA,SAAoBC,EAAcC,GAChC,OAAO9B,KAAK+B,cAAcF,EAAMC,GAAO,IAGzCF,0BAAA,SACIC,EAAcC,EAA4BE,gBAAAA,MAC5C,IAAMC,EAAMC,EAAaL,EAAMC,GAY/B,GAXK9B,KAAKmC,YAAYC,IAAIH,IACxBjC,KAAKmC,YAAYE,IAAIJ,EAAK,IAGvBjC,KAAKsC,YAAYF,IAAIH,IACxBjC,KAAKsC,YAAYD,IAAIJ,EAAK,IAG5BjC,KAAKuC,cAAgBV,EACrB7B,KAAKwC,iBAEDxC,KAAKmC,YAAYM,IAAIR,GAAK3B,OAAS,EAAG,CACxCN,KAAK0C,iBAEL,IAAMC,EAAY3C,KAAKmC,YAAYM,IAAIR,GAAKW,QAE5C,OADA5C,KAAKsC,YAAYG,IAAIR,GAAK1B,KAAKoC,GACxBA,EAGT3C,KAAK6C,mBAAqBhB,EAC1B,IAAMiB,EAAY9C,KAAK0B,OAAOqB,aAAa,CAAClB,OAAMC,QAAOE,qBAGzD,OAFAhC,KAAKsC,YAAYG,IAAIR,GAAK1B,KAAKuC,GAExBA,GAGTlB,0BAAA,SAAcoB,EAAmBnB,EAAcC,GAC7C,GAA8B,IAA1B9B,KAAKmC,YAAYN,KAArB,CAIA,IAAMI,EAAMC,EAAaL,EAAMC,GAC1B9B,KAAKmC,YAAYC,IAAIH,IACxBjC,KAAKmC,YAAYE,IAAIJ,EAAK,IAG5BjC,KAAKmC,YAAYM,IAAIR,GAAK1B,KAAKyC,GAC/BhD,KAAK0C,iBACL1C,KAAKwC,iBAEL,IAAMS,EAAajD,KAAKsC,YAAYG,IAAIR,GAClCiB,EAAcD,EAAWE,QAAQH,GACvC,GAAIE,EAAc,EAChB,MAAM,IAAIE,MACN,0EAGNH,EAAWI,OAAOH,EAAa,GAC/BlD,KAAKuC,cAAgBV,IAGvBD,gCAAA,SACIoB,EAAmBnB,EAAcC,GADrC,WAEEkB,EAAOM,SAASC,WAAWC,OACtB1E,MACG,WACE2E,EAAKC,cAAcV,EAAQnB,EAAMC,MAEnC,SAAC6B,QAKX/B,8BAAA,WACE,OAAO5B,KAAKwC,gBAGdZ,8BAAA,WACE,OAAO5B,KAAK0C,gBAGdd,oBAAA,WACE5B,KAAKmC,YAAYyB,SAAQ,SAACC,EAAS5B,GACjC4B,EAAQD,SAAQ,SAAAZ,GACdA,EAAOc,gBAIX9D,KAAKsC,YAAYsB,SAAQ,SAACC,EAAS5B,GACjC4B,EAAQD,SAAQ,SAAAZ,GACdA,EAAOc,gBAIX9D,KAAKmC,YAAc,IAAIR,IACvB3B,KAAKsC,YAAc,IAAIX,IACvB3B,KAAKwC,eAAiB,EACtBxC,KAAK0C,eAAiB,EACtB1C,KAAKuC,aAAe,EACpBvC,KAAK6C,kBAAoB,QAI7B,SAASX,EAAaL,EAAcC,GAClC,OAAUD,MAAQC,CACpB,CCjHA,iBASE,WAAoBJ,GAAA1B,YAAA0B,EARZ1B,qBAAkB,EAClBA,qBAAkB,EAClBA,kBAA0C,IAAI2B,IAC9C3B,kBAA0C,IAAI2B,IAE/C3B,kBAAe,EACfA,uBAAoB,SAI3B+D,2BAAA,SACIC,EAAeC,EAAgBC,EAC/BpC,GACF,IACMqC,EAAWH,EAAQC,EADDG,EAAmBF,GAErCjC,EAAMoC,EAAcL,EAAOC,EAAQC,EAAQpC,GAYjD,GAXK9B,KAAKsE,aAAalC,IAAIH,IACzBjC,KAAKsE,aAAajC,IAAIJ,EAAK,IAGxBjC,KAAKuE,aAAanC,IAAIH,IACzBjC,KAAKuE,aAAalC,IAAIJ,EAAK,IAG7BjC,KAAKuC,cAAgB4B,EACrBnE,KAAKwE,kBAEDxE,KAAKsE,aAAa7B,IAAIR,GAAK3B,OAAS,EAAG,CACzCN,KAAKyE,kBAEL,IAAMC,EAAa1E,KAAKsE,aAAa7B,IAAIR,GAAKW,QAE9C,OADA5C,KAAKuE,aAAa9B,IAAIR,GAAK1B,KAAKmE,GACzBA,EAGT1E,KAAK6C,mBAAqBsB,EAE1B,IAAMQ,EAAa3E,KAAK0B,OAAOkD,cAAc,CAC3C/C,KAAM,CAACmC,EAAOC,GACdC,SACApC,UAIF,OAFA9B,KAAKuE,aAAa9B,IAAIR,GAAK1B,KAAKoE,GAEzBA,GAGTZ,2BAAA,SACIc,EAAqBb,EAAeC,EACpCC,EAA0BpC,GAC5B,GAA+B,IAA3B9B,KAAKsE,aAAazC,KAAtB,CAIA,IAAMI,EAAMoC,EAAcL,EAAOC,EAAQC,EAAQpC,GAC5C9B,KAAKsE,aAAalC,IAAIH,IACzBjC,KAAKsE,aAAajC,IAAIJ,EAAK,IAG7BjC,KAAKsE,aAAa7B,IAAIR,GAAK1B,KAAKsE,GAChC7E,KAAKyE,kBACLzE,KAAKwE,kBAEL,IAAMM,EAAc9E,KAAKuE,aAAa9B,IAAIR,GACpC8C,EAAeD,EAAY3B,QAAQ0B,GACzC,GAAIE,EAAe,EACjB,MAAM,IAAI3B,MACN,4EAGN0B,EAAYzB,OAAO0B,EAAc,GACjC,IACMZ,EAAWH,EAAQC,EADDG,EAAmBF,GAE3ClE,KAAKuC,cAAgB4B,IAGvBJ,+BAAA,WACE,OAAO/D,KAAKwE,iBAGdT,+BAAA,WACE,OAAO/D,KAAKyE,iBAGdV,oBAAA,WACE/D,KAAKsE,aAAaV,SAAQ,SAACoB,EAAU/C,GACnC+C,EAASpB,SAAQ,SAAAiB,GACfA,EAAQf,gBAIZ9D,KAAKuE,aAAaX,SAAQ,SAACoB,EAAU/C,GACnC+C,EAASpB,SAAQ,SAAAiB,GACfA,EAAQf,gBAIZ9D,KAAKsE,aAAe,IAAI3C,IACxB3B,KAAKuE,aAAe,IAAI5C,IACxB3B,KAAKwE,gBAAkB,EACvBxE,KAAKyE,gBAAkB,EACvBzE,KAAKuC,aAAe,EACpBvC,KAAK6C,kBAAoB,QAI7B,SAASwB,EACLL,EAAeC,EAAgBC,EAC/BpC,GACF,OAAUkC,MAASC,MAAUC,MAAUpC,CACzC,CAEA,SAASsC,EAAmBF,GAC1B,GAAe,eAAXA,EACF,OAAO,GAEP,MAAM,IAAId,MAASc,uBAEvB,UCtHgBe,EACZC,EAAsBC,GACxB,GAAIC,KAAKC,UAALD,OAAYF,IAAc,EAC5B,MAAM,IAAI9B,MAAM,4DAGlB,IAAMkC,EAAYJ,EAAW5E,OACvBiF,EAAQL,EAAWM,KAAI,SAAApI,GAAK,OAAG+H,MAAgB/H,SAC/CqI,EAAU,IAAIhI,MAAM6H,EAAY,GACtCG,EAAQH,EAAY,GAAKC,EAAMD,EAAY,GAC3C,IAAK,IAAI1E,EAAI0E,EAAY,EAAG1E,GAAK,IAAKA,EACpC6E,EAAQ7E,GAAK,IAAI6E,EAAQ7E,EAAI,SAAQ2E,EAAM3E,EAAI,OAGjD,OAAO6E,CACT,CCoBO,IAAMC,EACT,SAAChE,EAAmBiE,EAAwBC,EAC3CC,GACC,IACMC,EA+FZ,SACIC,EAAwBC,EACxBL,GACF,IAAMM,EAA2B,GA4BjC,GA3BAA,EAAe1F,KAAK,kCACSoF,EAAQO,cAAc,uCACtBP,EAAQO,cAAc,uCACtBP,EAAQO,cAAc,6PAS/CC,EAAeR,GACX,4BACA,2jBAaJA,EAAQS,aAYV,OAXAH,EAAe1F,KAAK,8NAQhB8F,EAAeL,EAAWM,MAAOX,EAAQY,qFAGtC,CACLC,EACAP,EAAeQ,KAAK,MACpBC,EAA0BV,EAAWT,OACrCI,EAAQgB,eACRF,KAAK,MAGT,IAAIG,EAAqB,gCACzBjB,EAAQkB,cAAcjD,SAAQ,SAACkD,EAAGlG,GAChC,IAAMmG,EAAcC,EAAkBjB,EAAUnF,GAAG2E,MAAMjF,QACzDsG,GACOE,EAAEG,OAAO,GAAGC,cAAgBJ,EAAEK,MAAM,cAAaJ,UAE1D,IAAMK,EAAiBJ,EAAkBhB,EAAWT,MAAMjF,QAC1DsG,GAAsB,cAAcQ,OACpC,IACMC,EAAkBL,EADFhB,EAAWT,MAAMjF,OAAS,GAEhDsG,GAAsB,+BACIS,OAEtB1B,EAAQ9D,OACV+E,GAAsB,gBAGpBjB,EAAQ2B,WACVV,GAAsBjB,EAAQ2B,UAGhCV,EAimBF,SAAyBW,GAEvB,IAAMC,EAAc,wBACpBD,EAAgBA,EAAcE,QAAQD,GAAa,SAACE,GAClD,MAAO,cAAgBA,KAIzB,IAAMC,EAAc,wBAIpB,OAHAJ,EAAgBA,EAAcE,QAAQE,GAAa,SAACrI,EAAGsI,EAAIC,GACzD,MAAO,MAAMD,kBAAkBC,IAGnC,CA9mBuBC,CADrBlB,GAAsB,MAGtBX,EAAe1F,KAAKqG,GAGhBjB,EAAQoC,OACV9B,EAAe1F,KAAK,4FAIpB0F,EAAe1F,KAAK,wEAEhB8F,EAAeL,EAAWM,MAAOX,EAAQY,oBAG/CZ,EAAQkB,cAAcjD,SAAQ,SAACkD,EAAGlG,GAChCqF,EAAe1F,KAAK,+BACG,EAAIK,2BAAyBkG,cAChDnB,EAAQqC,cACJrC,EAAQqC,cAAcpH,GACtByF,EAAeN,EAAUnF,GAAG0F,MAAOX,EAAQY,4BAI1B,KAAvBK,GACFX,EAAe1F,KAAK,+BAEhB,EAAIoF,EAAQkB,cAAcvG,sDAIhC,IAAM2H,EAkXR,SACIC,EACAC,GACK,IAAArB,MAAGsB,MAAAjJ,kBAAQkJ,MAAAC,kBAEZC,EAAUL,EAAS5H,OACnBkI,EAAO1B,EAAExG,OAASnB,EAAEmB,OAASgI,EAAEhI,OAGrC,GAAIkI,IAASD,EACX,MAAO,GAGT,GAAIzB,EAAExG,SAAWiI,EAAS,CAOxB,MALgB,2BADFvB,EAAkBuB,sGAYlC,IAHA,IAAIE,EAAsB,GACpBC,EAAO,CAAC5B,EAAG3H,EAAGmJ,GAEX1H,EAAI,EAAGA,EAAI8H,EAAKpI,OAAQM,IAAK,CACpC,IAAM+H,EAAMD,EAAK9H,GAEjB,GAAmB,IAAf+H,EAAIrI,OAIR,GAAmB,IAAfqI,EAAIrI,OACNmI,GAAuB,QAAQE,EAAI,sBAAqB/H,YACnD,CACL,IAAM6E,EAAUR,EAA2B0D,EAAK,qBAChDF,GAAuB,YAAY7H,qBAAoBA,QACvD,IAAK,IAAIgI,EAAI,EAAGA,EAAInD,EAAQnF,OAAQsI,IAClCH,GAAuB,QAAQE,EAAIC,cAAahI,QAAO6E,EAAQmD,OAE3DA,IAAMnD,EAAQnF,OAAS,EACzBmI,GAAuB,QAAQE,EAAIC,EAAI,GAAhB,WACXhI,SAAQ+H,EAAIC,SAAQnD,EAAQmD,OAExCH,GACI,QAAQ7H,aAAYA,SAAQ+H,EAAIC,SAAQnD,EAAQmD,QAM5D,IAAMC,EAAa,GACnB,IAASjI,EAAI,EAAGA,EAAI4H,EAAM5H,IACxBiI,EAAWtI,KAAK,IAAIK,GAGtB,IAAM0F,EAAQU,EAAkBwB,GAC5BM,EAAU,2BAA2BxC,WACvCmC,OAEwB,IAAtBI,EAAWvI,OACbwI,GAAW,UAAUxC,WAErBwC,GAAW,UAAUxC,MAASuC,EAAWpC,KAAK,YAGhD,OAAOqC,CACT,CArbMC,CAAuB/C,EAAWT,MAAOI,EAAQwC,gBAE/Ca,EAAU,CACdxC,EAAeP,EAAeQ,KAAK,MACnCC,EAA0BV,EAAWT,OAAQ0C,EAC7CgB,EAAgCjD,EAAWT,MAAMjF,SAE9CqF,EAAQoC,QACXiB,EAAQzI,KAigBZ,SACI2H,EAAoBgB,EAAyB3C,GAC/C,IAEIuC,EAFEP,EAAUL,EAAS5H,OACnB6I,EAAW9C,EAAe6C,EAAe3C,GAG7CuC,EADEvC,EACQ,wFACc4C,kHAGAA,oBAGd,kFACcA,4GAGAA,oBAG1B,GAAIZ,GAAW,EAAG,CAChB,IAAMG,EAAO,CAAC,KAAM,KAAM,KAAM,KAAM,KAAM,MAAMvB,MAAM,EAAGoB,GACrDa,EAAOpC,EAAkBuB,GAG7BO,GADEvC,EACS,gCAEPmC,EAAKlD,KAAI,SAAApI,GAAK,OAAGA,cAAWqJ,KAAK,kFACQ2C,MAAQV,EAAKjC,KAAK,sGAI3DiC,EAAKlD,KAAI,SAAApI,GAAK,OAAGA,cAAWqJ,KAAK,kFACQ2C,MAAQV,EAAKjC,KAAK,+EAKpD,gCAEPiC,EAAKlD,KAAI,SAAApI,GAAK,OAAGA,cAAWqJ,KAAK,4EACQ2C,MAAQV,EAAKjC,KAAK,kGAI3DiC,EAAKlD,KAAI,SAAApI,GAAK,OAAGA,cAAWqJ,KAAK,4EACQ2C,MAAQV,EAAKjC,KAAK,2EAOnE,OAAOqC,CACT,CAtjBQO,CAAiBrD,EAAWT,MAAOS,EAAWM,MAAOX,EAAQY,SAGnE,IAAM+C,EACFvD,EACKP,KACG,SAACsB,EAAGlG,GAAM,OAgVxB,SACImF,EAAsBmC,EAAoB3B,EAC1CgD,GACF,IAAIC,EAzLN,SACIzD,EAAsBQ,GACxB,IAAMkD,EAAU1D,EAAU2D,KACpBlB,EAAOzC,EAAUR,MAAMjF,OACvB8I,EAAOpC,EAAkBwB,GACzBmB,EAAW,MAAQF,EAAQxC,OAAO,GAAG2C,cAAgBH,EAAQtC,MAAM,GACnEuB,EAAO,CAAC,KAAM,KAAM,KAAM,KAAM,KAAM,MAAMvB,MAAM,EAAGqB,GACrDqB,EAASnB,EAAKlD,KAAI,SAAApI,GAAK,OAAGA,cAAWqJ,KAAK,MAEhD,GAAI+B,EAAO,EACT,OAAIjC,EACK,gBACAoD,mDACgBF,6BAKlB,cACAE,oCACUF,yBAKnB,IAAMK,EACF,aAAYL,EAAQxC,OAAO,GAAGC,cAAgBuC,EAAQtC,MAAM,YAC5D4C,EAAavB,MACJ,IAATA,IACFuB,EAAU,MAGZ,GAAIxD,EACF,MAAO,cACAoD,MAAYE,gDACIJ,wBAA6BM,MAAWX,MAC3DV,EAAKjC,KAAK,sBACNqD,8BAKV,MAAO,YACAH,MAAYE,kCACFJ,wBAA6BM,MAAWX,MACrDV,EAAKjC,KAAK,oBACNqD,oBAGV,CAwIYE,CAAwBjE,EAAWQ,GAE7BR,EAAUR,MACdjF,QAAU4H,EAAS5H,SAC7BkJ,GA1IJ,SACIzD,EAAsBmC,EAAoB3B,EAC1CgD,GACF,IAAME,EAAU1D,EAAU2D,KACpBO,EAAiBR,EAAQxC,OAAO,GAAG2C,cAAgBH,EAAQtC,MAAM,GAEjEwC,EAAW,MAAQM,EAAiB,WAEpCC,EAASnE,EAAUR,MAAMjF,OACzBiI,EAAUL,EAAS5H,OACnB8I,EAAOpC,EAAkBuB,GAK/B,GAAI4B,OAAKC,YAAYrE,EAAUR,MAAO2C,IAAaqB,EACjD,OAAIhD,EACK,cACFoD,uEACgBF,0CAGhBE,qBAA2BP,gDACXK,OACjBlB,EAAU,EAAI,mCAAqC,qCAIhD,YACJoB,yDACUF,sCAGVE,qBAA2BP,kCACjBK,OACTlB,EAAU,EAAI,mCAAqC,6BAM3D,IAAM8B,EACFC,eAAaC,iBAAiBxE,EAAUR,MAAO2C,GAC7CsC,EAAWjC,EAAU2B,EAEvBjC,EAAgB,GAEpB,GAAe,IAAXiC,EACF,OAAI3D,EACK,YACJoD,8DACSM,0BAGTN,qBAA2BP,uCAClBa,mBAIP,YACFN,uDACSM,0BAGTN,qBAA2BP,gCAClBa,mBAKZhC,EADEM,EAAU,GAAK8B,EAAc/J,QAAU,EACzB,cAGZ+J,EAAc7E,KAAI,SAAApI,GAAK,MAAA,UAAUqN,EAAarN,EAAIoN,cAC7C/D,KAAK,MAIlB,IAAIiE,EAAwB,GAC5B,GAAInC,EAAU,GAAK2B,EAAS,EAC1BQ,EAAwB,cAExB,GAAInC,EAAU,EAAG,CACf,IAAMoC,EAAa3D,EAAkBkD,GAC/BU,EACF7E,EAAUR,MAAMC,KAAI,SAAC9E,EAAGE,GAAM,MAAA,UAAU6J,EAAa7J,EAAI4J,MACpD/D,KAAK,MACdiE,EAA2BC,MAAcC,WAEzCF,EAAwB,SAI5B,IAAMZ,EACF,aAAYL,EAAQxC,OAAO,GAAGC,cAAgBuC,EAAQtC,MAAM,YAC1D4C,EAAaG,MACnB,GAAI3D,EACF,MAAO,YACFoD,yGAED1B,oBACOwB,wBAA6BM,MACpCW,OAA0BZ,8BAGzBH,uBAA6BP,2DAE9BnB,oBACOwB,wBAA6BM,MACpCW,OAA0BZ,uBAKhC,MAAO,UACFH,+FAED1B,sBACWwB,wBAA6BM,MACxCW,OAA0BZ,uBAGzBH,uBAA6BP,iDAE9BnB,sBACWwB,wBAA6BM,MACxCW,OAA0BZ,eAGhC,CASWe,CACH9E,EAAWmC,EAAU3B,EAAQgD,IAGnC,OAAOC,CACT,CA5VwBsB,CACNhE,EAAGd,EAAWT,MACdI,EAAQqC,cAC0B,cAA7BrC,EAAQqC,cAAcpH,GACvB+E,EAAQY,OACZZ,EAAQwC,eAAerB,EAAExG,SAAW0F,EAAWT,MAAMjF,WAC5DmG,KAAK,MAKd,OAJAuC,EAAQzI,KAAK+I,GAEbN,EAAQzI,KAAKoF,EAAQgB,eACNqC,EAAQvC,KAAK,KAE9B,CAlOqBsE,CAAWnF,EADP,CAACU,MAAOT,EAAOS,MAAOf,MAAOM,EAAON,OACLI,GAC5CqF,EAAStJ,EAAOuJ,mBAClB,CAACC,KAAMpF,EAAQvG,MAAOoG,EAAQwF,YAAYzB,OAO9C,OANiBhI,EAAO0J,sBAAsB,CAC5CC,QAAS,CAACL,SAAQM,WAAY,UAC9B/L,MAAOoG,EAAQwF,YAAYzB,KAC3B6B,OAAQ,QAIZ,WAEYvE,EAAkBwB,GAChC,GAAIA,GAAQ,EACV,MAAO,MACF,GAAa,IAATA,EACT,MAAO,YACF,GAAa,IAATA,EACT,MAAO,YACF,GAAa,IAATA,EACT,MAAO,YACF,GAAa,IAATA,EACT,MAAO,OACF,GAAa,IAATA,EACT,MAAO,OAEP,MAAMpF,MAAM,gBAAgBoF,0BAEhC,UAEgBiC,EAAae,GAC3B,GAAc,IAAVA,EACF,MAAO,IACF,GAAc,IAAVA,EACT,MAAO,IACF,GAAc,IAAVA,EACT,MAAO,IACF,GAAc,IAAVA,EACT,MAAO,IACF,GAAc,IAAVA,EACT,MAAO,IACF,GAAc,IAAVA,EACT,MAAO,IAEP,MAAMpI,MAAM,SAASoI,0BAEzB,UAIgBC,QAAoB,IAC9B3C,WAD8B4C,mBAAAA,IAAAC,kBAElC,OAAQA,EAAOrL,QACb,KAAK,EACHwI,EAAU,qdAaV,MACF,KAAK,EACHA,EAAU,4dAWE6C,EAAO,qBAEnB,MACF,QACE,MAAMvI,MAAM,eAEhB,OAAO0F,CACT,CAsKA,IAAMtC,EAAgB,4nFAsEtB,SAASE,EAA0BnB,GACjC,IAAMiD,EAAOjD,EAAMjF,OAEnB,GAAIkI,GAAQ,EACV,MAAO,8DAOT,IAJA,IAcIM,EAdErD,EAAU0E,OAAKyB,eAAerG,GAC9Be,EAAQU,EAAkBwB,GAE1BqD,EAAmB,GAChBjL,EAAI,EAAGA,EAAI4H,EAAM5H,IACxBiL,EAAOtL,KAAK,IAAIK,GAGlB,OAAuB,IAAnB6E,EAAQnF,OACH,mMAMTwI,EAAU,sBACNrD,EACKD,KAAI,SAAClG,EAAGsB,GASP,MAPI,OAAOiL,EAAOjL,2CACV6J,EAAa7J,SACPA,IAAM6E,EAAQnF,OAAS,EACjC,OAAOuL,EAAOjL,EAAI,kBACdiL,EAAOjL,kCAAiC6J,EAAa7J,GACzD,qBAAqBiL,EAAOjL,kCACxB6J,EAAa7J,WAGtB6F,KAAK,IAEP,+CACmCH,eACpCwC,oBACOxC,MAASuF,EAAOpF,KAAK,qBAGpC,CAgRA,SAASwC,EAAgCV,GACvC,IAAIO,EAAU,GACd,OAAQP,GACN,KAAK,EACL,KAAK,EACHO,GAAW,8GAKX,MACF,KAAK,EACHA,GAAW,iKAKX,MACF,KAAK,EACHA,GAAW,+LAKX,MACF,KAAK,EACHA,GAAW,yOAMX,MACF,KAAK,EACHA,GAAW,6UASX,MACF,KAAK,EACHA,GAAW,oYAUX,MACF,QACEqB,OAAK2B,QAAO,GAAO,WAAM,MAAA,eAAevD,eAG5C,OAAOO,CACT,CAEA,SAAS3C,EAAeR,GACtB,OAA+B,IAAxBA,EAAQoG,SAAS,IAAoC,IAAxBpG,EAAQoG,SAAS,EACvD,UAEgB1F,EAAe+C,EAAgB7C,GAE7C,MAAa,YAAT6C,EACK7C,EAAS,YAAc,MACZ,UAAT6C,GAES,SAATA,EADF7C,EAAS,YAAc,MAOzB6C,CACT,CCrvBA,IA6JY4C,EA7JNC,EAAe,SAACtD,GAEpB,IADA,IAAIuD,EAAU,EACLtL,EAAI,EAAGA,EAAI+H,EAAIrI,OAAQM,IAC9BsL,GAAWvD,EAAI/H,GAEjB,OAAOsL,CACT,WAgBgBC,EACZZ,EAAmDa,EACnDlG,EACAmG,gBADAnG,GAA2C,EAAG,EAAG,iBACjDmG,GACK,EAAG,EAAG,IACP,IAAAjE,+MAaN,MAAO,gBACT,UAOgBkE,EACZC,EAAmBC,EAAkBC,EACrCC,gBAAAA,MAQF,IAAMxG,EAA0C,CAAC,EAAG,EAAG,GACjDmG,EAA8C,CAAC,EAAG,EAAG,GAY3D,OAVKK,IACCH,GAAa,IACfF,EAAkB,GAAK,GAGrBG,GAAY,IAAMC,GAAa,KACjCvG,EAAc,GAAK,IAIhB,CAACA,gBAAemG,oBACzB,UAEgBM,EACZpB,EAAmDa,EACnD7F,GACF,gBADEA,MACEA,EACF,MAAO,CAAC,EAAG,EAAG,GAGhB,IAAMqG,EAAOX,EAAaV,EAAOzE,EAAEtB,KAAI,SAAApI,GAAK,OAAAgP,EAAYhP,OAClDyP,EAAOZ,EAAaV,EAAOpM,EAAEqG,KAAI,SAAApI,GAAK,OAAAgP,EAAYhP,OASxD,OAAIwP,GAAQ,EACH,CAAC,EAAG,GAAI,GAEbC,GAAQ,EACH,CAAC,GAAI,EAAG,GAGV,CAAC,GAAI,GAAI,EAClB,UAEgBC,EACZvB,EAAmDa,EACnD7F,GACF,gBADEA,MACEA,EACF,MAAO,CAAC,EAAG,EAAG,GAGhB,IAAMqG,EAAOX,EAAaV,EAAOzE,EAAEtB,KAAI,SAAApI,GAAK,OAAAgP,EAAYhP,OAClDyP,EAAOZ,EAAaV,EAAOpM,EAAEqG,KAAI,SAAApI,GAAK,OAAAgP,EAAYhP,OAIxD,OAAIwP,GAAQ,EACH,CAAC,EAAG,EAAG,GAEZC,GAAQ,EACH,CAAC,EAAG,EAAG,GAGT,CAAC,EAAG,EAAG,EAChB,UAEgBE,EAAmBxH,GACjC,MAAO,CAACuB,EAAGvB,EAAMC,KAAI,SAACpI,EAAGwD,GAAM,OAAAA,KACjC,UAEgBoM,EAAmB1G,GACjC,GAAc,YAAVA,GAAiC,UAAVA,GAA+B,SAAVA,GAClC,WAAVA,EACF,OAAO,EACF,GAAc,cAAVA,EACT,OAAO,EAEP,MAAM,IAAIlD,MAAM,iBAAiBkD,EAErC,UAEgB2G,EAAwBC,EAAmB5G,GACzD,GAAc,YAAVA,EACF,OAAO,IAAI6G,aAAaD,GACnB,GAAc,UAAV5G,EACT,OAAO,IAAI8G,WAAWF,GACjB,GAAc,SAAV5G,GAA8B,WAAVA,EAC7B,OAAO+G,WAAWC,KAAK,IAAIF,WAAWF,IAEtC,MAAM,IAAI9J,MAAM,iBAAiBkD,EAErC,UAEgBiH,IACd,OAA2B,oBAAXC,QAEsB,oBAAtBC,sBACVC,UAAUC,GAClB,EAEA,SAAY3B,GACVA,iDACAA,iDACAA,mEACAA,iDACAA,4BACD,CAND,CAAYA,IAAAA,8DApJR4B,EAAoBrI,GACtB,GAAIqI,EAAStN,SAAWiF,EAAMjF,OAC5B,MAAM,IAAI8C,MACN,+BAA+BwK,EAAStN,OACxC,+BAA+BiF,EAAMjF,OADrC,8BAIN,OAAOiF,EAAMsI,OACT,SAACC,EAAaC,GAAmB,OAAAD,EAAMF,EAASG,IAAY,IAClE,+OCwCMC,EACF3M,QAAM4M,UAAU,mDA8DlB,WAAYvM,EAAmBH,GAA/B,MACE2M,mBACA,GAtBMzK,uBAAuB,IAAI0K,QAC3B1K,0BAA0B,EAC1BA,YAAW,EACXA,iBAAiB,EAGjBA,4BAAsC,GAKtCA,yBAAuC,GAEvCA,yBAAuC,GACvCA,eAAe,GAQhB2K,IACH,MAAM,IAAIhL,MAAM,iDAElBK,EAAK4K,cAAgB,GACrB5K,EAAK/B,OAASA,EACd+B,EAAK6K,MAAQ5M,EAAO4M,MACpB7K,EAAK8K,sBAAwB,KAC7B9K,EAAK+K,mBAAqB,KAC1B/K,EAAKgL,iBAAmB/M,EAAOgN,SAAStM,IAAI,mBAC5CqB,EAAKlC,YAAc,IAAIE,EAAYF,GAEnCkC,EAAKkL,cAAgB,IAAI/M,EAAc6B,EAAK/B,QAC5C+B,EAAKmL,eAAiB,IAAI7K,EAAeN,EAAK/B,QAC9C+B,EAAKoL,UAAY,IAAIC,cAAYrL,EAAMsL,YACnCtL,EAAKgL,mBACPhL,EAAKuL,SAAWvL,EAAK/B,OAAOuN,eAAe,CACzC7F,KAAM,YACN8F,MAAO,KAMP7N,QAAM8N,QAAQ,6BAChB1L,EAAK2L,YAAcC,SAASC,cAAc,UAC1C7L,EAAK2L,YAAYpL,MAAQ,EACzBP,EAAK2L,YAAYnL,OAAS,EAE1BR,EAAK8L,aAAe9L,EAAK2L,YAAYI,WAAW,UAChD/L,EAAK8L,aAAaE,UAAU,CAC1B/N,SACAwC,OAAQ,eAGVmL,SAASpQ,KAAKyQ,YAAYjM,EAAK2L,gCRxJXhS,EAAGC,GACzB,GAAiB,mBAANA,GAA0B,OAANA,EAC3B,MAAM,IAAI+C,UAAU,uBAAyBuP,OAAOtS,GAAK,iCAE7D,SAASuS,IAAO5P,KAAKmL,YAAc/N,EADnCD,EAAcC,EAAGC,GAEjBD,EAAEO,UAAkB,OAANN,EAAaC,OAAOuS,OAAOxS,IAAMuS,EAAGjS,UAAYN,EAAEM,UAAW,IAAIiS,EACnF,CQ+EmCE,MA2BzBC,uBAAA,WACN,OAAOA,EAAcC,cA2CvBD,2BAAA,WACE,OAAO,IAGTA,kCAAA,WACE,OAAOE,eAAeC,QAAUD,eAAeE,SAC3CF,eAAeG,UAUrBL,wBAAA,SAAYM,EAAgBC,GAC1B,gBAD0BA,MACtBtQ,KAAKuQ,0BAA0BpN,QAAQkN,IAAW,EACpD,OAAO,EAET,IAAKrQ,KAAK6O,UAAUzM,IAAIiO,GACtB,OAAO,EAGT,IAAMG,EAAaxQ,KAAK6O,UAAUpM,IAAI4N,GAEtC,GADArQ,KAAKyQ,OAAOJ,IACPC,GAASE,EAAWE,SAAW,EAClC,OAAO,EAIT,GAAI1Q,KAAK2Q,qBAAqBvO,IAAIiO,GAEhC,OADArQ,KAAKuQ,0BAA0BhQ,KAAK8P,IAC7B,EAGF,IAAAO,2CASP,OAR0B,MAAtBA,IACF5Q,KAAK6Q,YAAYD,EAAmBE,KAAKT,OAAQC,GACjDtQ,KAAK6Q,YAAYD,EAAmBG,KAAKV,OAAQC,IAGnDtQ,KAAKgR,gBAAgBX,GACrBrQ,KAAK6O,UAAUoC,OAAOZ,IAEf,GAGTN,mBAAA,WACE,MAAO,CACLmB,cAAelR,KAAK2O,cAAcpM,aAClC4O,uBAAwBnR,KAAK2O,cAAc9L,kBAC3CuO,YAAY,IAIhBrB,4BAAA,SAAgBM,GACd,IAAMG,EAAaxQ,KAAK6O,UAAUpM,IAAI4N,GACtC,GAAKG,GAAeA,EAAWa,aAA/B,CAGA,GAAI,YAAab,EAAWa,aAAc,CACxC,IAAMC,EAAcd,EAAWa,aAC3BC,EAAYzM,mBAAmB0M,YACjCvR,KAAK4O,eAAe4C,eAChBF,EAAYzM,QAASyM,EAAYtN,MAAOsN,EAAYrN,OACpDqN,EAAYpN,OAAQoN,EAAYxP,OAEtCwP,EAAYzM,QAAU,SACjB,CACL,IAAM4M,EAAajB,EAAWa,aAC9BrR,KAAK2O,cAAcjL,cACf+N,EAAWzO,OAAQyO,EAAW5P,KAAM4P,EAAW3P,OACnD2P,EAAWzO,OAAS,KAEtBwN,EAAWa,aAAe,OAI5BtB,qBAAA,SAASM,GACP,OAAIrQ,KAAK6O,UAAUzM,IAAIiO,GACFrQ,KAAK6O,UAAUpM,IAAI4N,GACpBK,SAEb,GAITX,mBAAA,SAAOM,GACcrQ,KAAK6O,UAAUpM,IAAI4N,GAC3BK,YAIbX,mBAAA,SAAOM,GACDrQ,KAAK6O,UAAUzM,IAAIiO,IACFrQ,KAAK6O,UAAUpM,IAAI4N,GAC3BK,YAIfX,kBAAA,SAAM2B,EAAoCnM,EAAiBe,GAEzD,GAAc,cAAVA,GAAmC,MAAVoL,EAC3B,MAAM,IAAItO,MACN,yEAGN,IAAMiN,EAAS,CAACsB,GAAI3R,KAAKgQ,cAEzB,OADAhQ,KAAK6O,UAAUxM,IAAIgO,EAAQ,CAAC/J,QAAOf,QAAOmM,SAAQhB,SAAU,IACrDL,GAGTN,iBAAA,SACIM,EAAgBqB,EAAoCnM,EACpDe,EAAiBoK,GACnB,GAAc,cAAVpK,EACF,MAAM,IAAIlD,MACN,yEAGNpD,KAAK6O,UAAUxM,IAAIgO,EAAQ,CAAC/J,QAAOf,QAAOmM,SAAQhB,cAGpDX,wBAAA,WAAA,WACE/P,KAAK4R,yBACL5R,KAAKsO,MAAMuD,OAAO,CAAC7R,KAAKuO,sBAAsBuD,WAC9C9R,KAAKuO,sBAAwB,KAC7BvO,KAAK+R,wBAA0B,EAE/B/R,KAAK2Q,qBAAuB,IAAIxC,QAEhCnO,KAAKuQ,0BAA0B3M,SAAQ,SAAAxG,GACrCqG,EAAKuN,gBAAgB5T,GACrBqG,EAAKoL,UAAUoC,OAAO7T,MAExB4C,KAAKgS,uBAAuBpO,SACxB,SAAAxG,GAAK,OAAAqG,EAAKkL,cAAcjL,cAActG,EAAE4F,OAAQ5F,EAAEyE,KAAMzE,EAAE0E,UAC9D9B,KAAKiS,uBAAuBrO,SACxB,SAAAxG,GAAK,OAAAqG,EAAKkL,cAAcuD,oBAAoB9U,EAAE4F,OAAQ5F,EAAEyE,KAAMzE,EAAE0E,UAEpE9B,KAAKuQ,0BAA4B,GACjCvQ,KAAKgS,uBAAyB,GAC9BhS,KAAKiS,uBAAyB,IAGhClC,sCAAA,WACO/P,KAAKuO,wBACRvO,KAAKuO,sBAAwBvO,KAAK0B,OAAOyQ,yBAI7CpC,mCAAA,WACM/P,KAAKwO,qBACPxO,KAAKwO,mBAAmB4D,MACxBpS,KAAKwO,mBAAqB,OAI9BuB,2BAAA,WAIE,OAHK/P,KAAKwO,qBACRxO,KAAKwO,mBAAqBxO,KAAKuO,sBAAsB8D,oBAEhDrS,KAAKwO,oBAGDuB,0BAAN,SAAoB/M,EAAmBnB,qGAS5C,OAPMyQ,EAAUtS,KAAK2O,cAAc5M,cAC/BF,EAAMoO,eAAeG,SAAWH,eAAesC,UACnDvS,KAAKwS,4BACLxS,KAAK4R,yBACL5R,KAAKuO,sBAAsBkE,mBAAmBzP,EAAQ,EAAGsP,EAAS,EAAGzQ,GACrE7B,KAAK0S,iBAECJ,EAAQhP,SAASC,WAAWoP,cAkBlC,OAlBAtK,SACMqJ,EAASY,EAAQM,iBAAiBzL,MAAM,GAE9CmL,EAAQO,QACO,MAAXP,GACFtS,KAAK2O,cAAcjL,cACf4O,EAASzQ,EAAMoO,eAAeG,SAAWH,eAAesC,UAK1DlR,QAAM8N,QAAQ,6BAChBhF,OAAK2B,YACqBgH,IAAtB9S,KAAKuP,cACL,WAAM,MAAA,4CACVvP,KAAKuP,aAAawD,wBAGbrB,WAGD3B,iCAAA,SAAqBM,EAAgBnD,GAE3C,IAAMsD,EAAaxQ,KAAK6O,UAAUpM,IAAI4N,GAGtC,OAFArQ,KAAKgR,gBAAgBX,GACrBG,EAAWkB,OAASxE,EACbsD,EAAWkB,QAKpB3B,qBAAA,SAASM,GACP,IACOqB,EADY1R,KAAK6O,UAAUpM,IAAI4N,UAGtC,GAAc,MAAVqB,EACF,MAAM,IAAItO,MACN,+DAGN,OAAOsO,GAGH3B,iBAAN,SAAWM,iHACT,IAAKrQ,KAAK6O,UAAUzM,IAAIiO,GACtB,MAAM,IAAIjN,MAAM,UAAUiN,0BAM5B,OAJMG,EAAaxQ,KAAK6O,UAAUpM,IAAI4N,GAIxB,OAFPqB,EAAUlB,aAKRxQ,KAAKgT,qBACD3C,EAAQqB,IAMI,cAArBlB,EAAWlK,eACInI,QAAQ8U,IAAI,CAC3BjT,KAAKkT,KAAK1C,EAAWI,mBAAmBE,KAAKT,QAC7CrQ,KAAKkT,KAAK1C,EAAWI,mBAAmBG,KAAKV,yBAFzC8C,EAAK9K,SAKL+K,EAAaD,EAAG,GAChBE,EAAaF,EAAG,GACtBG,EAAOhJ,eAAaiJ,uBAChBH,EAA4BC,gBAGnB,OADP5B,EAAajB,EAAWa,gBACXrR,KAAKwT,cAAc/B,EAAWzO,OAAQyO,EAAW5P,cAA9DqL,EAAO7E,SACbiL,EAAOG,EACHvG,EAAqBsD,EAAWlK,wBAGtC,OADAtG,KAAKgT,qBAAqB3C,EAAQiD,MAC3BA,WAOTvD,sBAAA,SAAUM,GACR,IAAMqD,EAAgB1T,KAAK6O,UAAUpM,IAAI4N,GAClCqB,WAAQpL,UAAOf,UAAO8L,iBAE7B,GAAc,cAAV/K,EACF,MAAM,IAAIlD,MAAM,wDAGlB,GAAoB,MAAhBiO,EACF,MAAc,MAAVK,EACI,IAAItO,MAAM,kCAEV,IAAIA,MAAM,mCAIpB,IAAMvB,EAAQwP,EAA4BxP,KACpCmB,EAAShD,KAAK2O,cAAc5M,cAAcF,EAAMwP,EAAavP,OACnE9B,KAAKwS,4BACLxS,KAAK4R,yBACL5R,KAAKuO,sBAAsBkE,mBACtBpB,EAA4BrO,OAAQ,EAAGA,EAAQ,EAAGnB,GACvD7B,KAAK0S,cAEL,IAAMiB,EAAa3T,KAAK4T,eAAerO,EAAOe,GAExCuN,EAAY9E,WAAS+E,yBAAyBH,GAMpD,OAJmB3T,KAAK6O,UAAUpM,IAAIkR,EAAWtD,QAE5CgB,aAAe,CAACxP,OAAMC,MAAO9B,KAAK+T,wBAAyB/Q,UAEzD,CAAC6Q,YAAW7Q,SAAQgR,QAASnS,IAGtCkO,uBAAA,SAA+C3Q,GAE7C,IAAM8N,EAAOlN,KAAKiU,SAAS7U,EAAEiR,QAC7B,GAAgB,WAAZjR,EAAEkH,MACJ,IAEE,IAAM4N,EAAWhH,EAAsB1H,KAAI,SAAApI,GAAK,OAAA+M,OAAKgK,aAAa/W,MAClE,OAAO4F,SAAO5D,EAAEmG,MAAsBnG,EAAEkH,MAAO4N,GAE/C,SACA,MAAM,IAAI9Q,MAAM,oDAGpB,OAAOJ,SAAO5D,EAAEmG,MAAsBnG,EAAEkH,MAAO4G,IAI3C6C,iBAAN,SAAW7Q,+GA2CQ,OA1CZc,KAAKyO,kBACR2F,QAAQC,KACJ,kVAOAC,EAAkBtU,KAAKuU,aACvBC,EAA+B,GAEjCC,GAAgB,EACW,MAA3BzU,KAAK0U,oBACP1U,KAAK0U,mBAAqBF,EAC1BC,GAAgB,GAEhBzU,KAAKuU,aAAahU,KAAKiU,GAEzBxU,KAAKuU,aAAeC,EAEpBtV,IAEMyV,EACFxK,OAAKyK,QAAQ5U,KAAKuU,aAAa/O,KAAI,SAACpI,GAAwB,OAAAA,EAAEyX,UACzDC,QAAO,SAAA1X,GAAK,OAAK,MAALA,KACf2X,EACF5K,OAAKyK,QAAQ5U,KAAKuU,aAAa/O,KAAI,SAACpI,GAAwB,OAAAA,EAAEsM,SACzDoL,QAAO,SAAA1X,GAAK,OAAK,MAALA,KAErB4C,KAAKuU,aAAeD,EAEhBG,IACFzU,KAAK0U,mBAAqB,MAEtBlL,EAAwB,CAC5BwL,aAAchV,KAAKgV,aACnBC,eAAgBjV,KAAKiV,eACrBC,SAAU,KACVC,OAAQ,SAGahX,QAAQ8U,IAAI0B,WAQnC,OARMO,EAAW7M,SACjBmB,EAAc,SAAIW,OAAKiL,IAAIF,GAC3B1L,EAAyB,oBAAI,WACzB,OAAA0L,EAAS1P,KAAI,SAACpI,EAAGwD,GAAM,OAAE8I,KAAMqL,EAA0BnU,GAAIyU,GAAIjY,MAC5DoI,KAAI,SAAApI,GAAK,OAAGA,EAAEsM,UAAStM,EAAEiY,MACzB5O,KAAK,OACdzG,KAAKgV,aAAe,EACpBhV,KAAKiV,eAAiB,KACfzL,WAGTuG,2BAAA,SACIxK,EAAiBe,EACjBoL,GAOF,MANc,WAAVpL,GAAgC,MAAVoL,GAAkBA,EAAOpR,OAAS,GACxD6J,OAAKmL,SAAS5D,EAAO,MACvBA,EAAUA,EAA0BlM,KAAI,SAAApI,GAAK,OAAA+M,OAAKoL,aAAanY,OAI1D,CAACiT,OADJrQ,KAAKwV,MAAM9D,EAAsCnM,EAAOe,GAC5Cf,QAAOe,UAGjByJ,4BAAA,SAAgB0F,GACtB,IAAKA,EACH,OAAO,KAGT,IAAMjF,EAAaxQ,KAAK6O,UAAUpM,IAAIgT,EAAOpF,QAC7C,GAAI,YAAaG,EAAWa,aAAc,CACxC,IAAMqE,EAAOlF,EAAWa,aACxB,OAAIqE,EAAK7Q,mBAAmB8Q,mBACnBD,EAAK7Q,QAEL6Q,EAAK7Q,QAAQ+Q,aAGxB,IAAMnE,EAAajB,EAAWa,aAC9B,MAAO,CAACwE,OAAQ,EAAGhU,KAAM4P,EAAW5P,KAAMmB,OAAQyO,EAAWzO,SAGzD+M,yBAAN,SAAmB8E,sEACjB,OAAI7U,KAAKyO,oBACAzO,KAAK8V,oBAAoBjB,OAEzB,UAIX9E,wBAAA,SAAYM,GACV,IAAMG,EAAaxQ,KAAK6O,UAAUpM,IAAI4N,GAEtC,IAAIG,EAAWa,aAAf,CAIA,IAAMxP,EAAOkU,EAA+BvF,EAAWlK,OACnD6D,OAAK6L,cAAcxF,EAAWjL,OAC5BvC,EACFhD,KAAK2O,cAAc5M,cAAcF,EAAM7B,KAAK+T,yBAIhD,GAFAvD,EACKa,aAAe,CAACxP,OAAMC,MAAO9B,KAAK+T,wBAAyB/Q,UAC5DwN,EAAWkB,OAAQ,CACrB,IAAMuE,EAAgBjW,KAAK2O,cAAcuH,oBACrCrU,EAAMoO,eAAekG,UAAYlG,eAAeE,UAC9CiG,EAAcH,EAAcrD,iBACT,UAArBpC,EAAWlK,OAA0C,SAArBkK,EAAWlK,MAC7C,IAAI8G,WAAWgJ,GAAa/T,IAAImO,EAAWkB,QAE3C,IAAIvE,aAAaiJ,GAAa/T,IAAImO,EAAWkB,QAE/CuE,EAAcpD,QACd7S,KAAKwS,4BACLxS,KAAK4R,yBACL5R,KAAKuO,sBAAsBkE,mBACvBwD,EAAe,EAAGjT,EAAQ,EAAGnB,GAEjC,IAAMwU,EAAc,CAClBxU,OACAC,MAAOmO,eAAekG,UAAYlG,eAAeE,SACjDnN,OAAQiT,GAEVjW,KAAKiS,uBAAuB1R,KAAK8V,MAU7BtG,yBAAA,SAAauG,GACnB,IAAIC,EAAgB,EAChBC,EAAY,EACVC,EAAoB,GAC1BH,EAAe1S,SAAQ,SAACxG,GAKtB,IAAIsZ,EACJ,OALsB,IAAlBtZ,EAAE8P,KAAK5M,SACTlD,EAAE8P,KAAO,CAAC,IAIJ9P,EAAE8P,KAAK5M,QACb,KAAK,EACHoW,EAAgB,EAChB,MACF,KAAK,EACHA,EAAgB,EAChB,MACF,KAAK,EAGL,KAAK,EAGL,KAAK,EAGL,KAAK,EACHA,EAAgB,GAChB,MACF,QACEvM,OAAK2B,QAAO,GAAO,WAAM,MAAA,eAAe1O,EAAE8P,KAAK5M,oBAGjC,IAAdkW,GAAiC,IAAdA,IACrBE,EAAgB,IAElBH,EAAgBnR,KAAKuR,KAAKJ,EAAgBG,GAAiBA,EAC3DF,EAAYpZ,EAAE8P,KAAK5M,OACnBmW,EAAQlW,KAAKgW,GACbA,GAAiC,EAAhBnZ,EAAE8P,KAAK5M,UAG1B,IAAM8V,EAAc,IAAIQ,YAAYL,GACpCD,EAAe1S,SAAQ,SAACxG,EAAGwD,GACzB,IAAMiV,EAASY,EAAQ7V,GACR,UAAXxD,EAAEgM,KACJ,IAAIgE,WAAWgJ,EAAaP,EAAQzY,EAAE8P,KAAK5M,QAAQ+B,IAAIjF,EAAE8P,MACrC,WAAX9P,EAAEgM,KACX,IAAIyN,YAAYT,EAAaP,EAAQzY,EAAE8P,KAAK5M,QAAQ+B,IAAIjF,EAAE8P,MAE1D,IAAIC,aAAaiJ,EAAaP,EAAQzY,EAAE8P,KAAK5M,QAAQ+B,IAAIjF,EAAE8P,SAI/D,IAAM4J,EAAgB9W,KAAK2O,cAAc5M,cACrCwU,EAAetG,eAAeG,SAAWH,eAAe8G,SAC5D/W,KAAKsO,MAAM0I,YAAYF,EAAe,EAAGV,EAAa,EAAGG,GAEzD,IAAMU,EAAc,CAClBpV,KAAM0U,EACNzU,MAAOmO,eAAeG,SAAWH,eAAe8G,QAChD/T,OAAQ8T,GAIV,OAFA9W,KAAKgS,uBAAuBzR,KAAK0W,GAE1B,CAACpB,OAAQ,EAAGhU,KAAM0U,EAAevT,OAAQ8T,IAG3C/G,6BAAA,SACHpK,EAAuCkE,EACvCqN,EAAuBC,EACvBtR,GAHG,WAOL,GAHKA,IACHA,EAAS7F,KAAK4T,eAAejO,EAAQyG,YAAa8K,IAEX,IAArC/M,OAAK6L,cAAcnQ,EAAON,OAK5B,OAFAvF,KAAK6O,UAAUpM,IAAIoD,EAAOwK,QAAQqB,OAC9BvH,OAAKiN,uBAAuBvR,EAAOS,MAAoB,GACpDT,EAET7F,KAAKqX,YAAYxR,EAAOwK,QACxB1K,EAAQoG,SArnBR,SAACrK,EACAiE,GACC,IAAM2R,EACF5V,EAAO6V,OAAOC,iCACZjM,EAAS5F,EAAwB,eACjCoG,EAAWpG,EAAkB,SACnC,GAAIoG,EAAS8B,OAAM,SAACzQ,GAAM,OAAAA,GAAKka,KAC7B,OAAOvL,EAGT5B,OAAK2B,OACDC,EAAS,GAAKuL,QACGxE,IAAbvH,EAAOpM,QAAgC2T,IAAbvH,EAAOjD,GACrC,WAAM,MAAA,8DAEV,IAAImP,EAAkBrS,KAAKuR,KAAKvR,KAAKsS,KAAK3L,EAAS,KACnD,OAAI0L,EAAkBH,GACpBG,EAAkBrS,KAAKuR,KAAKvR,KAAKuS,KAAK5L,EAAS,KAC/C5B,OAAK2B,OACD2L,GAAmBH,GACnB,WAAM,MAAA,iDACH,CAACG,EAAiBA,EAAiBA,IAEnC,CAACA,EAAiBA,EAAiB,EAE9C,CA4lBmBG,CAAgB5X,KAAK0B,OAAQiE,GAIhD,IAAI2Q,EAAiC,GACjCuB,EAA2B,GAC/B,IAAKlS,EAAQS,aAAc,CACzBkQ,EAAe/V,KAAK,CAAC6I,KAAM,UAAW8D,KAAM,CAAC4K,OAC7CD,EAAehO,EAAO1I,OAAO0E,GAAQL,KAAI,SAAApI,GAAK,OAAAA,EAAEmI,SAChD,IAAMwS,EAAe,QACrBF,EAAarS,KAAI,SAAApI,GACfkZ,EAAe/V,KAAK,CAAC6I,KAAM2O,EAAc7K,KAAM9P,OAEjD,IAAMqI,EAAU0E,OAAKyB,eAAe/F,EAAON,OAE3C,GADA+Q,EAAe/V,KAAK,CAAC6I,KAAM2O,EAAc7K,KAAMzH,IAC3CE,EAAQ9D,KAAM,CAChB,IAAMA,EAAOsI,OAAK6L,cAAcrQ,EAAQyG,aACxCkK,EAAe/V,KACX,CAAC6I,KAAM2O,EAAc7K,KAAM,CAACvH,EAAQY,OAAS1E,EAAO,EAAIA,MAIhE,IAqBImW,EArBEpS,EAAaiE,EAAOrE,KAAI,SAACyS,EAAmBrX,GAChD,GAAoB,cAAhBqX,EAAM3R,MACR,MAAM,IAAIlD,MACN,mIAMN,OAFAK,EAAK4T,YAAYY,EAAM5H,QAEhB,CAGL/J,MAAO7C,EAAKoL,UAAUpM,IAAIwV,EAAM5H,QAAQ/J,MACxCf,MAAO0S,EAAM1S,MACbmE,KAAM/D,EAAQkB,cAAcjG,OAI1BqB,WFhdN0D,EAAwBuS,EAA4BtS,EACpDC,GACF,IAAI5D,EAAM0D,EAAQwS,UAClB,GAAIxS,EAAQS,aACV,OAAOnE,EAGT,IAAMmW,EAAQxS,EAAWJ,KAAI,SAAApI,GAAK,OAAAA,EAAEkJ,SAAOnF,OAAO0E,EAAOS,OACnD+D,EACFzE,EAAWJ,KAAI,SAAApI,GAAK,OAAAkN,eAAaC,iBAAiBnN,EAAEmI,MAAOM,EAAON,UAChE8S,EACFzS,EAAWJ,KAAI,SAAApI,GAAK,OAAA+M,OAAKC,YAAYhN,EAAEmI,MAAOM,EAAON,UAAQkB,KAAK,KAChE6R,EAAmBjO,EAAc7E,KAAI,SAAApI,GAAK,OAAAA,EAAEqJ,KAAK,QAAMA,KAAK,KAE5D8R,EAAqBpS,EAAeR,GAAW,eAAiB,GAOtE,OALA1D,EAAO,KAAO0D,EAAQO,cAAgBP,EAAQO,cAAcO,KAAK,KAAO,IACpEyR,EAAO1S,KAAI,SAAAD,GAAS,OAAAA,EAAMjF,UAAQmG,KAAK,KAAO2R,EAAM3R,KAAK,KACzDd,EAAQkB,cAAcJ,KAAK,KAAO6R,EAClCD,EAA4BE,CAGlC,CE2bQC,CAA6B7S,EAASkS,EAAcjS,EAAYC,GAGhE5D,KAAOjC,KAAKqO,cACd2J,EAAWhY,KAAKqO,cAAcpM,IAE9B+V,EAAWS,EACPzY,KAAK0B,OAAQiE,EAASC,EAAYC,GACtC7F,KAAKqO,cAAcpM,GAAO+V,GAGxBb,IACFb,IAAqBA,EAAmBa,IAE1C,IAAMuB,KACJ1Y,KAAK2Y,gBAAgB9S,IAAYgE,EAAOrE,KAAI,SAAApG,GAAK,OAAAqE,EAAKkV,gBAAgBvZ,OACtEY,KAAK4Y,aAAatC,KAGduC,EAAY7Y,KAAK0B,OAAOoX,gBAAgB,CAC5CvN,OAAQyM,EAASe,mBAAmB,GACpCC,QAASN,EAASlT,KAAI,SAACnI,EAAGuD,GAAM,OAAEqY,QAASrY,EAAGsY,SAAU7b,QAG1D2C,KAAKwS,4BACL,IAAM2G,EAAOnZ,KAAKoZ,iBACZC,EAAyC,MAArBrZ,KAAKuU,aAmC/B,OAlCI8E,GACErZ,KAAKyO,kBAEN0K,EAAaG,eAAetZ,KAAKgP,SAAU,GAGhDmK,EAAKI,YAAYvB,GACjBmB,EAAKK,aAAa,EAAGX,GACrBM,EAAKM,mBACD9T,EAAQoG,SAAS,GAAIpG,EAAQoG,SAAS,GAAIpG,EAAQoG,SAAS,IAC3DsN,GACErZ,KAAKyO,kBAEN0K,EAAaG,eAAetZ,KAAKgP,SAAU,GAGhDhP,KAAK+R,0BAELlI,EAAOjG,SAAQ,SAAAqU,GACbxU,EAAKkN,qBAAqB+I,IAAIzB,EAAM5H,WAEtCrQ,KAAK2Q,qBAAqB+I,IAAI7T,EAAOwK,QAEjChP,QAAMoB,IAAI,sCACAzC,KAAK+R,yBACjB/R,KAAK0S,cAGH2G,GACFrZ,KAAKuU,aAAahU,KAAK,CACrBmJ,KAAM/D,EAAQwF,YAAYzB,KAC1BmL,MAAO7U,KAAK2Z,aAAa3Z,KAAKgP,YAG3BnJ,GAGHkK,gCAAN,SAA0Bf,yGAWxB,OAVM4K,EAAc5Z,KAAK2O,cAAc5M,cACnC,GAAIkO,eAAeE,SAAWF,eAAe4J,eAC3CC,EAAM9Z,KAAK2O,cAAc5M,cAC3B,GAAIkO,eAAesC,SAAWtC,eAAeG,UAEjDpQ,KAAKwS,4BACLxS,KAAK4R,yBACL5R,KAAKuO,sBAAsBwL,gBAAgB/K,EAAU,EAAG,EAAG4K,EAAa,GACxE5Z,KAAKuO,sBAAsBkE,mBAAmBmH,EAAa,EAAGE,EAAK,EAAG,IACtE9Z,KAAK0S,iBACCoH,EAAIxW,SAASC,WAAWoP,cAU9B,OAVAtK,SACM2R,EAAW,IAAIC,eAAeH,EAAIlH,kBAClCsH,EAAmBC,OAAQH,EAAS,GAAKA,EAAS,IACxDF,EAAIjH,QACJ7S,KAAK2O,cAAcjL,cACfoW,EAAK,GAAI7J,eAAesC,SAAWtC,eAAeG,UACtDpQ,KAAK2O,cAAcjL,cACfkW,EAAa,GACb3J,eAAeE,SAAWF,eAAe4J,kBAEtCK,EAAmB,aAG5BnK,+BAAA,SACIlG,EACAuQ,GAFJ,WAGE,oBADEA,KACK/Y,QAAM8N,QAAQ,uBACjBtF,EAAOgE,OACH,SAAAoK,GAAS,OAAiD,MAAjDxU,EAAKoL,UAAUpM,IAAIwV,EAAM5H,QAAQgB,cACtClH,OAAK6L,cAAciC,EAAM1S,OAAS6U,MAGhDrK,uBAAA,WACE,OAAO/P,KAAK6O,UAAUwL,aAAera,KAAKuQ,0BAA0BjQ,QAGtEyP,oBAAA,WACM/P,KAAKsa,WAGTta,KAAK2O,cAAc4L,UACnBva,KAAK4O,eAAe2L,UACpBva,KAAKsa,UAAW,OA9uBeE,iBAkBlBzK,aAAa,EC7G9B,ICAY0K,EDORlN,KACFmN,kBAAgB,UAAU,4HAWR,OARhBrZ,QAAMgB,IAAI,gCAAgC,GAEpCsY,EAA0C,CAC9CC,gBAAiBvZ,QAAMoB,IAAI,4BACvB,YACA,uBAGgBiL,UAAUC,IAAIkN,eAAeF,WAezB,OAfpBG,EAAU1S,SACV2S,EAAgBD,EAAQvD,OACxByD,EAAwC,GACxCvM,EAAmBqM,EAAQpM,SAAStM,IAAI,mBAC9C4Y,EAAiBC,eAAiB,CAChCC,+BACIH,EAAcG,+BAClB1D,iCACIuD,EAAcvD,iCAClB2D,4BAA+BJ,EAAcI,6BAG3C1M,IACFuM,EAAiBI,iBAAmB,CAAC,uBAEPN,EAAQO,cAAcL,WAElC,OAFdtZ,EAAoB0G,YAEC0S,EAAgBQ,6BAC3C,OADM/Z,EAAc6G,YACb,IAAI2H,EAAcrO,EAAQH,aAChC,GCtCL,SAAYkZ,GACVA,iBACAA,iBACAA,qBACAA,iBACAA,iBACAA,qBACAA,yBACAA,qCACAA,mBACAA,+BACAA,kCACAA,8BACAA,gDACAA,0BACAA,kBACAA,sBACAA,kBACAA,kBACAA,sDACAA,qDACD,CArBD,CAAYA,IAAAA,OAuBZ,ICvBYc,ED4BNC,EAA+B,uOAe/BC,EAAyB,mDAE3BD,SAkHJ,SAASE,EACLvb,EAAYwb,EAAkBC,gBAAAA,kBAChC,IAAMC,EAAkBF,EAAUF,EA1IV,qEA2IxB,OAAOE,EAAU,2BACKC,uCACSzb,mBACzB0b,EACE,+BAGSA,EAAkB,gBACxB1b,eAEb,UAEgB2b,EACZ1S,EAAoBuS,GACtB,OAAQvS,GACN,KAAKqR,EAAasB,IAChB,MA1HM,gBA2HR,KAAKtB,EAAauB,IAChB,MApIM,gBAqIR,KAAKvB,EAAawB,MAChB,OAAOP,EAAuB,QAASC,GACzC,KAAKlB,EAAayB,IAChB,MA9HM,gBA+HR,KAAKzB,EAAa0B,IAChB,MAnIM,gBAoIR,KAAK1B,EAAa2B,MAChB,OAAOT,EAhIM,4BADL,sBAkIV,KAAKlB,EAAa4B,QAChB,OAAOV,EAhIQ,2BADL,qBAkIZ,KAAKlB,EAAa6B,cAChB,OAAOX,EAhIc,4BADL,sBAkIlB,KAAKlB,EAAa8B,KAChB,OAAOZ,EAhIK,2BADL,qBAkIT,KAAKlB,EAAa+B,WAChB,OAAOb,EAhIW,4BADL,sBAkIf,KAAKlB,EAAagC,YAChB,OAAOd,EAhIY,+EADL,8CAkIhB,KAAKlB,EAAaiC,UAChB,OAAOf,EA5FU,0XANL,iFAmGd,KAAKlB,EAAakC,mBAChB,MAjJqB,4BAkJvB,KAAKlC,EAAamC,QAChB,OAAOjB,EA7HQ,kjBAPL,2HAqIZ,KAAKlB,EAAaoC,MAChB,OAAOlB,EApDM,2IADL,4CAsDV,KAAKlB,EAAaqC,IAChB,OAAOpB,EAAuB,MAAOC,GACvC,KAAKlB,EAAasC,IAChB,OAAOrB,EAAuB,MAAOC,GACvC,KAAKlB,EAAauC,IAChB,OAAOrB,EArFI,u6BAZL,0NAkGR,KAAKlB,EAAawC,sBAChB,MAjKwB,wCAkK1B,KAAKxC,EAAayC,sBAChB,MAlKwB,wCAmK1B,QACE,MAAM,IAAI9Z,MAAM,cAAcgG,0BAEpC,EC5NA,SAAYmS,GACVA,iBACAA,mBACAA,iBACAA,mBACAA,iBACAA,iBACAA,qBACAA,qBACAA,uBACAA,uBACAA,kBACAA,kCACAA,kBACAA,oBACAA,sBACAA,8BACAA,gCACAA,sBACAA,kBACAA,oBACAA,0BACAA,oBACAA,wBACAA,oBACAA,uBACD,CA1BD,CAAYA,IAAAA,gBAyFI4B,EAAiB/T,EAAmBuS,GAClD,OAAQvS,GACN,KAAKmS,EAAY6B,IACf,MAhEM,iBAiER,KAAK7B,EAAY8B,IACf,MAhEM,iBAiER,KAAK9B,EAAY+B,KACf,MAjEO,8DAkET,KAAK/B,EAAYgC,KACf,MArEO,kBAsET,KAAKhC,EAAYiC,IACf,OAAO7B,EA/DI,wQADL,sDAiER,KAAKJ,EAAYkC,IACf,MAjDM,iBAkDR,KAAKlC,EAAYmC,MACf,MArEQ,uBAsEV,KAAKnC,EAAYoC,MACf,MApDQ,mBAqDV,KAAKpC,EAAYqC,OACf,MArDS,wBAsDX,KAAKrC,EAAYsC,OACf,MAtDS,YAuDX,KAAKtC,EAAYuC,IACf,MAvDM,0DAwDR,KAAKvC,EAAYwC,YACf,MAvDc,2BAwDhB,KAAKxC,EAAYyC,IACf,MAxDM,aAyDR,KAAKzC,EAAY0C,UACf,OAAOtC,EAxDU,sJADL,wDA0Dd,KAAKJ,EAAY2C,WACf,MAtDa,kBAuDf,KAAK3C,EAAY4C,KACf,OAAOxC,EAnDK,8DAJL,kCAwDT,KAAKJ,EAAY6C,MACf,OAAOzC,EAtDT,iFAFU,6BAyDV,KAAKJ,EAAY8C,MACf,MApDQ,sBAqDV,KAAK9C,EAAY+C,QACf,MArDU,sCAsDZ,KAAK/C,EAAYgD,IACf,MAtDM,iBAuDR,KAAKhD,EAAYiD,KACf,MAvDO,6DAwDT,KAAKjD,EAAYkD,KACf,MArDO,kBAsDT,KAAKlD,EAAYmD,OACf,MAtDS,gBAuDX,KAAKnD,EAAYoD,KACf,MAvDO,qFAwDT,KAAKpD,EAAYqD,OACf,MArDS,wBAuDX,QACE,MAAM,IAAIxb,MAAM,cAAcgG,0BAEpC,CC5IO,IAAMyV,EAAc,SAACC,GAC1B,OAAQA,GACN,KAAK,EACH,MAAO,MACT,KAAK,EACH,MAAO,YACT,KAAK,EACH,MAAO,YACT,KAAK,EACH,MAAO,YACT,QACE,MAAM,IAAI1b,MAAS0b,kCAEzB,WAEgBC,EACZC,EAAqCC,EACrCC,EAAgBC,GAClB,gBAFuCF,mBACrCC,mBAAgBC,KACC,OAAfH,EACF,MAAO,GAGT,IAAII,EAAsB,GAC1B,GAAmB,WAAfJ,EACFI,EAAsBjC,EAAiB5B,EAAYsC,aAC9C,GAAmB,SAAfmB,EACTI,EAAsBjC,EAAiB5B,EAAY4C,KAAMe,QACpD,GAAmB,QAAfF,EACTI,EAAsBjC,EAAiB5B,EAAYiC,IAAK0B,QACnD,GAAmB,UAAfF,EACTI,EAAsBjC,EAAiB5B,EAAY6C,MAAOc,QACrD,GAAmB,UAAfF,EACTI,EAAsBtD,EAAkBrB,EAAaoC,MAAOqC,QACvD,GAAmB,YAAfF,EACTI,EAAsBjC,EAAiB5B,EAAY+C,QAASY,OACvD,IAAmB,cAAfF,EAGT,MAAM,IAAI5b,MAAM,cACZ4b,uDAHJI,EAAsBjC,EAAiB5B,EAAY0C,UAAWiB,GAKhE,IACMG,EAAWR,EADGK,EAAS,EAAI,GAiBjC,OAdID,EACoB,6BACAI,mBAAyBF,eAC3CE,mFAEED,cAGgB,6BACAC,mBAAyBF,eAC3CE,iBACED,aAIV,UAEgBE,EACZC,EAAkBP,GACpB,MAAO,YACDO,EAAU,iDAAmD,gBAC7DP,EAAa,qCAAuC,cAE5D,UCnEgBQ,EACZC,EAAyBC,EAAyBhT,EAClDiT,EAAqBC,EAAmBC,EAAmBC,EAC3DhB,gBADqBc,mBAAsCE,mBAC3DhB,KACF3U,OAAK2B,OACDY,GAA4B,IAAdoS,IAAoBpS,GAClC,WAAM,MAAA,cAAcA,4CAChBoS,KACR,IAAMiB,EAAU,wBACEN,EAAiB,IAAM,wBAErC/S,EAAa,iCACA,6CAGXsT,EAAUL,EAAa,iCACA,iCAE7B,MAAO,0DAC8Cd,EAAYC,0BACjDD,EAAYC,oCACNA,aAElBc,GAAaE,EACTC,EACA,UAEIrT,EACI,0DACA,8EAEVqT,uGAM+ClB,EAAYC,gCAC3CA,yBACNY,EAAiB,IAAM,iCACvBb,EAAYC,kBACxBkB,gCAIN,UAEgBC,EACZV,EAAkBP,EAClBS,EAAyBC,EAAyBhT,EAClDiT,EAAqBC,EAAmBC,EAAmBC,EAC3DhB,GACF,oBAFuBc,mBAAmBC,mBAAmBC,mBAC3DhB,KACK,OAEHU,EACIC,EAAgBC,EAAgBhT,EAAYiT,EAAYC,EACxDC,EAAWC,EAAUhB,iEAEzBD,EAAYC,iCACMA,aAElBc,GAAaC,EACT,GACA,2JAIFP,EAAsBC,EAASP,uFAKvC,UAqDgBkB,EACZC,EAAyBja,EACzBwG,EAAoB0T,EAAgBC,EAAgBC,EACpDC,gBADA7T,mBAAoB0T,mBAAgBC,mBAAgBC,mBACpDC,MACF,IAAMC,EAAata,EAAc,GAAKia,EAAc,GAC9CM,EAAava,EAAc,GAAKia,EAAc,GAC9CO,EAAahU,EAAa8T,EAAaJ,EACvCO,EAAajU,EAAa0T,EAAYI,EACtCI,EAAmBF,EAAaxa,EAAc,GAC9C2a,EAAgBT,EAAYla,EAAc,GAahD,OAZAiE,OAAK2B,QACCY,GAAmC,IAArBkU,GAA+C,IAArBT,EAAc,KACrDzT,IAAoC,IAArBkU,GAA+C,IAArBA,KACxCF,EAAaxa,EAAc,IAAO,GAClCka,EAAYla,EAAc,IAAO,GAA0B,IAArBia,EAAc,IACxD,WAAM,MAAA,iBAAiBzT,gCACnBkU,2BAAyCT,EAAc,yDACzBS,wCACrBF,2CACTxa,EAAc,kBACdka,4CACAla,EAAc,qBAAoBia,EAAc,oBACjD,+CACmCS,YACtCF,EAAaE,QAAsBD,2DAEnCF,EAAaN,EAAc,SAAQC,kCAEhBD,EAAc,gCACdA,EAAc,oCACVS,4BACPR,qeAYFG,EAAY,IAAM,2FAGhBA,EAAY,IAAM,+FAEtBF,EAAS,IAAM,uEACeG,8BAG1CH,EAAS,GAAGjb,KAAKuR,KAAK2J,EAAkBF,GAC/B,mEACIC,EAAS,qBAAqBC,EAAoB,+HAKrCO,+RA9GG,SAACC,GAClC,OAAIA,EACK,4JAOA,sJAMX,CAqGcC,CAA2BrU,iHAKnCmU,goBAcqB,IAArBD,EACI,GACA,iFAvHN,SAAClU,EAAqBkU,GACpB,OAAIlU,EACK,yNAKkB,IAArBkU,EACI,GACA,sSAMiB,IAArBA,EACI,GACA,2DAGD,mRAOkB,IAArBA,EAAyB,GACA,wDAGjC,CA0FUI,CAAuBtU,EAAYkU,8NAUjD,CAEA,IAAMK,EAAyB,SAACH,GAC9B,OAAIA,EACK,yIAOA,wIAMX,WAUgBI,GACZf,EAAyBja,EACzBwG,EAAoB0T,EAAgBC,EAAgBC,EACpDa,gBADAzU,mBAAoB0T,mBAAgBC,mBAAgBC,mBACpDa,MACF,IAAMX,EAAaL,EAAc,GAAKja,EAAc,GAC9Cua,EAAaN,EAAc,GAAKja,EAAc,GAC9Cwa,EAAahU,EAAa8T,EAAaJ,EACvCO,EAAajU,EAAa0T,EAAYI,EAC5CrW,OAAK2B,OACD6U,EAAaza,EAAc,IAAO,GAC9Bwa,EAAaxa,EAAc,IAAO,GAClCka,EAAYla,EAAc,IAAO,GACrC,WAAM,MAAA,cAAcya,2CAChBza,EAAc,mBACdwa,2CACAxa,EAAc,kBACdka,2CAAkDla,EAAc,MACxE,IAAMkb,EAAgBT,EAAaza,EAAc,GAC3Cmb,EAAgBX,EAAaxa,EAAc,GAC3C2a,EAAgBT,EAAYla,EAAc,GAC1Cob,EAAgBH,EAClB,iIAG4CX,wDACAC,qMAMxCE,6BAAqCza,EAAc,8DAEnDwa,6BAAqCxa,EAAc,uBAC/C+a,EAAuBvU,mIAK3B0T,6BAAoCla,EAAc,kEAElDua,6BAAqCva,EAAc,+gBAaAA,EAAc,wIAIjEwG,EACI,oCAAoCxG,EAAc,QAClD,iCAAiCA,EAAc,qbAUDA,EAAc,yJAEZA,EAAc,oGAKtE,yPAMwCsa,4CAEVY,0CACAC,0CACAR,4KAK1BO,2EAEAC,qIAGAJ,EAAuBvU,8GAMvBmU,kwBA/GsB,SAACnU,GAC/B,OAAOA,EAAa,gDAEA,+CACtB,CA+HU6U,CAAwB7U,ygBAkBhC,MAAO,mDACuCgU,QAAgBC,uDAChBF,QAAgBL,kCACrCD,EAAc,kCACdA,EAAc,+BACjBC,idAWJC,EAAS,IAAM,+CAE7BA,EAAS,GAAGjb,KAAKuR,KAAK2J,EAAkBF,GAC/B,qEACMC,EAAS,qBAAqBC,EAAoB,8XAU/DgB,eAGR,CA+DA,kBAwBE,WACIE,EAAkCpV,EAClCqT,EAAyBC,EAAyBhT,EAClDiT,EAAoB8B,EACpBzC,EACA0C,EACAP,sBAJkDzU,mBAClDiT,mBAAoB8B,qBACpBzC,qBACA0C,qBACAP,MAzBJnhB,mBAAgB,CAAC,IAAK,KACtBA,cAAW,oDAyBTA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,IAC3C,IAAMkE,EAAWE,EAAa8U,EAAO,GAAKA,EAAO,GAMjD,GALAxhB,KAAKuG,QAAWiG,EAAW,GAAM,IAAME,GACvBN,EAAY,GAAK,GAAM,GAAKM,IACxCN,EAAY,GAAK,GAAM,IAAMuT,EACjC3f,KAAKugB,UAA+B,IAAnBnU,EAAY,KAAaM,GAErC1M,KAAKuG,QAAUvG,KAAKugB,UAEvBvgB,KAAKqM,kBAAoB,CAAC,EAAG,EAAG,GAChCrM,KAAKkG,cAAgB,CAAC,GAAI,EAAG,OACxB,CACL,IAAMyb,EAAgBrV,EAClBF,EAAY,GAAII,EAAUJ,EAAY,GAAIM,GAC9C1M,KAAKkG,cAAgByb,EAAczb,cACnClG,KAAKqM,kBAAoBsV,EAActV,kBAGzCrM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5ClG,KAAKqM,mBAET,IAAMuV,EAAkB,MAARH,EACVxC,EAAsD,MAA1ByC,EAC9BE,GACF5hB,KAAK6G,cAActG,KAAK,QAGtB0e,GACFjf,KAAK6G,cAActG,KAAK,0BAG1BP,KAAKmhB,0BAA4BA,EACjCnhB,KAAK0M,WAAaA,EAClB1M,KAAK2f,WAAaA,EAClB3f,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAKif,0BAA4BA,EACjCjf,KAAKyf,eAAiBA,EACtBzf,KAAK0f,eAAiBA,EACtBtX,qCAACpI,oBAAgBA,oBAAgBA,mBAEjCA,KAAKmY,UAAY,gBAAgBnY,KAAKqM,sBAAqBK,MACvDiT,MAAc3f,KAAKgf,eAAchf,KAAK4f,cAAa5f,KAAK6f,cACxD7f,KAAK8f,aAAY9f,KAAKuG,WAAUvG,KAAKugB,cACrCvgB,KAAKyf,mBAAkBzf,KAAK0f,mBAC5B1f,KAAKmhB,iCAGXU,wBAAA,SAAYtV,EAAmBE,EAAmBD,GAEhD,IAAMgU,EAAaxgB,KAAKkG,cAAc,GAAKlG,KAAKqM,kBAAkB,GAC5DoU,EAAazgB,KAAKkG,cAAc,GAAKlG,KAAKqM,kBAAkB,GAYlE,OAVKrM,KAAKuG,QAAUvG,KAAKugB,UAEvBvgB,KAAKogB,UAAoC,EAAxBpgB,KAAKkG,cAAc,GAEpClG,KAAKogB,UAAYK,EAMZ,CAHWlU,EAAYiU,GAAe,EAC3B/T,EAAYgU,GAAe,EAC5BjU,EAAWxM,KAAKogB,WAAc,IAIjDyB,wBAAA,WACE,IAjJA3b,EAAyCwG,EAiJnCoV,EAAW,WAEb/C,EACI/e,KAAKgf,WAAYhf,KAAKif,0BAA2Bjf,KAAKuG,mBAE1D0Z,EACIjgB,KAAK4hB,QAAS5hB,KAAKgf,WAAYhf,KAAKyf,eACpCzf,KAAK0f,gBACL,EACA1f,KAAK2f,WAAY3f,KAAK4f,UAAW5f,KAAK6f,UAAW7f,KAAK8f,SACtD9f,KAAKuG,OAAS,EAAI,eAEtBvG,KAAKuG,OACD2Z,EACIlgB,KAAKqM,kBAAmBrM,KAAKkG,cAAelG,KAAK0M,WACjD1M,KAAKogB,WAAW,EAAO,KAAMpgB,KAAKugB,WACrCvgB,KAAKugB,WAjKdra,EAkK8BlG,KAAKkG,wBAlKMwG,EAkKS1M,KAAK0M,cAlKdA,MAC3CvC,OAAK2B,OACoB,IAArB5F,EAAc,IAAiC,IAArBA,EAAc,IACxC,WAAM,MAAA,iDAAiDA,SACpD,0BACiC,EAAnBA,EAAc,uDACWA,EAAc,gBAExD6b,giBAxBqB,SAACjB,GAC1B,OAAOA,EAAY,sLAMA,qLAMrB,CAyBuCkB,CAAmBtV,6pBA6I5BwU,GACIlhB,KAAKqM,kBAAmBrM,KAAKkG,cAC7BlG,KAAK0M,WAAY1M,KAAKogB,WAAW,EAAO,KACxCpgB,KAAKmhB,qCAEnC,OAAOW,QC7jBX,kBAgBE,WACI1V,EAAuCqT,EACvCC,EAAyBhT,EAAoBiT,EAC7C8B,EAAyBzC,EACzB0C,gBAFyBhV,mBAAoBiT,mBAC7C8B,qBAAyBzC,qBACzB0C,QAfJ1hB,mBAAgB,CAAC,IAAK,KACtBA,cAAW,oDACXA,mBAA0C,CAAC,IAAK,EAAG,GAcjDA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB,CAACrB,EAAG,GAAI3H,EAAG,CAAC,EAAG,GAAImJ,EAAG,CAAC,IAC7CtI,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhD,IAAM0b,EAAkB,MAARH,EACVxC,EAAsD,MAA1ByC,EAC9BE,GACF5hB,KAAK6G,cAActG,KAAK,QAGtB0e,GACFjf,KAAK6G,cAActG,KAAK,0BAG1BP,KAAK0M,WAAaA,EAClB1M,KAAK2f,WAAaA,EAClB3f,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAKif,0BAA4BA,EACjCjf,KAAKyf,eAAiBA,EACtBzf,KAAK0f,eAAiBA,EACtB1f,KAAKmY,UAAY,gBAAgBnY,KAAKgf,eAActS,MAChDiT,MAAc3f,KAAKyf,mBAAkBzf,KAAK0f,sBAGhDuC,wBAAA,WASE,MARiB,WACblD,EAAoB/e,KAAKgf,WAAYhf,KAAKif,sCAE1CgB,EACIjgB,KAAK4hB,QAAS5hB,KAAKgf,WAAYhf,KAAKyf,eACpCzf,KAAK0f,eAAgB1f,KAAK0M,WAAY1M,KAAK2f,YALlC,6EAhFfoC,IAgFe,45BC3BrB,kBAgBE,WACIP,EAAkCU,EAClC9V,EAAuCM,EACvCiT,EAAoB8B,EACpBzC,EACA0C,gBAHuChV,mBACvCiT,mBAAoB8B,qBACpBzC,qBACA0C,QAhBJ1hB,mBAAgB,CAAC,IAAK,KACtBA,cAAW,oDACXA,mBAA0C,CAAC,GAAI,EAAG,GAehDA,KAAKoM,YAAcA,EAEnBpM,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,IAC3CtI,KAAK+L,SAAW,CACd3G,KAAKuR,KAAKvK,EAAY,GAAKpM,KAAKkG,cAAc,IAC9Cd,KAAKuR,KAAKvK,EAAY,GAAKpM,KAAKkG,cAAc,IAAKkG,EAAY,IAGjE,IAAMwV,EAAkB,MAARH,EACZG,GACF5hB,KAAK6G,cAActG,KAAK,QAG1B,IAAM0e,EAAsD,MAA1ByC,EAC9BzC,GACFjf,KAAK6G,cAActG,KAAK,0BAG1BP,KAAK0M,WAAaA,EAClB1M,KAAK2f,WAAaA,EAClB3f,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAKif,0BAA4BA,EACjCjf,KAAKyf,eAA+B,IAAd+B,EAAO,GAC7BxhB,KAAK0f,eAA+B,IAAdwC,EAAO,GAC7BliB,KAAKmY,UAAY,yBAAyBnY,KAAKgf,eAActS,MACzDiT,MAAc3f,KAAKyf,mBAAkBzf,KAAK0f,sBAGhDyC,wBAAA,WACE,IA7GAjc,EACIsa,EACAC,EACAL,EAkHJ,MARiB,WACbrB,EAAoB/e,KAAKgf,WAAYhf,KAAKif,sCAE1CgB,EACIjgB,KAAK4hB,QAAS5hB,KAAKgf,WAAYhf,KAAKyf,eACpCzf,KAAK0f,eAAgB1f,KAAK0M,WAAY1M,KAAK2f,wBAlHnDzZ,EAmHoClG,KAAKkG,cAlHrCsa,EAAata,EAAc,GAC3Bua,EAAava,EAAc,GAE1B,kDADDka,EAAYI,EAAaC,EAAaD,EAAaC,SAEED,qDACfC,QAAgBL,6dAQ1D2B,kSAQ2C3B,kUAQfA,sCACAA,scAYEA,wCACAA,oCAENA,EA+DP,mNC3FnB,WACIhU,EAAuCI,EACvCiT,EAAyBC,EAAyBhT,EAClDiT,gBADkDjT,mBAClDiT,MAfJ3f,mBAAgB,CAAC,IAAK,KACtBA,cAAW,oDACXA,mBAA0C,CAAC,EAAG,EAAG,GAIjDA,aAAS,EAGTA,aAAS,EACTA,qBAAkB,IAMhBmK,OAAK2B,OACkB,IAAnBM,EAAY,IACZ,WAAM,MAAA,kDACVpM,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,EAAG,IAC9CtI,KAAKuG,QAAUmG,GAAc1M,KAAKoM,YAAY,GAAK,GAAM,IACzCM,GAAcF,EAAW,GAAM,IAC3CxM,KAAKoM,YAAY,GAAK,GAAM,EAChCpM,KAAKqM,kBAAoB,CAAC,EAAG,EAAGrM,KAAKsgB,iBAEhCtgB,KAAKuG,SACJvG,KAAKoM,YAAY,GAAK,KACxBpM,KAAKqM,kBAAkB,GAAK,GAE1BrM,KAAKoM,YAAY,GAAK,KACxBpM,KAAKqM,kBAAkB,GAAK,IAIhCrM,KAAK+L,SAAWI,EACZnM,KAAKmI,eACL,CACEnI,KAAKoM,YAAY,GAAIpM,KAAKoM,YAAY,GAAIpM,KAAKoM,YAAY,GAC3DI,GAEFxM,KAAKkG,cAAelG,KAAKqM,mBAE7BrM,KAAK0M,WAAaA,EAClB1M,KAAK2f,WAAaA,EAClB3f,KAAKyf,eAAiBA,EACtBzf,KAAK0f,eAAiBA,EACtB1f,KAAKmY,UACD,gBAAgBzL,MAAciT,MAAcF,MACxCC,MAAkB1f,KAAKqM,sBAAqBrM,KAAKuG,cAG3D6b,wBAAA,WAGE,IAkBMtD,EAAY9e,KAAKuG,OAAS,EAAI,EAC9Bub,EAAW,WAEbtC,EACIxf,KAAKyf,eAAgBzf,KAAK0f,gBAAgB,EAAO1f,KAAK2f,YACtD,EAAO,GAAO,EAAOb,sEAEzBD,EAAYC,qCACQA,qWA1BC,SAACA,GACxB,MAAO,+BACeA,+MAMlBA,EAAY,EAAI,WAAa,uQAwB3BuD,CAAiBvD,mCAIrB9e,KAAKuG,OAAS2Z,EACIlgB,KAAKqM,kBAAmBrM,KAAKkG,cAC7BlG,KAAK0M,WAAY,IAAI,EAAM1M,KAAKsgB,iBACpCY,GACIlhB,KAAKqM,kBAAmBrM,KAAKkG,cAC7BlG,KAAK0M,WAAY,IAAI,EAAM1M,KAAKsgB,2BAEtD,OAAOwB,sBAiBT,WACI1V,EAAuBqV,EACvBzC,EACA0C,gBAFuBD,qBACvBzC,qBACA0C,QAbJ1hB,cAAW,GAGXA,mBAAgB,CAAC,KACjBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EASLA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAK4hB,QAAkB,MAARH,EACfzhB,KAAKif,0BAAsD,MAA1ByC,EACjC1hB,KAAKgf,WAAaA,EACdhf,KAAK4hB,SACP5hB,KAAK6G,cAActG,KAAK,QAGtBP,KAAKif,2BACPjf,KAAK6G,cAActG,KAAK,0BAG1BP,KAAKmY,UAAY,kBAAkB6G,SAGrCsD,wBAAA,WACE,MAAO,SACLvD,EAAoB/e,KAAKgf,WAAYhf,KAAKif,oCAC1C8C,EAAK,4JAIDzC,EAAsBtf,KAAK4hB,QAAS5hB,KAAKgf,kGC/IjD,WAAYzZ,GATZvF,mBAA0B,GAC1BA,iBAAwB,GAIxBA,cAAW,eACXA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,cAGnBoK,wBAAA,WAQE,MAPiB,SACfR,EAAK,uICnBKS,GAAKC,GAEZ,IAAAC,YAASC,UACTpd,UAAOhH,UACT+H,UAIL,GAAc,YAFdA,EAAQA,GAAS6D,OAAKyY,WAAWrkB,IAET,CAEtB,IAAMmT,EAASvH,OAAK0Y,kBAAkBvc,EAAO6D,OAAK6L,cAAczQ,IAEhE,OADAmM,EAAO8Q,KAAKjkB,GACLmkB,EAAQ9O,eAAerO,EAAOe,EAAOoL,GAE5C,IAAM/L,EAAU,IAAI4c,GAAYhd,GAC1Bud,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAAC3O,KAC9C,OAAOmkB,EAAQK,iBAAiBpd,EAAS,GAAIW,EAAOwc,EAExD,CAEO,IAAME,GAA2B,CACtCC,WAAYC,OACZC,YAAa,SACbC,WAAYZ,aCxBEa,GACZZ,GAEK,IAAA5Y,WAAQ8Y,UACR7b,MACAvB,UAED+d,EAAQnZ,OAAK6L,cAAclP,EAAEvB,OAC7Bge,EAASpZ,OAAKqZ,uBAAuBje,EAAO+d,GAC5CG,EAAStZ,OAAK6L,cAAcuN,GAUlC,OARApZ,OAAK2B,OACDwX,IAAUG,GACV,WAAM,MAAA,kBAAkBF,WAAeE,EAAjC,gCACQ3c,EAAEvB,eAAc+d,EADxB,mFAKVb,EAAKC,QAAQgB,OAAO5c,EAAEuJ,QACf,CAACA,OAAQvJ,EAAEuJ,OAAQ9K,MAAOge,EAAQjd,MAAOQ,EAAER,MACpD,CAEO,IAAMqd,GAA8B,CACzCV,WAAYW,UACZT,YAAa,SACbC,WAAYC,aCJEQ,GAAgBzb,eAC9B0b,MACAzmB,MACAqP,eACAiT,eACA+C,YACAqB,SAAAtC,oBACAuC,2BAAAtC,oBACAuC,mBAAAC,iBACAC,eAAAnF,oBAEMoF,EAAQN,EAAEve,MAAMjF,OAChB+jB,EAAQhnB,EAAEkI,MAAMjF,OAEhBgkB,EAAc5X,EAAaoX,EAAEve,MAAM6e,EAAQ,GAAKN,EAAEve,MAAM6e,EAAQ,GAChEG,EAAc5E,EAAatiB,EAAEkI,MAAM8e,EAAQ,GAAKhnB,EAAEkI,MAAM8e,EAAQ,GAEhEG,EAAc9X,EAAaoX,EAAEve,MAAM6e,EAAQ,GAAKN,EAAEve,MAAM6e,EAAQ,GAChEK,EAAc9E,EAAatiB,EAAEkI,MAAM8e,EAAQ,GAAKhnB,EAAEkI,MAAM8e,EAAQ,GAEhEK,EAAaZ,EAAEve,MAAM4B,MAAM,GAAI,GAC/Bwd,EAAatnB,EAAEkI,MAAM4B,MAAM,GAAI,GAE/Byd,EAAYza,OAAK6L,cAAc0O,GAC/BG,EAAY1a,OAAK6L,cAAc2O,GAI/Bzc,EAFoB4c,iBAAeC,2BACrCjB,EAAEve,MAAM4B,MAAM,GAAI,GAAI9J,EAAEkI,MAAM4B,MAAM,GAAI,IACThG,OAAO,CAACqjB,EAAaC,IAExDta,OAAK2B,OACDwY,IAAgBC,GAChB,WAAM,MAAA,kCAAkCD,YACjCC,8BAAuCT,EAAEve,cACzClI,EAAEkI,yBAAwBmH,EAC7B,mBAAmBiT,oBAE3B,IAsBIha,EACAqf,EAvBEC,EAAqCvY,EACvC,CAACkY,EAAWN,EAAaE,GACzB,CAACI,EAAWJ,EAAaF,GACvBY,EAAqCvF,EACvC,CAACkF,EAAWJ,EAAaF,GACzB,CAACM,EAAWN,EAAaE,GAGvBU,EAAM9B,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGgd,GAAIpB,UAASC,MAAO,CAACpd,MAAO0f,KACvDG,EAAM/B,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGzJ,GAAIqlB,UAASC,MAAO,CAACpd,MAAO2f,KACvDG,EAA8B,CAACF,EAAKC,GAEpCE,EAAWlgB,KAAKC,IAAIuf,EAAWC,GAC/BpF,EAA+B,IAAdmF,EACjBlF,EAA+B,IAAdmF,EAEjBhb,EAAuB,CAACsb,EAAKC,GAC7Bvc,EAAa,CACjB,CAACO,KAAM,QAAS8D,KAAM,CAACsX,IAAe,CAACpb,KAAM,QAAS8D,KAAM,CAACuX,IAC7D,CAACrb,KAAM,QAAS8D,KAAM,CAACoX,KAKnBlY,EACF,CAACkZ,EAAUd,EAAaC,GACxBc,EAAoBlkB,QAAMoB,IAAI,8BA+BlC,OA9BI8iB,EAAoB,IAEpBA,EADEf,EAAcC,GAAe,IACXzY,EAAkBiW,oBAKvB,IAAbqD,GAAkBd,GAAe,KAAOC,GAAe,IACvDF,GAAe,IACGvY,EAAkBoW,oBAWnCoC,GAAe,KACdC,GAAe,KAAOF,GAAe,EAAIE,IAC1CA,GAAe,KACdD,GAAe,KAAOF,GAAe,EAAIE,GACzBxY,EAAkBmW,6BAElBnW,EAAkB6V,qBAIlC0D,GACN,KAAKvZ,EAAkBiW,oBACrBtc,EAAU,IAAIsc,GACV7V,EAAaqT,EAAgBC,EAAgBhT,EAAYiT,EACzD8B,EAAMzC,EAAY0C,GACtB,MACF,KAAK1V,EAAkBoW,oBAQrB,GALA4C,EAAMxC,GACF,CAACE,UAASC,MAAO,CAACpd,MAAO6G,EAAa7N,MAAO,EAAG+H,MAAOwd,EAAExd,SAC7DX,EAAU,IAAIyc,GACVhW,EAAamY,EAAa9E,EAAgBC,EAAgBhT,EAC1DiT,GACA8B,GAAQzC,EAAY,CACtBgG,EACItC,EAAQK,iBAAiBpd,EAASkE,EAAQia,EAAExd,MAAOuC,EAAYmc,GACnE,IAAMQ,EAAwB,IAAIlD,GAC9B0C,EAAIzf,MAAOkc,EAAMzC,EAAY0C,GAC7BoB,EAAc,KACZ2C,EAAiC,CAACT,GACpCvD,GACFgE,EAAiBllB,KAAKkhB,GAEpBC,GACF+D,EAAiBllB,KAAKmhB,GAEL,cAAf1C,IACF8D,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACgX,KACxCsB,EAAsBle,UAAY,iBAEpC,IAAMoe,EAAehD,EAAQK,iBACzByC,EAAuBC,EAAkBT,EAAI1e,MAAOwc,GACxDuC,EAAc9kB,KAAKykB,GACnB,IAAMW,EAActC,GAChB,CAACxZ,OAAQ,CAAC/C,EAAG4e,GAAehD,UAASC,MAAO,CAACpd,MAAO2C,KACxDmd,EAAc9kB,KAAKmlB,OACnB,IAAgB,IAAAE,EAAAC,EAAAR,iCAAe,CAA1B,IAAMzkB,UACT8hB,EAAQ7R,YAAYjQ,EAAEyP,0GAExB,OAAOsV,EAET,MAEF,KAAK3Z,EAAkBmW,6BACrBxc,EAAU,IAAIwc,GACV8C,EAAUC,EAAU9Y,EAAaM,EAAYiT,EAAY8B,EACzDzC,EAAY0C,GAChB,MACF,KAAK1V,EAAkB6V,oBAGrB,IAAMV,GAA4BuB,EAAQnhB,YAAYukB,UACtDngB,EAAU,IAAIkc,GACVoD,EAAU7Y,EAAaqT,EAAgBC,EAAgBhT,EACvDiT,EAAY8B,EAAMzC,EAAY0C,EAC9BP,IACJ,MACF,QACE,MAAM,IAAI/d,MAAM,iCAAiCmiB,OAGjD9D,GACF5X,EAAOtJ,KAAKkhB,GAEVC,GACF7X,EAAOtJ,KAAKmhB,GAEK,cAAf1C,IACFnW,EAAWtI,KAAK,CAAC6I,KAAM,UAAW8D,KAAM,CAACgX,KACzCve,EAAQ2B,UAAY,iBAGtB,IAAMye,GACF1C,GAAQ,CAACxZ,OAAQ,CAAC/C,EAFtBke,EAAMtC,EAAQK,iBAAiBpd,EAASkE,EAAQia,EAAExd,MAAOuC,EAAYmc,IAEtCtC,UAASC,MAAO,CAACpd,MAAO2C,KACvDmd,EAAc9kB,KAAKykB,OACnB,IAAgB,IAAAgB,GAAAH,EAAAR,sCAAe,CAApBzkB,WACT8hB,EAAQ7R,YAAYjQ,EAAEyP,8GAExB,OAAO0V,EACT,CC5KO,IAAME,GAAmC,CAC9ChD,WAAYiD,eACZ/C,YAAa,SACbC,oBAzB2BX,GAKpB,IAAA5Y,WAAQ6Y,YAASC,UACjBmB,MAAGzmB,MAAGokB,SAAMC,2BACZhV,eAAYiT,eAAYX,eAE/B,OAAO6E,GAAgB,CACrBC,IACAzmB,IACAqP,aACAiT,aACA+C,UACAjB,OACAC,yBACAwC,gCACAlF,cAEJ,iBCVE,WAAY7e,EAAkBqhB,EAAkBU,GAThDliB,mBAAgB,CAAC,QAAS,QAAS,QAAS,SAK5CA,mBAA0C,CAAC,IAAK,EAAG,GAEnDA,WAAO,EAGLA,KAAKoM,YAAc9B,eAAaya,2BAA2BvD,EAAQU,GACnEliB,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,mBAAmBhY,EACpCH,KAAKG,GAAKA,SAGZgmB,wBAAA,WAkBE,MAhBiB,gHADHrK,EAAkB9b,KAAKG,IAAI,yBAOrC4hB,EAAK,sYCVX,WAAY5hB,EAAkBqhB,EAAkBU,GAVhDliB,WAAO,EACPA,mBAAgB,CAAC,IAAK,KAUpBA,KAAKoM,YAAc9B,eAAaya,2BAA2BvD,EAAQU,GACnEliB,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAKG,GAAKA,EAEVH,KAAKomB,qBACD5E,EAAOlhB,QAAU,GAAK4hB,EAAO5hB,OAAS,GAAKkhB,EAAO,GAAK,IAC3DxhB,KAAKqmB,qBACDnE,EAAO5hB,QAAU,GAAKkhB,EAAOlhB,OAAS,GAAK4hB,EAAO,GAAK,IAEvDliB,KAAKomB,sBAAwBpmB,KAAKqmB,sBACpCrmB,KAAKuG,QAAS,EAGdvG,KAAKsmB,kBACDtmB,KAAKqmB,qBAAuBnE,EAAO,GAAKV,EAAO,GACnDxhB,KAAKmY,UAAY,UAAUnY,KAAKoJ,SAAQjJ,MAAMH,KAAKsmB,sBAC/CtmB,KAAKqmB,qBACTrmB,KAAKoJ,KAAO,SAGZpJ,KAAKkG,cAAgB,CAAC,IAAK,EAAG,GAC9BlG,KAAKmgB,cAAgB,IAEjBhW,OAAKC,YAAYoX,EAAQU,IACzB/X,OAAK6L,cAAcwL,GAAU,GAAM,GACrCxhB,KAAKuG,QAAS,EACdvG,KAAKoJ,KAAO,OACZpJ,KAAKmgB,cAAgB,IAErBngB,KAAKuG,QAAS,EACdvG,KAAKoJ,KAAO,QACZpJ,KAAKmgB,cAAgB,GAEvBngB,KAAKmY,UAAY,UAAUnY,KAAKoJ,SAAQjJ,EAGxCH,KAAKkG,cAAgB,CAAC,IAAK,EAAG,IAEhClG,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,WAG9BoG,wBAAA,WACE,IAAIzE,EACE0E,EAAQxmB,KAAKuG,OAAS,YAAc,MACpCkgB,EAAU,gCACSD,WAAcA,UAAaA,eAChD1K,EAAkB9b,KAAKG,GAAIH,KAAKuG,yBAIpC,GAAkB,WAAdvG,KAAKoJ,KAAmB,CAC1B,IAAMsd,EAAqB1mB,KAAKsmB,kBAAoB,EAChD,WAAUtmB,KAAKoM,YAAY9L,OAAS,OACpC,IACEqmB,EAAoB3mB,KAAKqmB,qBAC3B,kEACoBK,OACpB,qBAAqBA,oDAEzB5E,EAAW,aACP2E,qDACsCzmB,KAAKsmB,iCAC3CvE,EAAK,sIAGa/hB,KAAKsmB,mEAEvBtmB,KAAKqmB,qBAAuB,IAAM,8KAM9BM,yGAMR7E,EAAW,YACR2E,cACA1E,EAAK,4OAUV,OAAOD,iBClHK8E,GACZnE,GACK,IACA3b,aAGP,OADA2b,EAAKC,QAAQgB,OAAO5c,EAAEuJ,QACf,CAACA,OAAQvJ,EAAEuJ,OAAQ9K,MAAOuB,EAAEvB,MAAOe,MAAOQ,EAAER,MACrD,CAEO,IAAMugB,GAA+B,CAC1C5D,WAAY6D,WACZ3D,YAAa,SACbC,WAAYwD,aCFEG,GAAQtE,GAEf,IAAA5Y,WAAQ6Y,YACR5R,SAAMC,SAEPiW,EAActE,EAAQ9O,eAAe9C,EAAKvL,MAAO,aACjDwhB,EAAUrE,EAAQ7T,UAAUpM,IAAIukB,EAAY3W,QAE5C4W,EAAiBL,GAAS,CAAC/c,OAAQ,CAAC/C,EAAGgK,GAAO4R,YAE9CwE,EAAiBN,GAAS,CAAC/c,OAAQ,CAAC/C,EAAGiK,GAAO2R,YAIpD,OAFAqE,EAAQnW,mBAAqB,CAACE,KAAMmW,EAAgBlW,KAAMmW,GAEnDF,CACT,CAEO,IAAMG,GAA8B,CACzClE,WAAYmE,UACZjE,YAAa,SACbC,WAAY2D,kBClBZ,WAAY3a,EAAuBjM,GANnCH,mBAAgB,CAAC,KAIjBA,WAAO,EAKLA,KAAKkG,cAAgB,CADE,IACe,EAAG,GACzClG,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKG,GAAKA,EACVH,KAAKmY,UAAY,SAAShY,SAG5BknB,wBAAA,WACE,MAAO,wDAEDlK,EAAiBnd,KAAKG,IAAI,uBAE5B4hB,EAAK,mMCLGuF,GACZlf,OAACmf,WAAQC,kBAAelhB,UAC1B,OAAO,SAAC8B,OAACyB,WAAQ6Y,YACR5b,MACD2gB,EAAgB/E,EAEhBgF,EAASphB,GAASQ,EAAER,MAC1B,GAAImhB,EAAcE,mBAAmB,CAAC7gB,KAAwB,MAAjB0gB,EAAuB,CAClE,IAAMI,EAAQH,EAAc5Y,UAAUpM,IAAIqE,EAAEuJ,QACtCwX,EAAYL,EAAcI,EAAMlW,OAAsBgW,GAC5D,OAAOD,EAAc7T,eAAe9M,EAAEvB,MAAOmiB,EAAQG,GAGvD,IAAMliB,EAA0B,IAAI0hB,GAAevgB,EAAEvB,MAAOgiB,GAC5D,OAAOE,EAAc1E,iBAAiBpd,EAAS,CAACmB,GAAI4gB,GAExD,UAkBgBI,GACZ1f,OAACmf,WAAQC,kBAAenf,oBAAA0f,gBAAyBzhB,UAEnD,OAAO,SAAC8B,SAACyB,WAAQ6Y,YACRoB,MAAGzmB,MACJoqB,EAAgB/E,EAEtB,GAAIqF,GAA+B,cAAZjE,EAAExd,MAAuB,CAC9C,IAAM0hB,EAAQP,EAAc5Y,UAAUpM,IAAIqhB,EAAEzT,QACtC4X,EAAQR,EAAc5Y,UAAUpM,IAAIpF,EAAEgT,QACxCS,SAAkBC,SACtB,GAAIwW,IAAW9M,EAAasB,IAC1B1T,iWAACyI,OAAMC,WAsBF,CACL,IAAMmX,EAAc,IAAI/B,GACpB1L,EAAawC,sBAAuB6G,EAAEve,MAAOlI,EAAEkI,OAC7C4iB,EAAc,IAAIhC,GACpB1L,EAAayC,sBAAuB4G,EAAEve,MAAOlI,EAAEkI,OAE7C6iB,EAAS,CACb,CACE/X,OAAQ2X,EAAMpX,mBAAmBE,KAAKT,OACtC/J,MAAO0hB,EAAMpX,mBAAmBE,KAAKxK,MACrCf,MAAOue,EAAEve,OAEX,CACE8K,OAAQ2X,EAAMpX,mBAAmBG,KAAKV,OACtC/J,MAAO0hB,EAAMpX,mBAAmBG,KAAKzK,MACrCf,MAAOue,EAAEve,OAEX,CACE8K,OAAQ4X,EAAMrX,mBAAmBE,KAAKT,OACtC/J,MAAO2hB,EAAMrX,mBAAmBE,KAAKxK,MACrCf,MAAOlI,EAAEkI,OAEX,CACE8K,OAAQ4X,EAAMrX,mBAAmBG,KAAKV,OACtC/J,MAAO2hB,EAAMrX,mBAAmBG,KAAKzK,MACrCf,MAAOlI,EAAEkI,QAIbuL,EAAO2W,EAAc1E,iBAAiBmF,EAAaE,EAAQ,WAC3DrX,EAAO0W,EAAc1E,iBAAiBoF,EAAaC,EAAQ,WAG7D,IAAMC,EACFtB,GAAQ,CAACld,OAAQ,CAACiH,OAAMC,QAAO2R,QAAS+E,IAO5C,OALAA,EAAc5W,YAAYC,EAAKT,QAC/BoX,EAAc5W,YAAYE,EAAKV,QAIxBgY,EAGT,IAAMX,EAASphB,GAASgiB,aAAWxE,EAAExd,MAAOjJ,EAAEiJ,OAC9C,IAAiB,WAAZwd,EAAExd,OAAkC,WAAZjJ,EAAEiJ,OAC1BmhB,EAAcE,mBAAmB,CAAC7D,EAAGzmB,MACrB,MAAjBmqB,EAAuB,CACnBQ,EAAQP,EAAc5Y,UAAUpM,IAAIqhB,EAAEzT,QAAQqB,OAC9CuW,EAAQR,EAAc5Y,UAAUpM,IAAIpF,EAAEgT,QAAQqB,OADpD,IAEM6W,EAA2B,WAAZzE,EAAExd,MAEnBgE,eAAake,uBAAuBR,GACpCA,EACES,EAA2B,WAAZ3E,EAAExd,MAEnBgE,eAAake,uBAAuBP,GACpCA,EACES,gCAACb,OAAW3f,OAGlB,OAAOuf,EAAc7T,eAAe1L,EAAUwf,EAAQG,GAExD,IAAMliB,EAAU,IAAI4gB,GAAgBgB,EAAQzD,EAAEve,MAAOlI,EAAEkI,OACvD,OAAOkiB,EAAc1E,iBAAiBpd,EAAS,CAACme,EAAGzmB,GAAIqqB,GAE3D,UC1JgBiB,GAA6BxoB,GAE3C,OAAO,SAACqhB,EAAkBU,EAAkB0G,EACpCC,EAAmBviB,GACzB,IAAMwiB,EAAWxe,eAAaya,2BAA2BvD,EAAQU,GAE3D6G,EAAaD,EAASxoB,OACtB0oB,EAAgB7e,OAAKyB,eAAekd,GACpCG,EAAa9e,OAAK6L,cAAc8S,GAEhClqB,EACFuL,OAAKiN,uBAAuB9Q,EAA0B2iB,GAEpD7E,EAAQ5C,EAAOlhB,OACf+jB,EAAQnC,EAAO5hB,OAEf4oB,EAAW/e,OAAKyB,eAAe4V,GAC/B2H,EAAWhf,OAAKyB,eAAesW,GAE/BkH,EAAiB9e,eAAaC,iBAAiBiX,EAAQsH,GACvDO,EAAiB/e,eAAaC,iBAAiB2X,EAAQ4G,GAE7D,GAAIM,EAAe9oB,OAAS+oB,EAAe/oB,SAAW,EACpD,IAAK,IAAIM,EAAI,EAAGA,EAAIhC,EAAO0B,SAAUM,EACnChC,EAAOgC,GAAKT,EAAGyoB,EAAMhoB,EAAIgoB,EAAMtoB,QAASuoB,EAAMjoB,EAAIioB,EAAMvoB,6BAGjDM,GACP,IAAM0oB,EAAMnf,OAAKof,WAAW3oB,EAAGmoB,EAAYC,GAErCQ,EAAOF,EAAIniB,OAAOid,GACxBgF,EAAexlB,SAAQ,SAAAxG,GAAK,OAAAosB,EAAKpsB,GAAK,KACtC,IAAMqsB,EAAStf,OAAKuf,WAAWF,EAAMpF,EAAO8E,GAEtCS,EAAOL,EAAIniB,OAAOkd,GACxBgF,EAAezlB,SAAQ,SAAAxG,GAAK,OAAAusB,EAAKvsB,GAAK,KACtC,IAAMwsB,EAASzf,OAAKuf,WAAWC,EAAMtF,EAAO8E,GAE5CvqB,EAAOgC,GAAKT,EAAGyoB,EAAMa,GAASZ,EAAMe,KAXtC,IAAShpB,EAAI,EAAGA,EAAIhC,EAAO0B,SAAUM,IAA5BA,GAeX,MAAO,CAAChC,EAAQkqB,GAEpB,CC9CO,IAAMe,GACTlB,aAA+B7E,EAAWzmB,GAAc,OAAAymB,EAAIzmB,CAAC,aCCjDysB,GAAsB3pB,GAEpC,OAAO,SAACuR,EAAQpL,EAAOqc,GAGrB,IAFA,IAAMoH,EACF5f,OAAKiN,uBAAuB9Q,EAA0BoL,EAAOpR,QACxDM,EAAI,EAAGA,EAAI8Q,EAAOpR,SAAUM,EACnCmpB,EAAUnpB,GAAKT,EAAGuR,EAAO9Q,GAAI+hB,GAE/B,OAAOoH,EAEX,CCZO,IAAMC,GAAWF,IAAsB,SAACG,GAAO,OAAA7kB,KAAKuR,KAAKsT,MCAzD,IAAMC,GACTvB,IAA6B,SAAC7E,EAAWzmB,GAAc,OAACymB,IAAMzmB,EAAK,EAAI,KCD9D8sB,GAAUL,IAAsB,SAACG,GAAO,OAAA7kB,KAAKglB,IAAIH,MCAjDI,GAAYP,IAAsB,SAACG,GAAO,OAAA7kB,KAAKklB,MAAML,MCArDM,GAAYT,IAAsB,SAACG,GAAO,OAAA7kB,KAAKolB,MAAMP,MCA3D,IAAMQ,GACT9B,IAA6B,SAAC7E,EAAWzmB,GAAc,OAACymB,EAAIzmB,EAAK,EAAI,KCD5DqtB,GACT/B,IAA6B,SAAC7E,EAAWzmB,GAAc,OAACymB,GAAKzmB,EAAK,EAAI,KCD7DstB,GACThC,IAA6B,SAAC7E,EAAWzmB,GAAc,OAACymB,EAAIzmB,EAAK,EAAI,KCD5DutB,GACTjC,IAA6B,SAAC7E,EAAWzmB,GAAc,OAACymB,GAAKzmB,EAAK,EAAI,KCD7DwtB,GAAUf,IAAsB,SAACG,GAAO,OAAA7kB,KAAK0lB,IAAIb,MCAvD,IAAMc,GAAcpC,aACrBqC,EAAQC,GAAW,OAAA7lB,KAAKC,IAAI2lB,EAAkBC,EAAiB,ICDxDC,GAAcvC,aACrBqC,EAAQC,GAAW,OAAA7lB,KAAK+lB,IAAIH,EAAkBC,EAAiB,ICFxDG,GAAezC,aACtBqC,EAAgBC,GAAmB,OAAAD,EAASC,CAAM,ICAjD,IAAMI,GACT1C,aAA+B7E,EAAGzmB,GAAM,OAACymB,IAAMzmB,EAAK,EAAI,CAAC,mBCJtBiuB,iBCGhC,IAAMC,GAAYzB,IAAsB,SAACG,GAAO,OAAA,EAAI7kB,KAAKsS,KAAKuS,MCGrE,kBAQE,WACIuB,EAAmBC,EAAuBC,EAC1CC,EAAkBC,EAAkBC,GACtC7rB,KAAKwrB,UAAYrhB,OAAKoL,aAAaiW,GACnCxrB,KAAKyrB,YAAcA,EACnBzrB,KAAK0rB,QAAUvhB,OAAKoL,aAAamW,GACjC1rB,KAAK2rB,SAAWxhB,OAAKoL,aAAaoW,GAClC3rB,KAAK4rB,SAAWA,EAChB5rB,KAAK8rB,cAAgBD,SAGfE,wBAAA,SAAYC,GAIlB,OAAO5mB,KAAK+lB,IACRnrB,KAAK4rB,SAAW,EAAII,EAAa,EAAIhsB,KAAK4rB,SAAUI,EAAa,IAG/DD,yBAAA,SAAazrB,EAAgB0rB,GACnC,IAAMJ,EAAW5rB,KAAKisB,YAAYD,GAClC,OAAO5mB,KAAKC,IAAI,EAAK/E,EAAS,EAAIsrB,EAAYI,EAAc,IAGtDD,yBAAA,SACJ7e,EAAoBgf,EAAoBrmB,EACxCsmB,EAA0BC,EAAmBJ,GAC/C,mBAASK,GACP,IAAMT,EAAWU,EAAKL,YAAYD,GAC5BO,EAAcnnB,KAAKC,IAAI,EAAGumB,EAAWS,GACrCG,EACFpnB,KAAKC,IAAI,EAAGumB,GAAYQ,GAAaC,EAAa,KAChDI,EAAYT,GAAcO,EAAcC,GACxCE,EACFR,GAAcK,EAAc,EAAI,EAAIF,EAAaT,GAIjDe,EAAY,EAEhBA,GAAaJ,EAAcD,EAAKZ,QAAQprB,OAExC,IAAK,IAAIL,EAAI,EAAGA,EAAIwsB,IAAaxsB,EAC/B0sB,GAAazf,EAAKwf,EAAiBzsB,GAAGK,OAGxCqsB,GAAaH,EAAeF,EAAKX,SAASrrB,OAG1CqsB,IADsBJ,EAAcC,EAAeC,EAAY,GAClCH,EAAKd,UAAUlrB,OAG5CuF,EAAOsmB,EAAmBE,GAAc,IAAIhf,WAAWsf,GACvD,IAAMC,EAAQ/mB,EAAOsmB,EAAmBE,GAEpCQ,EAAiB,EACfC,EAAgB,SAACC,GACnB,OAAAA,EAAInpB,SAAQ,SAACrF,GAAU,OAAAquB,EAAMC,KAAoBtuB,MAErD,IAAS0B,EAAI,EAAGA,EAAIssB,IAAetsB,EACjC6sB,EAAcR,EAAKZ,SACnBoB,EAAcR,EAAKd,WAGrB,IAASvrB,EAAI,EAAGA,EAAIwsB,EAAY,IAAKxsB,EACnC6sB,EAAc5f,EAAKwf,EAAiBzsB,IACpC6sB,EAAcR,EAAKd,WAIrB,GAAIiB,EAAY,EAAG,CAIjBK,EAAc5f,EAAKwf,EAAiBD,EAAY,IAChD,IAASxsB,EAAI,EAAGA,EAAIusB,IAAgBvsB,EAClC6sB,EAAcR,EAAKd,WACnBsB,EAAcR,EAAKX,cAEhB,CAKL,IAAS1rB,EAAI,EAAGA,EAAIusB,EAAe,IAAKvsB,EACtC6sB,EAAcR,EAAKX,UACnBmB,EAAcR,EAAKd,WAErBsB,EAAcR,EAAKX,mBA7DdU,EAAa,EAAGA,EAAaD,IAAaC,IAA1CA,IAqEJN,oBAAA,SAAQ7e,EAAoB8f,GAA5B,WAICC,EAAgB/f,EAAK5M,OACrB4sB,EAAaF,EAAO1sB,OAC1B,GAAI4sB,EAAa,EAAG,CAClB,IAAIC,EAAYH,EAAO,GACvB,GAAkB,IAAdG,EACF,MAAM,IAAI/pB,MAAM,oCAAoC+pB,GAEtD,IAAK,IAAIvsB,EAAI,EAAGA,EAAIssB,IAActsB,EAAG,CACnC,IAAIwsB,EAAcJ,EAAOpsB,IAAMusB,EAE/B,KADAC,EAAcA,GAAgBJ,EAAOpsB,IAAMqsB,GAEzC,MAAM,IAAI7pB,MAAM,uBAAuB4pB,EAAOpsB,oBAC1CusB,OAAcF,OAEpBE,EAAYH,EAAOpsB,GAErB,GAAIusB,IAAcF,EAChB,MAAM,IAAI7pB,MAAM,gDACZ6pB,WAAsBE,GAI9B,IAAME,EAAgBH,EAAa,EAC7BI,EAAenjB,OAAK0Y,kBAAkB,QAASqK,GAErD,GAAsB,IAAlBD,GAAsC,IAAfC,EAAkB,CAC3C,IAAMK,EAAsB,IAAI9vB,MAAMwvB,GACtC,IAASrsB,EAAI,EAAGA,GAAKysB,IAAiBzsB,EACpC0sB,EAAa1sB,GAAK,EAEpB,MAAO,CAAC2sB,EAAOD,GAGjBA,EAAa,GAAK,iBACT1sB,GACP,IAAMN,EAAS0sB,EAAOpsB,GAAKosB,EAAOpsB,EAAI,GAClCwrB,EAAY,EAChBoB,EAAK/B,YAAY7nB,SAAQ,SAACooB,GACxBI,GAAa3oB,EAAKgqB,aAAantB,EAAQ0rB,MAErCwB,EAAK1B,eAAiBxrB,EAAS,GAAmB,IAAd8rB,IACtCA,EAAY,GAEdkB,EAAa1sB,GAAK0sB,EAAa1sB,EAAI,GAAKwrB,UAT1C,IAASxrB,EAAI,EAAGA,GAAKysB,IAAiBzsB,IAA7BA,GAYT,IAAM8sB,EAAuB,IAAIjwB,MAAM6vB,EAAaD,eAE3CzsB,GACP,IAAMsrB,EAAac,EAAOpsB,GACtB+sB,EAAiBL,EAAa1sB,GAalC,GAZAgtB,EAAKnC,YAAY7nB,SAAQ,SAACooB,GACxB,IAAM1rB,EAAS0sB,EAAOpsB,EAAI,GAAKosB,EAAOpsB,GAChCwrB,EAAY3oB,EAAKgqB,aAAantB,EAAQ0rB,GAC5CvoB,EAAKoqB,aACD3gB,EAAMgf,EAAYwB,EAAQC,EAAgBvB,EAAWJ,GACzD2B,GAAkBvB,KAOhBwB,EAAK9B,eAAiB6B,IAAmBL,EAAa1sB,GAAI,CAC5D,IAAMktB,EAAad,EAAOpsB,EAAI,GAAKosB,EAAOpsB,GAG1C,GAAmB,IAAfktB,mBAMJ,IAAM9B,EAAa8B,EAAa,EAAIF,EAAKhC,SAEzCgC,EAAKC,aACD3gB,EAAMgf,EAAYwB,EAAQC,EAFZ,EAEuC3B,YA5B7D,IAASprB,EAAI,EAAGA,EAAIysB,IAAiBzsB,IAA5BA,GA+BT,MAAO,CAAC8sB,EAAQJ,SC9Lb,IAAMS,GAAUpF,aACjBqC,EAAgBC,GAAmB,OAAAD,EAASC,CAAM,ICGxD,IAAM+C,GAAc,SAAClK,EAASzmB,GAC5B,IAAM4wB,EAAY5wB,EAAEkB,MAAQulB,EAAEvlB,MAC9B,OAAqB,IAAd0vB,EAAkBnK,EAAEtY,MAAQnO,EAAEmO,MAAQyiB,CAC/C,EAaA,SAASC,GAAOC,EAAeC,EAAWC,EAAUC,GAClD,iBADwCD,kBAAUC,EAAQH,EAAM7tB,OAAS,GAClEguB,EAAQD,GAAM,CAInB,GAAIC,EAAQD,EAAO,IAAK,CACtB,IAAMpuB,EAAIquB,EAAQD,EAAO,EACnBE,EAAIH,EAAIC,EAAO,EACf/lB,EAAIlD,KAAK0lB,IAAI7qB,GACbS,EAAI,GAAM0E,KAAKglB,IAAI,EAAI9hB,EAAI,GAC3BkmB,EAAK,GAAMppB,KAAKsS,KAAKpP,EAAI5H,GAAKT,EAAIS,GAAKT,GAAKmF,KAAKqpB,KAAKF,EAAItuB,EAAI,GAGpEiuB,GAAOC,EAAOC,EAFEhpB,KAAKC,IAAIgpB,EAAMjpB,KAAKolB,MAAM4D,EAAIG,EAAI7tB,EAAIT,EAAIuuB,IACzCppB,KAAK+lB,IAAImD,EAAOlpB,KAAKolB,MAAM4D,GAAKnuB,EAAIsuB,GAAK7tB,EAAIT,EAAIuuB,KAIpE,IAAMpvB,EAAI+uB,EAAMC,GACZxtB,EAAIytB,EACJzlB,EAAI0lB,EAOR,IALAnkB,OAAKukB,KAAKP,EAAOE,EAAMD,GAEnBJ,GAAYG,EAAMG,GAAQlvB,GAAK,GACjC+K,OAAKukB,KAAKP,EAAOE,EAAMC,GAElB1tB,EAAIgI,GAAG,CAIZ,IAHAuB,OAAKukB,KAAKP,EAAOvtB,EAAGgI,GACpBhI,IACAgI,IACOolB,GAAYG,EAAMvtB,GAAIxB,GAAK,GAChCwB,GAAQ,EAEV,KAAOotB,GAAYG,EAAMvlB,GAAIxJ,GAAK,GAChCwJ,GAAQ,EAGwB,IAAhColB,GAAYG,EAAME,GAAOjvB,GAC3B+K,OAAKukB,KAAKP,EAAOE,EAAMzlB,IAEvBA,GAAQ,EACRuB,OAAKukB,KAAKP,EAAOvlB,EAAG0lB,IAIlB1lB,GAAKwlB,IACPC,EAAOzlB,EAAI,GAETwlB,GAAKxlB,IACP0lB,EAAQ1lB,EAAI,GAGlB,KChEE+lB,MACAC,YCHEld,EAAoBnM,EAAiBspB,EACrCvoB,GACF,GAAc,UAAVA,EAEF,MAAO,CAACf,EAAO,QADM6H,WAAWE,KAAKoE,IAIvC,GAAc,SAAVpL,EAAkB,CAIpB,IAAMwoB,EAAO3kB,OAAK4kB,aAAa,CAAC,GAAIF,GAE9BzmB,8DAAC4mB,OAGP,MAAO,MAAc,OAAQA,GAE/B,MAAM,IAAI5rB,MAAM,iCAAiCyrB,SAAgBvoB,EACnE,EDfE2oB,MACAC,YEZErlB,EAAuD3B,EACvD5B,EAAiB6oB,GACnB,IAAMC,EAAUjlB,OAAK0Y,kBAAkBvc,EAAO6D,OAAK6L,cAAc9N,IAEjE,GAAIinB,GAA0B,WAAV7oB,EAAoB,CAEtC,IAAI+oB,EAAS,EACbxlB,EAAOjG,SAAQ,SAAAqU,GACb,IAAMpW,EAAOsI,OAAK6L,cAAciC,EAAM1S,OAErC6pB,EAAuB/sB,IAAI4V,EAAM3E,KAAoB+b,GACtDA,GAAUxtB,SAEP,CACL,IAAIytB,EAAY,EAEhBzlB,EAAOjG,SAAQ,SAAAqU,GAOb,IANA,IAAMsX,EAAwB,WAAVjpB,EAChBgE,eAAake,uBAAuBvQ,EAAM3E,MAC1C2E,EAAM3E,KAENkc,EAAO,EAEFC,EAAM,EAAGA,EAAMxX,EAAM1S,MAAM,KAAMkqB,EAExC,IADA,IAAMC,EAASD,EAAMvnB,EAAS,GAAKonB,EAC1BK,EAAM,EAAGA,EAAM1X,EAAM1S,MAAM,KAAMoqB,EACxCP,EAAQM,EAASC,GAAOJ,EAAYC,KAIxCF,GAAarX,EAAM1S,MAAM,MAI7B,OAAO6pB,CACT,EFtBEQ,MACAC,MACAC,MACAC,MACAC,YGjBEC,EAAyBC,EAA4B5pB,EACrD6pB,EAAmBC,EAAmBC,EAAmB5qB,EACzD6qB,EAAuBC,GAGzB,IAFA,IAAMC,EAASxtB,SAAO,CAACmtB,EAAWE,GAAY/pB,GAErC1F,EAAI,EAAGA,EAAIuvB,EAAWvvB,IAAK,CAGlC,IAFA,IAAM4K,EAAQ,GACVilB,EAAe,EACV7nB,EAAI,EAAGA,EAAIwnB,EAAWxnB,IAAK,CAClC,IAAMkF,EAAMmiB,EAAYrvB,EAAIwvB,EAAYxnB,GACxC6nB,GAAgB3iB,EAAMrI,EAAQmD,GAC9B4C,EAAMjL,KAAKuN,GAEb,GAAI2iB,EAAe,GAAKA,GAAgBF,EAAaF,EACnD,MAAM,IAAIjtB,MACN,oBAAoBoI,0BAA6B8kB,GAGvD,IAAK,IAAIlC,EAAI,EAAGA,EAAIiC,EAAWjC,IAC7BoC,EAAO9e,OAAO9Q,EAAIyvB,EAAYjC,GAC1B8B,EAAUztB,UAAVytB,IAAiBA,EAAU3G,WAAWkH,EAAeJ,EAAYjC,KAIzE,OAAOoC,CACT,EHPEE,YIlBEC,EAA0BC,EAC1BC,GAEF,IADA,IAAML,EAASxtB,SAAO6tB,EAAoBF,EAAKrqB,OACtC1F,EAAI,EAAGA,EAAI4vB,EAAO3uB,OAAQjB,EAAG,CACpC,IAEMkwB,EAFSN,EAAOjH,WAAW3oB,GAEIuG,QAC/B4pB,EAAWD,EAAY,GACvBE,EAAaF,EAAY,GACzBG,EAAeL,EAAWlH,WAAW,CAACqH,EAAUC,IACtDF,EAAY,GAAKF,EAAWlf,OAAOuf,GAEnC,IAAMC,EAAgBP,EAAKjH,WAAWoH,GAElC,GAAKI,GAAiBA,EAAgBP,EAAKjf,OAAOpR,SACpDkwB,EAAO9e,OAAO9Q,GAAK+vB,EAAKjf,OAAOwf,IAInC,OAAOV,CACT,EJDEW,MACAC,MACAC,MACAC,MACAC,MACAC,YKxBE5I,EAAmB6I,EAAoBvpB,EACvC5B,GAIF,IAHA,IAAMgN,EAAOnJ,OAAKiN,uBACd9Q,EAA0B6D,OAAK6L,cAAc9N,IAExCtH,EAAI,EAAGA,EAAI0S,EAAKhT,SAAUM,EAAG,CAGpC,IAFA,IAAMiV,EAASjV,EAAI6wB,EACfpsB,EAAMujB,EAAM/S,GACPjN,EAAI,EAAGA,EAAI6oB,IAAc7oB,EAAG,CACnC,IAAMrK,EAAQqqB,EAAM/S,EAASjN,IACzBuR,OAAOuX,MAAMnzB,IACbA,EAAQ8G,KACVA,EAAM9G,GAGV+U,EAAK1S,GAAKyE,EAEZ,OAAOiO,CACT,ELOEqe,MACAC,MACAC,MACAC,YMzBsBC,EAAmBC,EAAkBC,GAE3D,IAAMC,EACF/nB,OAAKgoB,mBAAmB,EAAsBF,GAClD,OAAO7G,GAAa,GAAI4G,EAAQE,EAAUH,EAAOE,EACnD,ENqBEG,MACAC,YO1BEL,EAAkBC,EAAkBF,EACpCO,GASF,IAPM,IAAAlqB,qDAACF,OAAUqqB,OAEXC,EAAWlK,aAAW2J,EAAQ,SAC9B7C,EAAUjlB,OAAKsoB,oBACDtoB,OAAK6L,cAAc9N,GAAWsqB,GAC5Cf,EAAatnB,OAAK6L,cAAcuc,GAE7B3xB,EAAI,EAAGA,EAAIwuB,EAAQ9uB,SAAUM,EAAG,CAGvC,IAFA,IAAMiV,EAASjV,EAAI6wB,EACfiB,EAAO,EACF9pB,EAAI,EAAGA,EAAI6oB,IAAc7oB,EAChC8pB,GAAQX,EAAMlc,EAASjN,GAEzBwmB,EAAQxuB,GAAK8xB,EAGf,MAAO,CAACtD,UAASlnB,WAAUsqB,WAC7B,EPOEG,YQ/BEC,EAAeC,EAAcr0B,EAC7B8H,GAKF,GAJsBssB,IAAUC,GACID,EAAQC,GAAQr0B,EAAO,GACvBq0B,EAAOD,GAASp0B,EAAO,EAIzD,OAAO2L,OAAKsoB,oBAAoB,EAAGnsB,GAGrC,IAAMwsB,EAAc1tB,KAAK2tB,IAAI3tB,KAAKuR,MAAMkc,EAAOD,GAASp0B,IAClDkT,EAASvH,OAAKsoB,oBAAoBK,EAAaxsB,GAEjDusB,EAAOD,GAAkB,IAATp0B,IAGlBA,GAAQ,GAGVkT,EAAO,GAAKkhB,EACZ,IAAK,IAAIhyB,EAAI,EAAGA,EAAI8Q,EAAOpR,OAAQM,IACjC8Q,EAAO9Q,GAAK8Q,EAAO9Q,EAAI,GAAKpC,EAE9B,OAAOkT,CACT,EROEshB,MACAC,YS1BEC,EAAmCC,EACnC5tB,EAAiB6tB,EAAoB/C,EAAmBgD,EACxDjD,EAAmB3qB,EAAmB6tB,EACtCC,GACF,IAAMC,EAAe,CAACJ,EAAa/C,EAAWA,GAExCJ,EAAciD,EAAQxhB,OACtB+hB,EAAcN,EAAQzhB,OAE5B,GAAmB,IAAf0hB,EACF,OAAOpwB,SAAOuC,EAAsB4tB,EAAQ7sB,OAG9C,IAAMkqB,EAASxtB,SAAOwwB,EAAcL,EAAQ7sB,OAChB,iBAAjBgtB,GAEwB,iBAAjBA,EADf9C,EAAO9e,OAAoB8Q,KAAK8Q,GAGA,kBAAjBA,GACf9C,EAAO9e,OAAsB8Q,MAAM8Q,GAGtC,IAAK,IAAI1yB,EAAI,EAAGA,EAAIyyB,EAAYzyB,IAAK,CAGnC,IAFA,IAAM4K,EAAQ,GACVilB,EAAe,EACV7nB,EAAI,EAAGA,EAAIwnB,EAAWxnB,IAAK,CAClC,IAAMkF,EAAMmiB,EAAYrvB,EAAIwvB,EAAYxnB,GACxC4C,EAAMjL,KAAKuN,GACX2iB,GAAgB3iB,EAAMrI,EAAQmD,GAGhC,GAAI6nB,EAAe,GAAKA,GAAgB2C,EAAa/C,EACnD,MAAM,IAAIjtB,MAAM,oBAAoBoI,0BAA6BjG,GAGnE,IAAK,IAAI6oB,EAAI,EAAGA,EAAIiC,EAAWjC,IACzBmF,EACD/C,EAAO9e,OAAsB+e,EAAeJ,EAAYjC,IACpDqF,EAA2B7yB,EAAIyvB,EAAYjC,GAEhDoC,EAAO9e,OAAO+e,EAAeJ,EAAYjC,GAAsB,IAAjB+E,EAAQ3qB,KAClDirB,EAAY,GACZA,EAAY7yB,EAAIyvB,EAAYjC,GAKtC,OAAOoC,CACT,ETrBEkD,YUhC4BpgB,GAE5B,IADA,IAAMqgB,EAAe,IAAIxmB,aAAamG,EAAKhT,QAClCM,EAAI,EAAGA,EAAI0S,EAAKhT,SAAUM,EACjC+yB,EAAa/yB,GAAKwE,KAAK2tB,IAAIzf,EAAK1S,IAElC,OAAO+yB,CACT,EV2BEC,YWhCEtgB,EAAqBugB,EAAiBhyB,EAAgB0D,EACtDe,GACF,IAAMwtB,EAAcC,aAAWC,iBAAiBzuB,EAAOsuB,EAAOhyB,GACxDvB,EAAS6J,OAAK6L,cAAcnU,GAC5BoyB,EAAW9pB,OAAKyB,eAAerG,GAErC,GAAIuuB,EAAa,CACf,IAAMI,EAAaH,aAAWI,kBAAkBN,EAAOI,GAEvD,MAAc,WAAV3tB,EACMgN,EAAsBnM,MAAM+sB,EAAYA,EAAa5zB,GAGvDgT,EAAoB8gB,SAASF,EAAYA,EAAa5zB,GAShE,IANA,IAAMivB,EAAwB,WAAVjpB,EAChBgE,eAAake,uBAAuBlV,GACpCA,EAEE+gB,EAAQrxB,SAAOuC,EAAOe,EAAOipB,GAC7BiB,EAASxtB,SAAOnB,EAAMyE,GACnB1F,EAAI,EAAGA,EAAI4vB,EAAO3uB,OAAQjB,EAAG,CACpC,IAAM0zB,EAAS9D,EAAOjH,WAAW3oB,GAC3B2zB,EAAQD,EAAO9uB,KAAI,SAACgvB,EAAa5rB,GAAM,OAAA4rB,EAAMX,EAAMjrB,MACzD4nB,EAAOnuB,UAAPmuB,KAAW6D,EAAM5xB,UAAN4xB,IAAaE,KAAWD,IAGrC,MAAc,WAAVhuB,EACKgE,eAAamqB,uBAAuBjE,EAAO9e,QAE7C8e,EAAO9e,MAChB,EXCEgjB,YYpCExsB,EAAoByoB,EAAuBlrB,EAC3CouB,GAGF,IAFA,IAAMrD,EAASxtB,SAAOkF,EAAUyoB,EAAKrqB,OAE5B1F,EAAI,EAAGA,EAAI4vB,EAAO3uB,KAAMjB,IAAK,CAIpC,IAHA,IAAM0oB,EAAMkH,EAAOjH,WAAW3oB,GAExB+zB,EAAmB,IAAIl3B,MAAM6rB,EAAIhpB,QAC9BsI,EAAI,EAAGA,EAAI+rB,EAAOr0B,OAAQsI,IACjC+rB,EAAO/rB,GAAK0gB,EAAI1gB,GAAKnD,EAAQmD,GAAKirB,EAAMjrB,GAE1C4nB,EAAOnuB,UAAPmuB,KAAWG,EAAKluB,UAALkuB,IAAYgE,KAAYrL,IAGrC,OAAOkH,CACT,EZsBEoE,YHgKE1nB,EAAoB2nB,EAAwBrJ,EAC5CC,EAAuBC,EAAiBC,EAAkBC,EAC1DC,GACF,OAAO,IAAIE,GACAP,EAAWC,EAAaC,EAASC,EAAUC,EAC3CC,GACNxgB,QAAQ6B,EAAM2nB,EACrB,EGtKEC,MACAC,YalCEpE,EACAqE,GAEF,IADA,IAAMlM,EAAqB,IAAIrrB,MAAMkzB,EAAKnoB,MACjC5H,EAAI,EAAGA,EAAIkoB,EAASxoB,OAAQM,IACnCkoB,EAASloB,GAAK+vB,EAAKprB,MAAM3E,GAAKo0B,EAAKp0B,GAErC,IAAMhC,EAASoE,SAAO8lB,EAAU6H,EAAKrqB,OACrC,IAAS1F,EAAI,EAAGA,EAAIhC,EAAO8S,OAAOpR,SAAUM,EAAG,CAI7C,IAHA,IAAM+zB,EAAS/1B,EAAO2qB,WAAW3oB,GAE3BkwB,EAAwB,IAAIrzB,MAAMkzB,EAAKnoB,MACpCI,EAAI,EAAGA,EAAIkoB,EAAYxwB,OAAQsI,IACtCkoB,EAAYloB,GAAK+rB,EAAO/rB,GAAK+nB,EAAKprB,MAAMqD,GAG1C,IAAMsoB,EAAgBP,EAAKjH,WAAWoH,GAEtClyB,EAAO8S,OAAO9Q,GAAK+vB,EAAKjf,OAAOwf,GAEjC,OAAOtyB,CACT,EbeEq2B,YDoCEnuB,EAAekrB,EAAkBC,EAAyB7D,EAC1D8G,GAQF,IALA,IAAMC,EAAUnD,EAAOA,EAAO1xB,OAAS,GACjC8H,sBAACgtB,OAAOvzB,OACRwzB,EAAclrB,OAAKiN,uBAAuB6a,EAAQmD,EAAQhH,GAC1DkH,EAAiBnrB,OAAKiN,uBAAuB,QAASge,EAAQhH,cAE3D/wB,GACP,IAAMwY,EAASxY,EAAIwE,EACbyR,EAAOxM,EAAEstB,SAASve,EAAQA,EAAShU,GAErC0zB,EAAoB,IAAI93B,MAAM6V,EAAKhT,QACvCgT,EAAK1P,SACD,SAACrF,EAAeiN,GAAkB,OAAA+pB,EAAU/pB,GAAS,CAACjN,QAAOiN,YAE7D4iB,EAAImH,EAAUj1B,SAChB4tB,GAAOqH,EAAWnH,GAClBmH,EAAYA,EAAUpuB,MAAM,EAAGinB,IAG7B8G,GACFK,EAAUC,KAAKxH,IAMjB,IAHA,IAAMyH,EAAYp4B,EAAI+wB,EAChBsH,EAAWL,EAAYjB,SAASqB,EAAWA,EAAYrH,GACvDuH,EAAcL,EAAelB,SAASqB,EAAWA,EAAYrH,GAC1DxtB,EAAI,EAAGA,EAAIwtB,EAAGxtB,IACrB80B,EAAS90B,GAAK20B,EAAU30B,GAAGrC,MAC3Bo3B,EAAY/0B,GAAK20B,EAAU30B,GAAG4K,OAtBzBnO,EAAI,EAAGA,EAAI+3B,EAAO/3B,MAAlBA,GA2BT,IAAM+O,EAAc4lB,EAAO7qB,QAG3B,OAFAiF,EAAYA,EAAY9L,OAAS,GAAK8tB,EAE/B,CACLprB,SAAOoJ,EAA4B6lB,EAAQoD,GAC3CryB,SAAOoJ,EAA4B,QAASkpB,GAEhD,EC9EEM,YcxCE7D,EAAmBC,EAAkB1rB,EAAiBuvB,EACtD/M,GASF,IARA,IAAMgN,EAAQ9D,EAAO1xB,OACfgjB,EAAQnZ,OAAK6L,cAAcgc,GAC3BiC,EAAW9pB,OAAKyB,eAAeomB,GAC/B+D,EAAa5rB,OAAKyB,eAAekd,GAEjClqB,EAASuL,OAAKiN,uBAChB9Q,EAA0B6D,OAAK6L,cAAc8S,IAExCloB,EAAI,EAAGA,EAAI0iB,IAAS1iB,EAAG,CAK9B,IAJA,IAAM0oB,EAAMnf,OAAKof,WAAW3oB,EAAGk1B,EAAO7B,GAGhCU,EAAmB,IAAIl3B,MAAM6rB,EAAIhpB,QAC9BiuB,EAAI,EAAGA,EAAIoG,EAAOr0B,OAAQiuB,IACjCoG,EAAOpG,GAAKjF,EAAIuM,EAAKtH,IAIvB3vB,EADiBuL,OAAKuf,WAAWiL,EAAQmB,EAAOC,IAC7BhE,EAAMnxB,GAE3B,OAAOhC,CACT,ECtBam0B,GACTzL,GAAgB,CAACC,OAAQhM,EAAY6B,IAAKoK,cAAekM,KAEhDsC,GAA0B,CACrC/S,WAAYgT,MACZ9S,YAAa,SACbC,WAAY2P,ICLDmD,GAAgBpO,GACzB,CAACP,OAAQ9M,EAAauB,IAAKwL,cAAe2O,GAAQpO,iBAAiB,IAE1DqO,GAA0B,CACrCnT,WAAYoT,MACZlT,YAAa,SACbC,WAAY8S,kBCCZ,WAAYhe,GAJZlY,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAc8L,EAAO,GAC1BlY,KAAK6G,cAAgBqR,EAAO1S,KAAI,SAAClG,EAAGsB,GAAM,MAAA,IAAIA,KAC9CZ,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAC5BngB,KAAKmY,UAAY,cAGnBme,wBAAA,WACE,IAAMC,EAAqB,GAE3Bv2B,KAAK6G,cAAcjD,SAAQ,SAAA4yB,GACzBD,EAASh2B,KAAK,QAAQi2B,WAAiBA,gCAGzC,IAAMC,EAAYz2B,KAAK6G,cACArB,KAAI,SAAAgxB,GACH,MAAO,IAAIA,KAEZ/vB,KAAK,OAc5B,MAZiB,WACbsb,EAAK,4CACiB/hB,KAAKmgB,mEACCngB,KAAKmgB,yIAG3BoW,EAAS9vB,KAAK,2DACcgwB,oDCpBnC,IAAMC,GAA2B,CACtCzT,WAAY0T,OACZxT,YAAa,SACbC,oBAnBmBX,GAEZ,IAAA5Y,WAAQ6Y,YAETkU,EAAU/sB,EAChB,GAAuB,IAAnB+sB,EAAQt2B,OACV,OAAOsmB,GAAS,CAAC/c,OAAQ,CAAC/C,EAAG8vB,EAAQ,IAAKlU,YAG5C,IAAMpc,EACFswB,EAAQpxB,KAAI,SAAApG,GAAK,OAAAA,EAAEkH,SAAOuwB,QAAO,SAACC,EAAIC,GAAO,OAAAzO,aAAWwO,EAAIC,MAC1D7e,EAAS0e,EAAQpxB,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAC5BI,EAAU,IAAI2wB,GAAkBpe,GACtC,OAAOwK,EAAQK,iBAAiBpd,EAASixB,EAAStwB,EACpD,iBCHE,WAAY0wB,EAAsBC,EAAcC,GAThDl3B,mBAA0C,CAAC,GAAI,EAAG,GAClDA,mBAAgB,CAAC,KACjBA,cAAW,uBAIXA,WAAO,EAIL,IAAMm3B,EAAO,CAACF,GAEdj3B,KAAKG,GAAoB,QAAf+2B,EAAuB,IAAM,IAGjC,IAAA9uB,qDAACgE,OAAammB,OAGpBvyB,KAAKoM,YAAqC,IAAvBA,EAAY9L,OAAe,CAAC,GAAK8L,EACpDpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAM1CjC,OAAK6L,cAAcuc,GAAe,IAClCpoB,OAAK6L,cAAc5J,GAAe,KACpCpM,KAAKoJ,KAAO,QACZpJ,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,iBAEhDlG,KAAKoJ,KAAO,SAGZpJ,KAAK+L,SACDI,EAAgBnM,KAAKmI,eAAgBnI,KAAKoM,YAAa,CAAC,EAAG,EAAG,KAGpEpM,KAAKg3B,WAAaA,EAClBh3B,KAAKmY,UAAY,aAAanY,KAAKG,OAAMH,KAAKoJ,YAGhDguB,wBAAA,WAAA,WACQC,EAAuB,WAC3B,OAA+B,IAA3B5zB,EAAKuzB,WAAW12B,OACX,kBAEA,mBAAmBmK,EAAahH,EAAKuzB,WAAW12B,OAAS,IAI9Dg3B,EAAoB,WACxB,IAAIxuB,EAAU,GACd,GAAgC,IAA5BrF,EAAK2I,YAAY9L,OACY,IAA3BmD,EAAKuzB,WAAW12B,SAClBwI,GAAW,sBAGb,IAAK,IAAIlI,EAAI,EAAGA,EAAI6C,EAAK2I,YAAY9L,OAAQM,IAC3CkI,GAAW,gBAAgB2B,EAAa7J,OAG5C,OAAOkI,GAGT,MAAkB,WAAd9I,KAAKoJ,KAKU,0GAJW,oDACepJ,KAAKkG,cAAc,wDACpBlG,KAAKkG,cAAc,4BAS3D6b,EAAK,mGAEgBsV,qUAOIC,0DACct3B,KAAKG,ilBAexBH,KAAKG,kcAiBV,WACf4hB,EAAK,4KAIoBuV,0CACFD,+FAEIC,uCACPt3B,KAAKG,qNC7H/B,WAAYqhB,EAAkB+V,GAR9Bv3B,mBAAgB,CAAC,KAMjBA,mBAA0C,CAAC,GAAI,GAAI,GAIjD,IADA,IAAMoM,EAAwB,IAAI3O,MAAM+jB,EAAOlhB,QACtCM,EAAI,EAAGA,EAAIwL,EAAY9L,OAAQM,IACtCwL,EAAYxL,GAAK4gB,EAAO+V,EAAO32B,IAEjCZ,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,IACnCa,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAAe,CAAC,EAAG,EAAG,IAEtElG,KAAKmY,UAAY,yBAGnBqf,wBAAA,WAyBE,MAxBiB,4BACIx3B,KAAKkG,cAAc,uDACGlG,KAAKkG,cAAc,GAAK,SAC/DlG,KAAKkG,cAAc,GAHN,g3BCZnB,WAAYsb,EAAkB+V,GAV9Bv3B,mBAAgB,CAAC,KAKjBA,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAIL,IADA,IAAMoM,EAAwB,IAAI3O,MAAM+jB,EAAOlhB,QACtCM,EAAI,EAAGA,EAAIwL,EAAY9L,OAAQM,IACtCwL,EAAYxL,GAAK4gB,EAAO+V,EAAO32B,IAEjCZ,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAE5BngB,KAAKu3B,OAASA,EACdv3B,KAAKmY,UAAY,aAAaof,SAGhCE,wBAAA,WACE,IAAMnxB,EAAQU,EAAkBhH,KAAKoM,YAAY9L,QAC3Co3B,EAmBV,SAA2BH,GACzB,IAAM/uB,EAAO+uB,EAAOj3B,OACpB,GAAIkI,EAAO,EACT,MAAMpF,MAAM,sBAAsBoF,2BAGpC,IADA,IAAMmvB,EAAiB,IAAIl6B,MAAM+K,GACxB5H,EAAI,EAAGA,EAAI22B,EAAOj3B,OAAQM,IACjC+2B,EAAeJ,EAAO32B,IAAM,SAAS6J,EAAa7J,GAGpD,OAAO+2B,EAAelxB,MACxB,CA9BqBmxB,CAAkB53B,KAAKu3B,QAexC,MAbiB,WACbxV,EAAK,2CACgB/hB,KAAKmgB,mEACEngB,KAAKmgB,uLAIjCngB,KAAKoM,YAAY9L,4BACTgG,MAASoxB,iFCjCT5W,GAAU2B,GAYxB,IAPO,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACA+uB,SACDpO,EAAgB/E,EAEhBoT,EAAQhvB,EAAEvB,MAAMjF,OAChBwoB,EAAqB,IAAIrrB,MAAMq4B,GAC5Bl1B,EAAI,EAAGA,EAAIkoB,EAASxoB,OAAQM,IACnCkoB,EAASloB,GAAKkG,EAAEvB,MAAMswB,EAAKj1B,IAE7B,GAAI8hB,EAAQiF,mBAAmB,CAAC7gB,IAAK,CACnC,IACM4K,EADQ+V,EAAc5Y,UAAUpM,IAAIqE,EAAEuJ,QACvBqB,OACfmW,EAAYgQ,GAAanmB,EAAQ5K,EAAEvB,MAAOuB,EAAER,MAAOuvB,EAAM/M,GAC/D,OAAOpG,EAAQ9O,eAAekV,EAAUhiB,EAAER,MAAOuhB,GAEnD,GAAuB,IAAnB/gB,EAAEvB,MAAMjF,QAAgB6J,OAAKC,YAAYyrB,EAAM,CAAC,EAAG,IAAK,CAC1D,IAAMiC,EAAU,IAAIN,GAAuB1wB,EAAEvB,MAAOswB,GACpD,OAAOpO,EAAc1E,iBAAiB+U,EAAS,CAAChxB,GAAIA,EAAER,OAExD,IAAMX,EAAU,IAAI8xB,GAAiB3wB,EAAEvB,MAAOswB,GAC9C,OAAOpO,EAAc1E,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MACxD,CAEO,IAAMyxB,GAAgC,CAC3C9U,WAAY+U,YACZ7U,YAAa,SACbC,WAAYtC,ICRP,IAAMmX,GAA6B,CACxChV,WAAYiV,SACZ/U,YAAa,SACbC,oBA3BEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAmwB,SAEHE,EAAOhtB,OAAKguB,eAAelB,EAAMnwB,EAAEvB,OACjC6yB,EAAe9tB,eAAa+tB,mBAAmBlB,EAAMrwB,EAAEvB,MAAMjF,QAC/Dg4B,EAAKxxB,EACHyxB,EAA0B,GACZ,MAAhBH,IACFE,EAAKxX,GAAU,CAACjX,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkT,KAAMuC,KACpDG,EAAwBh4B,KAAK+3B,GAC7BnB,EAAO7sB,eAAakuB,iBAAiBrB,EAAK72B,OAAQg4B,EAAG/yB,MAAMjF,SAG7DgK,eAAamuB,2BAA2B,SAAU,CAACtB,EAAK,IAAKmB,EAAG/yB,MAAMjF,QACtE,IAAMqF,EAAU,IAAIyxB,GAAiBkB,EAAG/yB,MAAO4xB,EAAK,GAAI,OAClDrU,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACiN,OAAOue,qBAC/C1T,EAAMtC,EAAQK,iBAAiBpd,EAAS,CAAC2yB,GAAK,QAASxV,GAE7D,OADAyV,EAAwB30B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WACpD2U,CACT,GCEO,IAAM2T,GAA6B,CACxC1V,WAAY2V,SACZzV,YAAa,SACbC,oBA3BEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAmwB,SAEHE,EAAOhtB,OAAKguB,eAAelB,EAAMnwB,EAAEvB,OACjC6yB,EAAe9tB,eAAa+tB,mBAAmBlB,EAAMrwB,EAAEvB,MAAMjF,QAC/Dg4B,EAAKxxB,EACHyxB,EAA0B,GACZ,MAAhBH,IACFE,EAAKxX,GAAU,CAACjX,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkT,KAAMuC,KACpDG,EAAwBh4B,KAAK+3B,GAC7BnB,EAAO7sB,eAAakuB,iBAAiBrB,EAAK72B,OAAQg4B,EAAG/yB,MAAMjF,SAG7DgK,eAAamuB,2BAA2B,SAAU,CAACtB,EAAK,IAAKmB,EAAG/yB,MAAMjF,QACtE,IAAMqF,EAAU,IAAIyxB,GAAiBkB,EAAG/yB,MAAO4xB,EAAK,GAAI,OAClDrU,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACiN,OAAO0e,qBAC/C7T,EAAMtC,EAAQK,iBAAiBpd,EAAS,CAAC2yB,GAAK,QAASxV,GAE7D,OADAyV,EAAwB30B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WACpD2U,CACT,GC1Ba8T,GAAQhR,GAAiB,CAACP,OAAQ9M,EAAawB,QAE/C8c,GAA4B,CACvC9V,WAAY+V,QACZ7V,YAAa,SACbC,WAAY0V,kBCSZ,WAAYG,EAAmCC,GAT/Cl5B,mBAAgB,CAAC,KACjBA,cACI,2GAGJA,mBAA0C,CAAC,IAAK,EAAG,GAEnDA,WAAO,EAGLA,KAAKoM,YAAc6sB,EAAS/wB,SAE5BlI,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAE9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,UAAU+gB,EAC3Bl5B,KAAKk5B,SAAWA,SAGlBC,wBAAA,WACE,IAAIC,EAAgB,yCACE,QAAlBp5B,KAAKk5B,WACPE,EAAgB,2DAGlB,IAAIC,EAAc,cAwClB,MAvCsB,QAAlBr5B,KAAKk5B,WACPG,EAAc,uBAGC,WACbtX,EAAK,sUASa,QAAlB/hB,KAAKk5B,SAAqB,MAAQ,olBAiB1BE,uEAIoBC,qDC1DlC,WAAYJ,GALZj5B,mBAAgB,CAAC,KACjBA,cAAW,sBACXA,mBAA0C,CAAC,IAAK,EAAG,GACnDA,WAAO,EAGLA,KAAKoM,YAAc6sB,EAAS/wB,SAC5BlI,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAE9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,qCAGnBmhB,wBAAA,WAiBE,MAhBiB,WACbvX,EAAK,qcCVX,WACIwX,EACArC,GATJl3B,mBAA0C,CAAC,GAAI,EAAG,GAClDA,mBAAgB,CAAC,KACjBA,cAAW,oBAGXA,WAAO,EAKLA,KAAKg3B,WAAa,CAACuC,EAAWC,UAAWD,EAAWE,QAC9C,IAACrtB,wEAEPpM,KAAKoM,YAAqC,IAAvBA,EAAY9L,OAAe,CAAC,GAAK8L,EAEpDpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAG9CpM,KAAK+L,SACDI,EAAgBnM,KAAKmI,eAAgBnI,KAAKoM,YAAa,CAAC,EAAG,EAAG,IAElEpM,KAAKk3B,WAAaA,EAClBl3B,KAAKmY,UAAY,UAAU+e,SAG7BwC,wBAAA,WACE,IAAIC,EAAW,GACXC,EAAY,MACQ,QAApB55B,KAAKk3B,YAA4C,QAApBl3B,KAAKk3B,YACpCyC,EAAW,gIAIa,QAApB35B,KAAKk3B,WAAuB,IAAM,2DAEtC0C,EAAY,kBACiB,QAApB55B,KAAKk3B,YAA4C,SAApBl3B,KAAKk3B,WAC3CyC,EAAW,uCACkB,SAApB35B,KAAKk3B,aACdyC,EAAW,uCACXC,EAAY,OAGd,IAAMC,EAAoC,SAApB75B,KAAKk3B,WAEvB,uEACA,4CAoDJ,MA9CiB,6GAJW,sDACmBl3B,KAAKkG,cAAc,6JAYlC,IAA5BlG,KAAKoM,YAAY9L,OACb,eACA,0FAGHyhB,EAAK,iJAGa6X,mTAMdD,4bAWCA,iOAQFE,gDC5FIhD,GACZ/vB,EAAemwB,EAAuB6C,EACtC5C,EAAyBxU,GAC3B,IAAMoT,EAAQhvB,EAAEvB,MAAMjF,OAChBy5B,EAAY,GAEZC,EAAW7vB,OAAKguB,eAAelB,EAAMnwB,EAAEvB,OACzC4xB,EAAO6C,EACL5B,EAAe9tB,eAAa+tB,mBAAmBlB,EAAMrB,GAEvD7d,EAAQnR,EACQ,MAAhBsxB,IACFngB,EAAQ6I,GAAU,CAACjX,OAAQ,CAAC/C,KAAI6b,MAAO,CAACkT,KAAMuC,GAAe1V,YAC7DyU,EAAO7sB,eAAakuB,iBAAiBrB,EAAK72B,OAAQw1B,GAClDiE,EAAUx5B,KAAK0X,IAGjB3N,eAAamuB,2BAA2BvB,EAAYC,EAAMrB,GAEpD,IAQFtsB,EAREpB,2DAAC6xB,OAAgB1H,OAEnB2H,EAAcD,EAOlB,GANIH,IAEFI,EAAc5vB,eAAa6vB,qBAAqBF,EAAgBD,IAI9C,QAAf9C,GAAuC,SAAfA,IACzBxU,EAAQiF,mBAAmB,CAAC1P,IAiBzB,CACL,IAAMwhB,EAAStvB,OAAK6L,cAAcuc,GAI5BgH,EAAa,CAACa,WAAYX,EAAQA,SAAQD,UAHlCrvB,OAAK6L,cAAciC,EAAM1S,OACbk0B,EAEiCY,QAAS,GAC9D/zB,EAAuB,SAAf4wB,EAAwB,UAAYoD,aAAWxzB,EAAER,OACzDwc,EAAc,CAClB,CAAC1Z,KAAM,QAAS8D,KAAM,CAACusB,KAEnB9zB,EAAU,IAAI+zB,GAAcH,EAAYrC,GACxCqD,EACF7X,EAAQK,iBAAiBpd,EAAS,CAACsS,GAAQ3R,EAAOwc,GACtDiX,EAAUx5B,KAAKg6B,GAEf/wB,EAAM6Z,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGyzB,GAAU5X,MAAO,CAACpd,MAAO20B,GAAcxX,gBAhC3B,CACvC,IAAMqP,EAAQrP,EAAQ7T,UAAUpM,IAAIwV,EAAM5H,QAAQqB,OAClD,OAAQwlB,GACN,IAAK,MACH,IAAMrP,EAAY2J,GACdO,EAAO5nB,OAAK6L,cAAcuc,GAAc2H,EAAapzB,EAAER,OAC3DkD,EAAMkZ,EAAQ9O,eAAesmB,EAAapzB,EAAER,MAAOuhB,GACnD,MACF,IAAK,OACG,IAAAxf,0BAAC+mB,YAASlnB,aAAUsqB,aAE1BhpB,EAAMkZ,EAAQ9O,eAAe1L,EAAUsqB,EAAUpD,GACjD,MACF,QACE,MAAM,IAAIhsB,MACH8zB,gDAsBb,OAFA6C,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEtC7G,CACT,UCzEgBnE,GACZod,GAEK,IAAA5Y,WAAQ6Y,YAASC,UAIxB,OAAOkU,qCAAsC,MAAOnU,EACtD,CAEO,IAAM8X,GAA0B,CACrCvX,WAAYwX,MACZtX,YAAa,SACbC,WAAY/d,aCbEq1B,GACZjY,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAgzB,aAEP,OAAOjD,GAAO/vB,SAASgzB,EAAU,OAAQpX,EAC3C,CAEO,IAAMiY,GAA2B,CACtC1X,WAAY2X,OACZzX,YAAa,SACbC,WAAYsX,aCPEG,GACZ/zB,EAAemyB,EAAmCC,EAClDxW,GACF,GAA6B,IAAzBuW,EAAS6B,aAA+C,IAA1B7B,EAAS8B,cACvC5wB,OAAKC,YAAY6uB,EAAS+B,QAAS/B,EAAS/wB,UAC9C,OAAO0e,GAAS,CAAC/c,OAAQ,CAAC/C,KAAI4b,YAGhC,GAAIuW,EAAS6B,cAAgB7B,EAASgC,SAClChC,EAAS8B,eAAiB9B,EAASiC,UAAmC,IAAvBjC,EAASO,WAC9B,UAA1BP,EAASkC,QAAQ/xB,KAAkB,CACrC,IAAM9I,EAASwG,EAAEvB,MAAMjF,OACjB86B,EAAW/X,GAAQ,CACvBxZ,OAAQ,CAAC/C,KACT4b,UACAC,MAAO,CACLpd,MAAO,CACLuB,EAAEvB,MAAMjF,EAAS,GAAKwG,EAAEvB,MAAMjF,EAAS,GACvCwG,EAAEvB,MAAMjF,EAAS,OAInB+6B,SACa,QAAbnC,EACFmC,EAAUX,GACN,CAAC7wB,OAAQ,CAAC/C,EAAGs0B,GAAW1Y,UAASC,MAAO,CAACsU,KAAM,EAAG6C,UAAU,MAEhE3vB,OAAK2B,OAAoB,QAAbotB,GAAoB,WAAM,MAAA,qBAAqBA,KAC3DmC,EAAUh2B,GAAI,CACZwE,OAAQ,CAAC/C,EAAGs0B,GACZ1Y,UACAC,MAAO,CAAC2Y,iBAAkB,EAAGxB,UAAU,MAI3C,IAAMl7B,EAASykB,GACX,CAACxZ,OAAQ,CAAC/C,EAAGu0B,GAAU3Y,UAASC,MAAO,CAACpd,MAAO0zB,EAAS/wB,YAG5D,OAFAwa,EAAQ7R,YAAYuqB,EAAS/qB,QAC7BqS,EAAQ7R,YAAYwqB,EAAQhrB,QACrBzR,EAGT,IAAI+G,EACEkD,EACF,CAAC,CAACO,KAAM,QAAS8D,KAAM,CAAC+rB,EAASsC,aAActC,EAASuC,eAsB5D,OArB8B,IAA1BvC,EAAS8B,cAA+C,IAAzB9B,EAAS6B,YAC1Cn1B,EAAU,IAAI2zB,GAAmCL,IAEhC,QAAbC,EACFvzB,EAAU,IAAIwzB,GAAcF,EAAU,QAEtC9uB,OAAK2B,OAAoB,QAAbotB,GAAoB,WAAM,MAAA,qBAAqBA,KAC3DvzB,EAAU,IAAIwzB,GAAcF,EAAU,QAGxCpwB,EAAWtI,KACP,CAAC6I,KAAM,QAAS8D,KAAM,CAAC+rB,EAASkC,QAAQM,IAAKxC,EAASkC,QAAQ9M,OAAQ,CACpEjlB,KAAM,QACN8D,KAAM,CAAC+rB,EAASyC,eAAgBzC,EAAS0C,gBAE3C,CAACvyB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASiC,SAAUjC,EAASgC,UAAW,CAC5D7xB,KAAM,QACN8D,KAAM,CAAC+rB,EAAS2C,sBAAuB3C,EAAS4C,yBAIjDnZ,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOuC,EACzD,CC5DO,IAAMizB,GAA8B,CACzC7Y,WAAY8Y,UACZ5Y,YAAa,SACbC,oBAhBEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAk1B,eAAYv2B,YAASw2B,QAAKC,oBAMjC,OAAOrB,GAAS/zB,EAJCwD,eAAa6xB,kBAC1Br1B,EAAEvB,MAA2Cy2B,EAAYv2B,EAF3C,EAGHw2B,EAAKC,GAES,MAAOxZ,EACtC,GCCO,IAAM0Z,GAAkC,CAC7CnZ,WAAYoZ,cACZlZ,YAAa,SACbC,oBAf0BX,GAKnB,IAAA5Y,WAAQ6Y,YAASC,UAIxB,OAAOkB,GAAgB,CAACC,MAAGzmB,MAAGqP,wBAAYiT,wBAAY+C,WACxD,iBCCE,WAAYkQ,EAAiB0J,GAZ7Bt8B,mBAAgB,CAAC,UAOjBA,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAGLA,KAAKoM,YAAckwB,EACnBt8B,KAAKwI,KAAO8zB,EAASh8B,OACrBN,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAE5BngB,KAAK4yB,MAAQA,EACb5yB,KAAKsH,SAAW,WAAWN,EAAkB4rB,EAAMtyB,aACnDN,KAAKmY,UAAY,eAGnBokB,wBAAA,WACE,IAEIC,EAFEl2B,EAAQU,EAAkBhH,KAAKwI,MAC/Bi0B,EA6BV,SAAmBj0B,GACjB,GAAa,IAATA,EACF,MAAO,YACF,GAAIA,GAAQ,EACjB,OAAOqD,GAAO1E,MAAM,EAAGqB,GAAMhD,KAAI,SAAAk3B,GAAS,MAAA,aAAaA,KAASj2B,KAAK,KAErE,MAAMrD,MAAM,oBAAoBoF,0BAEpC,CArCyBm0B,CAAU38B,KAAKwI,MAuBpC,OApBEg0B,EADwB,IAAtBx8B,KAAK4yB,MAAMtyB,OACFN,KAAKoM,YAAY5G,KAAI,SAAClG,EAAGsB,GAClC,MAAO,0CAGEZ,KAAKoM,YAAY5G,KAAI,SAAClG,EAAGsB,GAClC,MAAO,aAAaiL,GAAOjL,wBACvB6J,EAAa7J,gBAAeiL,GAAOjL,UAI1B,WACbmhB,EAAK,gFAEezb,qEAEhBk2B,EAAS/1B,KAAK,uDACoBg2B,wCAQxC5wB,GAAS,CAAC,IAAK,IAAK,IAAK,IAAK,IAAK,cCpDzB1E,GACZsb,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACA+sB,UAAOhyB,SAERuG,4CAACw0B,OAAQC,OAGf,GAFA9I,aAAW+I,kBAAkBh2B,EAAG81B,EAAQC,GAEpCna,EAAQiF,mBAAmB,CAAC7gB,KAAmB,WAAZA,EAAER,MAAoB,CAC3D,IAAMy2B,EAAcra,EAAQ7T,UAAUpM,IAAIqE,EAAEuJ,QACtCwX,EAAY+L,GACdmJ,EAAYrrB,OAAsBkrB,EAAQC,EAAO/1B,EAAEvB,MAAOuB,EAAER,OAChE,OAAOoc,EAAQ9O,eAAeipB,EAAO/1B,EAAER,MAAOuhB,GAGhD,GAAkC,IAA9B1d,OAAK6L,cAAc6mB,GACrB,OAAOna,EAAQ9O,eAAeipB,EAAO/1B,EAAER,MAAO,IAIhD,IAAMX,EAAU,IAAI42B,GAAaK,EAAQC,GACnC/Z,EAAc,CAAC,CAAC1Z,KAAM,QAAS8D,KAAM0vB,IAC3C,OAAOla,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,EACzD,CAEO,IAAMka,GAA4B,CACvC/Z,WAAYga,QACZ9Z,YAAa,SACbC,WAAYjc,ICsBD+1B,GAAqC,CAChDja,WAAYka,iBACZha,YAAa,SACbC,WArD4B,SAACX,GAKtB,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAs2B,eAAYC,UAEnBlzB,OAAK2B,OACDhF,EAAEvB,MAAMjF,QAAU,GAClB,WAAM,MAAA,2EAEV,IAAMg9B,EAAOF,EAAWvG,QAAO,SAAC/S,EAAGzmB,GAAM,OAAAymB,EAAIzmB,KAEvCkgC,EAAWjzB,eAAakzB,YAAY12B,EAAEvB,MAAO63B,EAAYE,GACzDG,EAAWnzB,eAAaozB,YAAYH,EAASj9B,OAAQ88B,EAAW98B,QAChEq9B,EACFrzB,eAAaszB,oBAAoB92B,EAAEvB,MAAO63B,EAAYE,GACpDO,EACFvzB,eAAawzB,oBAAoBT,EAAOD,EAAW98B,QACjD+vB,EACF/lB,eAAayzB,aAAaJ,EAAkBN,EAAOD,EAAW98B,QAE5Dy5B,EAAY,GAEZiE,EACF3a,GAAQ,CAACxZ,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACpd,MAAOg4B,KAC5CU,EAAyBnd,GAC3B,CAACjX,OAAQ,CAAC/C,EAAGk3B,GAAuBtb,UAASC,MAAO,CAACkT,KAAM4H,KACzDS,EAAwB7a,GAAQ,CACpCxZ,OAAQ,CAAC/C,EAAGm3B,GACZvb,UACAC,MAAO,CAACpd,MAAOo4B,KAEXQ,EAASh3B,GAAM,CACnB0C,OAAQ,CAAC/C,EAAGo3B,GACZxb,UACAC,MAAO,CAACkR,MAAOgK,EAAkBh8B,KAAMwuB,KASzC,OANA0J,EAAUx5B,KAAKy9B,GACfjE,EAAUx5B,KAAK09B,GACflE,EAAUx5B,KAAK29B,GAEfnE,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEtC8tB,CACT,GClDaC,GAAWtW,GAAiB,CACvCP,OAAQ9M,EAAaiC,UACrBpW,MAAO,OACPkhB,cAAe6W,KAGJC,GAA+B,CAC1Crb,WAAYsb,WACZpb,YAAa,SACbC,WAAYgb,aCVEttB,GAAK2R,GAEZ,IAAA5Y,WAAQ6Y,YACRzK,UAGP,OAAO2O,GAAS,CAAC/c,OAAQ,CAAC/C,EAFR4b,EAAQ7T,UAAUpM,IAAIwV,EAAM5H,QAEPO,mBAAmBE,MAAO4R,WACnE,CAEO,IAAM8b,GAA2B,CACtCvb,WAAYwb,OACZtb,YAAa,SACbC,WAAYtS,IC6DP,IAAM4tB,GAA2B,CACtCzb,WAAY0b,OACZxb,YAAa,SACbC,oBArEcwb,EACZnc,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAR,UAGP,GAAc,cAAVA,EAAuB,CACzB,GAAgB,cAAZQ,EAAER,MACJ,OAAOsgB,GAAS,CAAC/c,OAAQ,CAAC/C,KAAI4b,YAIhC,IAAMmc,EAAcC,EAAGC,MAAMj4B,EAAEvB,OACzBy5B,EAASJ,EAAK,CAAC/0B,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACrc,MAAO,aAEpD1H,EACFmoB,GAAQ,CAACld,OAAQ,CAACiH,KAAMkuB,EAAQjuB,KAAM8tB,GAAcnc,YAKxD,OAHAmc,EAAYtkB,UACZmI,EAAQ7R,YAAYmuB,EAAO3uB,QAEpBzR,EAIT,GAAgB,cAAZkI,EAAER,MAAuB,CAC3B,IAAM24B,EAAWnuB,GAAK,CAACjH,OAAQ,CAACoO,MAAOnR,GAAI4b,YACrC9jB,EAASggC,EAAK,CAAC/0B,OAAQ,CAAC/C,EAAGm4B,GAAWvc,UAASC,MAAO,CAACrc,WAE7D,OADAoc,EAAQ7R,YAAYouB,EAAS5uB,QACtBzR,EAGT,IAAKuL,OAAK+0B,gBAAgBp4B,EAAER,MAAOA,GAIjC,MAAO,CAAC+J,QADFzR,EAASgoB,GAAS,CAAC/c,OAAQ,CAAC/C,KAAI4b,aACfrS,OAAQ9K,MAAO3G,EAAO2G,MAAOe,SAGtD,GAAIoc,EAAQiF,mBAAmB,CAAC7gB,IAAK,CACnC,IAAM4K,EAASgR,EAAQ7T,UAAUpM,IAAIqE,EAAEuJ,QAAQqB,OACzCtJ,+BAAC+2B,OAAaC,OAAYpQ,OAEhC,OAAOtM,EAAQ9O,eAAeurB,EAAaC,EAAYpQ,GAGzD,GAAc,UAAV1oB,EACF,gBCxDgB2R,EAAmByK,GACrC,IAAM/c,EAAU,IAAI0hB,GAAepP,EAAM1S,MAAOgW,EAAYqD,QACtD/Y,EAAS6c,EAAQK,iBAAiBpd,EAAS,CAACsS,GAAQ,SAC1D,MAAO,CAAC5H,OAAQxK,EAAOwK,OAAQ9K,MAAOM,EAAON,MAAOe,MAAOT,EAAOS,MACpE,CDoDW+4B,CAAIv4B,EAAG4b,GAGhB,GAAc,SAAVpc,EAAkB,CACpB,IAAMg5B,EAAkB5c,EAAQ9O,eAC5B,GAAI,OAAQzJ,OAAKiN,uBAAuB,OAAQ,IAI9CxY,EAASw/B,GAAS,CAACv0B,OAFU,CAACia,EAAGhd,EAAGzJ,EAAGiiC,GAEE5c,YAE/C,OADAA,EAAQ7R,YAAYyuB,EAAgBjvB,QAC7BzR,EAGT,MAAM,IAAIwE,MAAM,iCAAiC0D,EAAER,aAAYA,EACjE,GEvEaqQ,GACT2Q,GAAgB,CAACC,OAAQhM,EAAYgC,KAAMiK,cAAeyH,KAEjDsQ,GAA2B,CACtCtc,WAAYuc,OACZrc,YAAa,SACbC,WAAYzM,kBCIZ,WAAYvK,GATZpM,mBAAgB,CAAC,KACjBA,cAAW,8BAGXA,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,aAAS,EACTA,WAAO,EAGLA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAC5BngB,KAAKmY,UAAY,kBAGnBsnB,wBAAA,WAkBE,MAjBiB,WACb1d,EAAK,mfCXX,WAAY3V,GATZpM,mBAAgB,CAAC,KACjBA,cAAW,8BAGXA,mBAA0C,CAAC,GAAI,EAAG,GAGlDA,WAAO,EAGLA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,cAGnBunB,wBAAA,WAaE,MAZiB,WACb3d,EAAK,sUCGN,IAAM4d,GAAkC,CAC7C1c,WAAY2c,cACZzc,YAAa,SACbC,oBAzB0BX,GAKnB,IAIH9c,EAJGkE,WAAQ6Y,YAASC,UACjB7b,MAIDgc,EAAc,CAClB,CAAC1Z,KAAM,UAAW8D,KAAM,kBACxB,CAAC9D,KAAM,UAAW8D,KAAM,mBAO1B,OAJEvH,EADEwE,OAAK6L,cAAclP,EAAEvB,OAAS,GAAM,EAC5B,IAAIk6B,GAAgB34B,EAAEvB,OAEtB,IAAIm6B,GAAY54B,EAAEvB,OAEvBmd,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,EACzD,iBCXE,WAAY5K,GANZlY,cAAW,GACXA,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAILA,KAAKoM,YACD9B,eAAau1B,gBAAgB3nB,EAAQ,GACzClY,KAAK6G,cAAgBqR,EAAO1S,KAAI,SAAClG,EAAGsB,GAAM,MAAA,IAAIA,KAC9CZ,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAE5BngB,KAAK8/B,aAAe5nB,EAAO5X,OAAS,EACpC,IAAK,IAAIM,EAAI,EAAGA,EAAIZ,KAAK8/B,aAAcl/B,IACrCZ,KAAKsH,UAAY,SAAS1G,YAE5BZ,KAAKmY,UAAY,gBAGnB4nB,wBAAA,WACE,IAAMxJ,EAAqB,GAC3B,GAAIv2B,KAAK8/B,aAAe,EAAG,CACzBvJ,EAASh2B,KACL,uFACJ,IAAK,IAAIK,EAAI,EAAGA,EAAIZ,KAAK8/B,aAAcl/B,IACrC21B,EAASh2B,KACL,gCAAgC,CAACK,GAAjC,gDAEIA,+BAA6BA,EAAI,YAE3C,IAAMo/B,EAAYhgC,KAAK8/B,aACjBG,EAAiBjgC,KAAK8/B,aAAe,EAC3CvJ,EAASh2B,KAAK,oDACVy/B,8BAAqCC,gBAEzC1J,EAASh2B,KAAK,yDAiBhB,MAdiB,WACbwhB,EAAK,2CACgB/hB,KAAKmgB,mEACEngB,KAAKmgB,0MAM3BoW,EAAS9vB,KAAK,uECvDZsK,GAAK0R,GAEZ,IAAA5Y,WAAQ6Y,YACRzK,UAGP,OAAO2O,GAAS,CAAC/c,OAAQ,CAAC/C,EAFR4b,EAAQ7T,UAAUpM,IAAIwV,EAAM5H,QAEPO,mBAAmBG,MAAO2R,WACnE,CAEO,IAAMwd,GAA2B,CACtCjd,WAAYkd,OACZhd,YAAa,SACbC,WAAYrS,aCNEqvB,GACZv2B,EAAsBotB,EAAcvU,WAChCpc,EAAQuD,EAAO,GAAGvD,MACxB,GAAc,cAAVA,EAAuB,CACzB,IAAM+5B,EAAQx2B,EAAOrE,KAAI,SAACpG,GAAM,OAAA0R,GAAK,CAACjH,OAAQ,CAACoO,MAAO7Y,GAAIsjB,eACpD4d,EAAQz2B,EAAOrE,KAAI,SAACpG,GAAM,OAAA2R,GAAK,CAAClH,OAAQ,CAACoO,MAAO7Y,GAAIsjB,eAEpD6d,EAAeH,GAAWC,EAAOpJ,EAAMvU,GACvC8d,EAAeJ,GAAWE,EAAOrJ,EAAMvU,GAEvC9jB,EACFmoB,GAAQ,CAACld,OAAQ,CAACiH,KAAMyvB,EAAcxvB,KAAMyvB,GAAe9d,YAO/D,OALA2d,EAAMz8B,SAAQ,SAAA9C,GAAK,OAAA4hB,EAAQ7R,YAAY/P,EAAEuP,WACzCiwB,EAAM18B,SAAQ,SAAAhD,GAAK,OAAA8hB,EAAQ7R,YAAYjQ,EAAEyP,WACzCqS,EAAQ7R,YAAY0vB,EAAalwB,QACjCqS,EAAQ7R,YAAY2vB,EAAanwB,QAE1BzR,EAGT,IAAI6hC,EAAW/d,EAAQiF,mBAAmB9d,GAY1C,GAJc,WAAVvD,IACFm6B,GAAW,GAGTA,EAAU,CAQZ,IAAMC,EAAY72B,EAAOrE,KAAI,SAAApG,GAC3B,IAAMuhC,EAAYx2B,OAAK6L,cAAc5W,EAAEmG,MAAM4B,MAAM8vB,IAEnD,OAAO5T,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG1H,GAAIsjB,UAASC,MAAO,CAACpd,MADnC,EAAE,EAAGo7B,SAIfC,EAAkBF,EAAUl7B,KAAI,SAAApG,GACpC,MAAO,CAACkU,KAAMoP,EAAQzO,SAAS7U,EAAEiR,QAAS9K,MAAOnG,EAAEmG,UAI/Cs7B,EACFv2B,eAAau1B,gBAAgBa,EAAUl7B,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAAQ,GACxD4pB,EAAyC,IAA1BuR,EAAU,GAAGn7B,MAAM,GAClC6pB,EACFF,GAAc0R,EAAiBC,EAAUv6B,EAAO6oB,GAE9C2R,EACFx2B,eAAau1B,gBAAgBh2B,EAAOrE,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAAQ0xB,GAErD8J,EAAUre,EAAQ9O,eAAektB,EAAex6B,EAAO8oB,GAI7D,OAFAsR,EAAU98B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEtC0wB,EAKT,IAAMC,EAActe,EAAQhhB,OAAO6V,OAAO0pB,gCAAkC,EAC5E,GAAIp3B,EAAOvJ,OAAS0gC,EAAa,CAE/B,IADA,IAAME,EAAgB,GACbtgC,EAAI,EAAGA,EAAIiJ,EAAOvJ,OAAQM,GAAKogC,EAAa,CACnD,IAAMG,EAAWt3B,EAAO1C,MAAMvG,EAAGA,EAAIogC,GACrCE,EAAc3gC,KAAK6/B,GAAWe,EAAUlK,EAAMvU,IAE1C9jB,EAASwhC,GAAWc,EAAejK,EAAMvU,OAE/C,IAAgB,IAAA0e,EAAAvb,EAAAqb,iCAAe,CAApBtgC,UACT8hB,EAAQ7R,YAAYjQ,EAAEyP,0GAGxB,OAAOzR,EAGH,IAAAyJ,EAyBR,SACIwB,EAAsBotB,EAAcvU,GACtC,IAAMxa,EAAWoC,eAAau1B,gBAAgBh2B,EAAOrE,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAAQ0xB,GAYxE,MAAO,CAACoK,UAXUx3B,EAAOrE,KAAI,SAAApG,GAAK,OAAAikB,GAAQ,CACXxZ,OAAQ,CAAC/C,EAAG1H,GACZsjB,UACAC,MAAO,CACLpd,MAAO,CACL4E,OAAK6L,cAAc5W,EAAEmG,MAAM4B,MAAM,EAAG8vB,IACpC9sB,OAAK6L,cAAc5W,EAAEmG,MAAM4B,MAAM8vB,WAKjD/uB,WACrB,SAxCSm5B,cAAWn5B,aACZgQ,EAAS,EAAY1S,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAChCI,EAAU,IAAIo6B,GAAc7nB,GAE5B4K,EAAqD,GACrDrM,EAAoB,IAAIhZ,MAAMya,EAAO5X,OAAS,GACpD,GAAImW,EAAQnW,OAAS,EAAG,CACtBmW,EAAQ,GAAKyB,EAAO,GAAG,GACvB4K,EAAYviB,KAAK,CAAC6I,KAAM,QAAS8D,KAAM,CAACuJ,EAAQ,MAChD,IAAS7V,EAAI,EAAGA,EAAI6V,EAAQnW,OAAQM,IAClC6V,EAAQ7V,GAAK6V,EAAQ7V,EAAI,GAAKsX,EAAOtX,GAAG,GACxCkiB,EAAYviB,KAAK,CAAC6I,KAAM,QAAS8D,KAAM,CAACuJ,EAAQ7V,MAIpD,IAAM4I,EAAMkZ,EAAQK,iBAChBpd,EAAS07B,EAAWA,EAAU,GAAG/6B,MAAOwc,GAC5Cue,EAAUz9B,SAAQ,SAAA9C,GAAK,OAAA4hB,EAAQ7R,YAAY/P,EAAEuP,WAE7C,IAAMixB,EACFje,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG0C,GAAMkZ,UAASC,MAAO,CAACpd,MAAO2C,KAEvD,OADAwa,EAAQ7R,YAAYrH,EAAI6G,QACjBixB,CACT,UCjHgBngC,GACZshB,GAEK,IAAA5Y,WAAQ6Y,YACRuU,eAEDsK,EAAQp3B,OAAKguB,eAAelB,EAAMptB,EAAO,GAAGtE,OAAO,GAEnD2S,EAASrO,EAAOrE,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SACjC+E,eAAak3B,uBAAuBtpB,EAAQqpB,GAE5C,IAAMr5B,EACFoC,eAAau1B,gBAAgBh2B,EAAOrE,KAAI,SAAApG,GAAK,OAAAA,EAAEmG,SAAQg8B,GAC3D,GAAqC,IAAjCp3B,OAAK6L,cAAc9N,GACrB,OAAOwa,EAAQ9O,eAAe1L,EAAU2B,EAAO,GAAGvD,MAAO,IAI3D,IAAMm7B,EAAU53B,EAAOiL,QAAO,SAAA1V,GAAK,OAAA+K,OAAK6L,cAAc5W,EAAEmG,OAAS,KACjE,OAAuB,IAAnBk8B,EAAQnhC,OACHsmB,GAAS,CAAC/c,OAAQ,CAAC/C,EAAG26B,EAAQ,IAAK/e,YAGrC0d,GAAWqB,EAASF,EAAO7e,EACpC,CAEO,IAAMgf,GAA6B,CACxCze,WAAY0e,SACZxe,YAAa,SACbC,WAAYjiB,ICsGd,kBAyBE,WACI83B,EAAmC1sB,EAAmBE,EACtDD,EAAkBoV,EAClB5C,EACAC,EAAmCkC,gBAFjBS,mBAClB5C,qBACAC,mBAAmCkC,MAxBvCnhB,mBAAgB,CAAC,IAAK,KAEtBA,cACI,uIAsBFA,KAAKoM,YAAc6sB,EAAS/wB,SAC5BlI,KAAK4hC,eAAyC,iBAAxB3I,EAAS4I,WAC/B7hC,KAAKuG,SACE0yB,EAAS6I,WAAa,GAAM,GAAK7I,EAAS6I,WAAa,GAAM,IAC9D9hC,KAAK4hC,gBACL3I,EAAS8I,SAAW,GAAM,IAAM/hC,KAAK4hC,iBACvC3I,EAAS+I,YAAc,GAAM,EACjChiC,KAAKmI,eAAiBnI,KAAK4hC,eAAiB,CAAC96B,EAAG,CAAC,GAAI3H,EAAG,CAAC,EAAG,GAAImJ,EAAG,CAAC,IACxB,CAACxB,EAAG,CAAC,EAAG,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,IACpEtI,KAAKkG,cAAgByG,EACjB3M,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKuG,QAChDvG,KAAKqM,kBAAoBS,EACrB9M,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKuG,QAEhDvG,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5ClG,KAAKqM,mBAELrM,KAAKuG,QACHvG,KAAK4hC,gBAAkB3I,EAAS6I,WAAa,GAAM,GACrD9hC,KAAK4gB,iBAAmB,EACxB5gB,KAAKgI,cAAgB,CAAC,MAAO,eAE7BhI,KAAK4gB,iBAAmB,EACxB5gB,KAAKgI,cAAgB,CAAC,YAAa,cAGjC4Z,IACF5hB,KAAK6G,cAActG,KAAK,QACxBP,KAAKgI,cAAczH,KAAK,cAGtB0e,IACFjf,KAAK6G,cAActG,KAAK,0BACxBP,KAAKgI,cAAczH,KAAK,gBAG1BP,KAAK4gB,iBAAmB5gB,KAAKqM,kBAAkB,GAC3CuV,GACF5hB,KAAK6G,cAActG,KAAK,QAGtB0e,GACFjf,KAAK6G,cAActG,KAAK,2BAI5BP,KAAKmhB,0BAA4BA,EACjCnhB,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAKif,0BAA4BA,EAEjCjf,KAAKwgB,WAAaxgB,KAAKkG,cAAc,GAAKlG,KAAKqM,kBAAkB,GACjErM,KAAKygB,WAAazgB,KAAKkG,cAAc,GAAKlG,KAAKqM,kBAAkB,GACjErM,KAAKogB,UAAYhb,KAAKC,IAClBrF,KAAKkG,cAAc,GAAKlG,KAAK4gB,iBAAkB5gB,KAAKkG,cAAc,IAEtElG,KAAK4f,UAAYrT,EAAYvM,KAAKwgB,YAAe,EACjDxgB,KAAK6f,UAAYpT,EAAYzM,KAAKygB,YAAe,EACjDzgB,KAAK8f,SAAWtT,EAAWxM,KAAKogB,WAAc,EAE9CpgB,KAAKmY,UAAY,YAAYnY,KAAKqM,sBAAqBrM,KAAKgf,gBACxDhf,KAAK4f,cAAa5f,KAAK6f,cAAa7f,KAAK8f,aAAY9f,KAAKuG,WAC1DvG,KAAK4gB,qBAAoB5gB,KAAK4hC,mBAC9B5hC,KAAKmhB,iCAGX8gB,wBAAA,WACE,IAAMC,EAAeliC,KAAKuG,OACtB2Z,EACIlgB,KAAKqM,kBAAmBrM,KAAKkG,eAAgBlG,KAAK4hC,eAClD5hC,KAAKogB,WACTc,GACIlhB,KAAKqM,kBAAmBrM,KAAKkG,eAAgBlG,KAAK4hC,eAClD5hC,KAAKogB,WAAW,EAAO,KAAMpgB,KAAKmhB,2BACpCghB,EACFniC,KAAKuG,OAAS,CAACvG,KAAK4gB,iBAAkB,EAAG,GAAK,CAAC,EAAG,EAAG,GASzD,MARiB,SA9OrB,SACIghB,EAAyBhiB,EAAoBC,EAC7CC,EAAmB8B,EACnB5C,EACAC,EAAmCmjB,EACnCC,EAAuBzhB,gBAHJgB,mBACnB5C,qBACAC,mBAAmCmjB,kBACnCC,kBAAuBzhB,KACzB,IAwBM0hB,EAAgBV,EAAiB,iEAGA,iEAIjCW,EAAkBX,EAAiB,2HAOA,2HAQnCY,EAASZ,EAAiB,qBAAuB,qBACjDa,EAASb,EAAiB,qBAAuB,qBACjDnS,EAAMmS,EAAiB,MAAQ,MAC/BjS,EAAMiS,EAAiB,MAAQ,MAC/Bc,EAAe,uEAGjBd,EAAiB,uBAAyB,iDAC3BnS,sCACAA,sCAEFE,iEACAA,4PAGDA,yCACI9Q,EAAYujB,8JAGFI,6BAAiCC,kBACvDH,mFAlEY,SAAC1hB,GACnB,OAAQA,GACN,KAAK,EACH,MAAO,uBACT,KAAK,EACH,MAAO,gEACT,KAAK,EACH,MAAO,2BACT,QACE,MAAM,IAAIxd,MACN,oBAAoBwd,yBA0DtB+hB,CAAYP,sCAIdQ,EAAUhB,EAAkBhiB,GAAaE,EAAW,6BAClCsiB,cAClBM,EACoD,6BAClCN,kFAEhBM,6BAEK7jB,EAAYujB,YACStiB,GAAYD,EAAY,6BAClCuiB,cAClBM,EACoD,6BAClCN,kFAEhBM,6BAEK7jB,EAAYujB,YAEnBS,EAAU,GA9EI,SAACjiB,GACnB,OAAQA,GACN,KAAK,EACH,MAAO,8CACT,KAAK,EACH,MAAO,kDACT,QACE,MAAM,IAAIxd,MACN,oBAAoBwd,yBAsEXkiB,CAAYT,GAEzBU,EAAUlkB,EAAY+B,GACtBoiB,EAAyBnkB,EAAjB+iB,EAA6BQ,EACAC,GACrCY,EAAyBpkB,EAAjB+iB,EAA6BS,EACAD,GAyB3C,MAxBiB,WAEbrjB,EACIC,EAAYC,EAAgD,IAArB2B,EAAwB,iEACdoiB,kBACjDpB,EAAiBgB,EAAUC,4EAGsBI,kBACjDrB,EAAiBiB,EAAUD,mFAG6BG,oCACtCniB,4IAKtBghB,EAAiB,uBAAyB,sCACtCW,eACAjjB,EAAsBsC,EAAS5C,wGAKzC,CA+GQkkB,CACIljC,KAAK4hC,eAAgB5hC,KAAK4f,UAAW5f,KAAK6f,UAAW7f,KAAK8f,SAC1D9f,KAAK4hB,QAAS5hB,KAAKgf,WAAYhf,KAAKif,0BACpCkjB,EAAa,GAAIA,EAAa,GAAIA,EAAa,aACrDD,6BCvOJ,WACIjJ,EAAmCrX,EACnC5C,EACAC,gBAFmC2C,mBACnC5C,qBACAC,MAZJjf,mBAAgB,CAAC,IAAK,KACtBA,cACI,iFACJA,mBAA0C,CAAC,EAAG,EAAG,GAU/CA,KAAKoM,YAAc6sB,EAAS/wB,SAC5BlI,KAAK4hC,eAAyC,iBAAxB3I,EAAS4I,WAC/B7hC,KAAKmI,eAAiBnI,KAAK4hC,eAAiB,CAAC96B,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,EAAG,IACxB,CAACxB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,EAAG,IACpEtI,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAKif,0BAA4BA,EAE7B2C,GACF5hB,KAAK6G,cAActG,KAAK,QAGtB0e,GACFjf,KAAK6G,cAActG,KAAK,0BAG1BP,KAAKmY,UAAY,eAAenY,KAAKgf,eAAchf,KAAK4hC,sBAG1DuB,wBAAA,WAwDE,MAvDiB,YAEbpkB,EACI/e,KAAKgf,WAAYhf,KAAKif,2BAA2B,EAAO,kvBAmB5Djf,KAAK4hC,eAAiB,oCACA,mJAGjBtiB,EAAsBtf,KAAK4hB,QAAS5hB,KAAKgf,4HAI7C+C,EAAK,sHAGc/hB,KAAK4hC,eAAiB,aAAe,0CACzC5hC,KAAK4hC,eAAiB,aAAe,0CACrC5hC,KAAK4hC,eAAiB,aAAe,ocAOrD5hC,KAAK4hC,eAAiB,sBACA,uEAEtB5hC,KAAK4hC,eAAiB,gDACA,yRCjE9B,SAASwB,GACL79B,EAAiBq8B,GACnB,IAAMthC,EAASiF,EAAMjF,OACrB,OAAIA,GAAU,IAGHiF,EAAM4B,MAAM,GAAI,GAFlBy6B,GAGDr8B,EAAMjF,EAAS,GAAKiF,EAAMjF,EAAS,GACnCiF,EAAMjF,EAAS,KAGoBiF,EAAMjF,EAAS,GAClDiF,EAAMjF,EAAS,GAAKiF,EAAMjF,EAAS,MAE/BshC,GAA6B,IAAXthC,GAAgBiF,EAAM,GAAK,EAChD,CAACA,EAAM,GAAI,GAEX,IAEX,UA4GgB89B,GAAWj7B,WAsCrBzC,EArCJmB,MACAgO,WACAmkB,aACAvW,YACAgG,SAAAjH,oBACAsC,2BAAArC,oBACAsC,mBAAAE,iBACAD,eAAAjF,oBAEMO,EAAkB,MAARkC,EACVxC,EAAsD,MAA1ByC,EAC5BkgB,EAAyC,iBAAxB3I,EAAS4I,WAC1ByB,EAAW1B,GACb3I,EAAS8B,eAAiB9B,EAASiC,UACnCjC,EAAS6B,cAAgB7B,EAASgC,SACR,UAA1BhC,EAASkC,QAAQ/xB,KACfm6B,EAAiBliC,QAAM8N,QAAQ,iCAErC,IAAKo0B,IACAD,GAC2B,IAA1BrK,EAAS8B,cAA+C,IAAzB9B,EAAS6B,aACZ,IAA5B7B,EAASyC,gBAAmD,IAA3BzC,EAAS0C,eAChB,IAA1B1C,EAASsC,cAA+C,IAAzBtC,EAASuC,cACb,SAA1BvC,EAASkC,QAAQ/xB,MACS,UAA1B6vB,EAASkC,QAAQ/xB,OACtB,OAjIJ,SAAwBhB,WAmBlBo7B,EACAC,EAsDIC,EAzER58B,MACAgO,WACAmkB,aACAvW,YACAgG,SAAAjH,oBACAsC,2BAAArC,oBACAsC,mBAAAE,iBACAD,eAAAjF,oBAEM4iB,EAAyC,iBAAxB3I,EAAS4I,WAC1Bn1B,GAAak1B,EAObvc,EAA8B,GAIpC,GARiBuc,GACb3I,EAAS8B,eAAiB9B,EAASiC,UACnCjC,EAAS6B,cAAgB7B,EAASgC,SACR,UAA1BhC,EAASkC,QAAQ/xB,KAKP,CACZ,IAAMu6B,EACF1K,EAASiC,SAAWjC,EAASgC,QAAUhC,EAAS6I,WACpD0B,EAAYngB,GAAQ,CAClBxZ,OAAQ,CAAC/C,KACT4b,UACAC,MAAO,CAACpd,MAAO,CAAC,EAAG0zB,EAASO,UAAWmK,MAEzCF,EAAiBpgB,GAAQ,CACvBxZ,OAAQ,CAAC/C,EAAGgO,GACZ4N,UACAC,MAAO,CAACpd,MAAO,CAAC,EAAGo+B,EAAW1K,EAAS+I,qBAGzCwB,EAAYngB,GAAQ,CAClBxZ,OAAQ,CAAC/C,KACT4b,UACAC,MAAO,CACLpd,MAAOq8B,EACH,CACE3I,EAASO,UAAWP,EAASiC,SAAWjC,EAASgC,QACjDhC,EAAS6I,YAEX,CACE7I,EAASO,UAAWP,EAAS6I,WAC7B7I,EAASiC,SAAWjC,EAASgC,YAIvCwI,EAAiBpgB,GAAQ,CACvBxZ,OAAQ,CAAC/C,EAAGgO,GACZ4N,UACAC,MAAO,CAACpd,MAAO,CAAC,EAAG0zB,EAAS6I,WAAY7I,EAAS+I,gBAGrD3c,EAAc9kB,KAAKijC,GACnBne,EAAc9kB,KAAKkjC,GAEW,MAA1B/hB,GAGiB,OAFbgiB,EACFN,GAAuB1hB,EAAuBnc,MAAOq8B,MAEvDlgB,EAAyB2B,GAAQ,CAC/BxZ,OAAQ,CAAC/C,EAAG4a,GACZgB,UACAC,MAAO,CAACpd,MAAOm+B,KAEjBre,EAAc9kB,KAAKmhB,IAIX,MAARD,GAEiB,OADbiiB,EAAcN,GAAuB3hB,EAAKlc,MAAOq8B,MAErDngB,EAAO4B,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG2a,GAAOiB,UAASC,MAAO,CAACpd,MAAOm+B,KAC3Dre,EAAc9kB,KAAKkhB,IAIvB,IAAM7iB,EAASilB,GAAgB,CAC7BC,EAAG8d,EAAiB4B,EAAYC,EAChCpmC,EAAGukC,EAAiB6B,EAAiBD,EACrC92B,aACAiT,YAzEiB,EA0EjB+C,UACAjB,OACAzC,aACA0C,yBACAwC,mBAEIc,EAAM3B,GACR,CAACxZ,OAAQ,CAAC/C,EAAGlI,GAAS8jB,UAASC,MAAO,CAACpd,MAAO0zB,EAAS/wB,YAC3Dmd,EAAc9kB,KAAK3B,OAEnB,IAAgB,IAAAgnB,EAAAC,EAAAR,iCAAe,CAA1B,IAAMzkB,UACT8hB,EAAQ7R,YAAYjQ,EAAEyP,0GAGxB,OAAO2U,CACT,CA4BW4e,CAAe,CACpB98B,IACAgO,SACAmkB,WACAvW,UACAjB,OACAzC,aACA0C,yBACAwC,mBAKJ,IAAMiX,EAAU,CAAClC,EAASkC,QAAQM,IAAKxC,EAASkC,QAAQ9M,MAClDxlB,EAAa,CACjB,CAACO,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS8B,aAAc9B,EAAS6B,cACvD,CAAC1xB,KAAM,QAAS8D,OAAUiuB,IAC1B,CAAC/xB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASsC,aAActC,EAASuC,cACvD,CAACpyB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASyC,eAAgBzC,EAAS0C,iBAE3D,GAAI4H,EACF59B,EAAU,IAAIw9B,GACVlK,EAAU1Z,EAASP,EAAYC,OAC9B,CACL,IAAM1S,EAAYq1B,EAAiB3I,EAAS4K,UAAY5K,EAAS8I,SAC9B9I,EAAS+I,YACtCv1B,EAAYm1B,EAAiB3I,EAAS+I,YACT/I,EAAS4K,UAAY5K,EAAS8I,SAC3Dv1B,EACFysB,EAAS8B,aAAe9B,EAAS6B,YAAc7B,EAAS6I,WAC5Dj5B,EAAWtI,KACP,CAAC6I,KAAM,QAAS8D,KAAM,CAACX,IAAa,CAACnD,KAAM,QAAS8D,KAAM,CAACT,IAC3D,CAACrD,KAAM,QAAS8D,KAAM,CAACV,KAG3B,IAAM2U,EAA4BuB,EAAQnhB,YAAYukB,UACtDngB,EAAU,IAAIs8B,GACVhJ,EAAU1sB,EAAWE,EAAWD,EAAU+S,EAASP,EACnDC,EAA2BkC,GAGjC,IAAMkE,EAA8B,GAC9Bye,EAAyB,CAACh9B,EAAGgO,GAC/ByK,IACGqiB,GAAwC,IAAtBngB,EAAKlc,MAAMjF,SAChCmhB,EAAO4B,GACH,CAACxZ,OAAQ,CAAC/C,EAAG2a,GAAOiB,UAASC,MAAO,CAACpd,MAAO,CAACkc,EAAKlc,MAAM,GAAI,EAAG,MACnE8f,EAAc9kB,KAAKkhB,IAErBqiB,EAASvjC,KAAKkhB,IAEZxC,IACG2iB,GAA0D,IAAxClgB,EAAuBnc,MAAMjF,SAClDohB,EAAyB2B,GAAQ,CAC/BxZ,OAAQ,CAAC/C,EAAG4a,GACZgB,UACAC,MAAO,CAACpd,MAAO,CAACmc,EAAuBnc,MAAM,GAAI,EAAG,MAEtD8f,EAAc9kB,KAAKmhB,IAErBoiB,EAASvjC,KAAKmhB,IAEG,cAAf1C,IACFnW,EAAWtI,KAAK,CAAC6I,KAAM,UAAW8D,KAAM,CAACgX,KACzCve,EAAQ2B,UAAY,iBAEtB,IAAM0d,EAAMtC,EAAQK,iBAAiBpd,EAASm+B,EAAUh9B,EAAER,MAAOuC,OACjE,IAAgB,IAAAmd,EAAAH,EAAAR,iCAAe,CAA1B,IAAMzkB,UACT8hB,EAAQ7R,YAAYjQ,EAAEyP,0GAExB,OAAO2U,CACT,CCxOO,IAAM+e,GAA6B,CACxC9gB,WAAY+gB,SACZ7gB,YAAa,SACbC,oBAfEX,GACK,IAAA5Y,WAAQ8Y,UAAOD,YACf5b,MAAGgO,WACHrP,YAASw2B,QAAK4F,eAAYoC,cAAW/H,oBACtCgI,EAAc55B,eAAa65B,wBAAwBtC,GAKzD,OAAOwB,GAAW,CAACv8B,IAAGgO,SAAQmkB,SAJb3uB,eAAa85B,kBAC1Bt9B,EAAEvB,MACFuP,EAAOvP,MAA2CE,EAASw+B,EAAWhI,EACtEC,GAAiB,EAAuBgI,GACJxhB,WAC1C,GC+EA,kBAaE,WAAYuW,GARZj5B,mBAAgB,CAAC,IAAK,KAEtBA,cACI,2IAMFA,KAAKoM,YAAc6sB,EAAS+B,QAE5B7wB,OAAK2B,OACuB,iBAAxBmtB,EAAS4I,YACT,WAAM,MAAA,iCACV7hC,KAAKuG,OACD0yB,EAAS6I,WAAa,GAAM,GAAK7I,EAAS+I,YAAc,GAAM,EAClEhiC,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,EAAG,GAAImJ,EAAG,CAAC,IAC9CtI,KAAKkG,cAAgByG,EACjB3M,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKuG,QAChDvG,KAAKqM,kBAAoBS,EACrB9M,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKuG,QAEhDvG,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5ClG,KAAKqM,mBAELrM,KAAKuG,SACPvG,KAAKgI,cAAgB,CAAC,YAAa,QAGrChI,KAAKmY,UACD,oBAAoBnY,KAAKuG,WAAUvG,KAAKqM,yBAG9Cg4B,wBAAA,WACE,IAAMnC,EAAeliC,KAAKuG,OACtB2Z,EAA2BlgB,KAAKqM,kBAAmBrM,KAAKkG,eACxDgb,GAAuBlhB,KAAKqM,kBAAmBrM,KAAKkG,eAKxD,MAJiB,SArIrB,SAAsC0a,gBAAAA,KACpC,IA2CMb,EAAU,kkBAbDlB,EAAY+B,sHAGZ/B,EAAY+B,sNAQvBA,EAEY,6BAGH/B,EAAY+B,YAwCzB,MAtCiB,0DAEb/B,EAAY+B,gCACMA,YAClBb,mEAIAlB,EAAY+B,gCACMA,ycAzDF,SAACA,GACnB,OAAQA,GACN,KAAK,EACH,MAAO,0DACT,KAAK,EACH,MAAO,ujBAUT,QACE,MAAM,IAAIxd,MACN,oBAAoBwd,yBAiDxBkiB,CAAYliB,0BAEP/B,EAAY+B,oFAInB/B,EAAY+B,iCACMA,oDAElBA,EAAmB,iRAQnBA,0BAIN,CA8CM0jB,CAA6BtkC,KAAKuG,OAAS,EAAI,YAC/C27B,+BC7HJ,WAAYjJ,GAXZj5B,mBAAgB,CAAC,KAAM,KACvBA,cACI,yFAKJA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAGLA,KAAKoM,YAAc6sB,EAAS+B,QAC5Bh7B,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAK4hC,eAAyC,iBAAxB3I,EAAS4I,WAC/B7hC,KAAKmY,UAAY,kBAAkBnY,KAAK4hC,sBAG1C2C,wBAAA,WACE,IAAMC,EAASxkC,KAAK4hC,eAAiB,EAAI,EACnC6C,EAASzkC,KAAK4hC,eAAiB,EAAI,EACnC8C,EAAa1kC,KAAK4hC,eAAiB,EAAI,EAC7C,MAAO,SACL7f,EAAK,4JAIe2iB,mDAEgBF,eAClCC,yoCA0BYzkC,KAAK4hC,wgBCJlB,ICxDK+C,GDwDCC,GAA0C,CACrD3hB,WAAY4hB,sBACZ1hB,YAAa,SACbC,oBAxDkCX,GAK3B,IA2BH9c,EA3BGkE,WAAQ6Y,YAASC,UACjBmiB,OAAIhwB,WACJkiB,eAAYvxB,YAASw2B,QAAK4F,eAAY3F,oBAEvCgI,EAAc55B,eAAa65B,wBAAwBtC,GACnD5I,EAAW3uB,eAAa85B,kBAC1BpN,EAAYliB,EAAOvP,MAA2CE,EAC9D,EAAmBw2B,EAAKC,GAAiB,EAAOgI,GAE9Cr7B,EAAa,CACjB,CAACO,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS8B,aAAc9B,EAAS6B,cACvD,CACE1xB,KAAM,QACN8D,KAAM,CACJ+rB,EAAS8B,aAAe,EAAI9B,EAASkC,QAAQM,IAC7CxC,EAAS6B,YAAc,EAAI7B,EAASkC,QAAQ9M,OAGhD,CAACjlB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASsC,aAActC,EAASuC,cACvD,CACEpyB,KAAM,QACN8D,KAAM,CACJ+rB,EAASO,UAAWP,EAAS4K,UAAW5K,EAAS8I,SACjD9I,EAAS+I,eAOf,GAAI3gC,QAAM8N,QAAQ,sCACd8pB,EAAS8B,cAAgB,GAAK9B,EAAS6B,aAAe,GAClD7B,EAAS+I,aAAe,IAA8B,IAAxB/I,EAAS6I,WAC7Cn8B,EAAU,IAAI4+B,GAAsBtL,OAC/B,CACLtzB,EAAU,IAAI0+B,GAAwBpL,GACtC,IAAM1sB,EAAY0sB,EAASiC,SAAWjC,EAASgC,QACzCxuB,EAAYwsB,EAAS6I,WACrBt1B,EACFysB,EAAS8B,aAAe9B,EAAS6B,YAAc7B,EAAS+I,YAC5Dn5B,EAAWtI,KACP,CAAC6I,KAAM,SAAU8D,KAAM,CAACX,IACxB,CAACnD,KAAM,SAAU8D,KAAM,CAACT,IACxB,CAACrD,KAAM,SAAU8D,KAAM,CAACV,KAE9B,OAAOkW,EAAQK,iBAAiBpd,EAAS,CAACm/B,EAAIhwB,GAAS,UAAWjM,EACpE,GEnDak8B,GAAMzd,GAAgB,CAACC,OAAQhM,EAAY8B,MAE3C2nB,GAA0B,CACrC/hB,WAAYgiB,MACZ9hB,YAAa,SACbC,WAAY2hB,ICLDG,GAAO5d,GAAgB,CAACC,OAAQhM,EAAY+B,OAE5C6nB,GAA2B,CACtCliB,WAAYmiB,OACZjiB,YAAa,SACbC,WAAY8hB,kBCKZ,WACIG,EAAkBC,EAA4BC,EAC9CC,GAVJxlC,mBAAgB,CAAC,QAAS,QAAS,UACnCA,cAAW,4BACXA,mBAA0C,CAAC,GAAI,EAAG,GAIlDA,WAAO,EAKC,IAACylC,YACPzlC,KAAKoM,YAAc,CAACq5B,EAAUF,EAAS,GAAIA,EAAS,GAAIF,GACxDrlC,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAK0lC,SAAsB,aAAXF,EAAwB,EAAI,EAC5CxlC,KAAK2lC,sBAAwB3lC,KAAKoM,YAAY,GAAK,EACnDpM,KAAK4lC,qBAAuB5lC,KAAKoM,YAAY,GAAK,EAClDpM,KAAKmY,UAAY,iBAAiBnY,KAAK0lC,aACnC1lC,KAAK2lC,0BAAyB3lC,KAAK4lC,4BAGzCC,wBAAA,WACQ,IAAAz9B,6EAAC09B,OAAkBC,OAGnB19B,6KAAC29B,OAAaC,OAAaC,OAW3Bxd,0KAACyd,OAAYC,OAAYC,OAwE/B,MAzDiB,SACftkB,EAAK,oIAGsBikB,uCACDG,8dAeHF,kCACDG,2BACPF,yCACcJ,6HAIdO,yCACcN,4KAKtB/lC,KAAK0lC,6qCC1ELY,GAAoC,CAC/CrjB,WAAYsjB,gBACZpjB,YAAa,SACbC,WAnB2B,SAACX,GAKrB,IAAA5Y,WAAQ6Y,YAASC,UACjB6jB,UAAOC,UAAOC,WACdnB,aAAUC,WAAQmB,uBAEnBhhC,EAAU,IAAIkgC,GAChBW,EAAMjhC,MAAM,GAAIkhC,EAAMlhC,MAA2BggC,EAAUC,GACzD1iB,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACy5B,KAC9C,OAAOjkB,EAAQK,iBACXpd,EAAS,CAAC6gC,EAAOC,EAAOC,GAAS,UAAW5jB,EAClD,IJhBA,SAAY6hB,GACVA,WACAA,SACD,CAHD,CAAYA,KAAAA,QAKZ,kBAcE,WACIxkC,EAAeoF,EAAiBqhC,EAAoBC,GAVxD7mC,mBAAgB,CAAC,KAGjBA,cAAW,eACXA,WAAO,EAQLA,KAAKkG,cAAgB,CADE,IACe,EAAG,GACzClG,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAK4mC,UAAYA,EACjB5mC,KAAK6mC,QAAUA,EACf7mC,KAAKG,GAAKA,EACVH,KAAKmY,UAAY,OAAOnY,KAAKG,OAAMH,KAAK4mC,cAAa5mC,KAAK6mC,eAG5DC,wBAAA,WACE,IAAMt+B,EAAOxI,KAAKoM,YAAY9L,OACxBymC,EAAU/mC,KAAKG,KAAOwkC,GAAUqC,KAAO,MAAQ,MAC/CC,EAAMjnC,KAAK4mC,UAAYG,EACA,QAAQpK,GAAUn0B,EAAM,SAAUxI,KAAKG,QAC9DG,EAASN,KAAKoM,YAAYpM,KAAKoM,YAAY9L,OAAS,GACtD4mC,EAAY,GACZC,EAAY,GAWhB,OAPInnC,KAAK4mC,WACPM,EAAYlnC,KAAK6mC,QAAU,WAAUvmC,EAAS,GAAM,WACpD6mC,EAAYnnC,KAAK6mC,QAAU,UAAY,YAEvCK,EAAYlnC,KAAK6mC,QAAU,gBAAgBvmC,EAAW,cACtD6mC,EAAannC,KAAK6mC,QAAU,aAAe,cAEtC,WACH9kB,EAAK,4HAIQqlB,GAAc5+B,EAAM,SAAUxI,KAAKG,6BACnC8mC,yEAENC,+BACQC,mBACVC,GAAc5+B,EAAM,SAAUxI,KAAKG,+BAC/BH,KAAKG,aAAYw8B,GAAUn0B,EAAM,SAAUxI,KAAKG,4FASjE,SAASw8B,GAAUn0B,EAAckB,EAAcvJ,GAC7C,GAAa,IAATqI,EACF,MAAO,GAAGkB,EACL,GAAa,IAATlB,EACT,OAAUkB,SAAWA,OAChB,GAAa,IAATlB,EACT,OAAUkB,SAAWA,SAAWA,OAC3B,GAAa,IAATlB,EACT,OAAUkB,SAAWA,SAAWA,SAAWA,OAE3C,MAAMtG,MAAM,cAAcjD,eAAeqI,0BAE7C,CAEA,SAAS4+B,GAAc5+B,EAAckB,EAAcvJ,GACjD,GAAa,IAATqI,EACF,MAAO,GAAGkB,EACL,GAAa,IAATlB,EACT,OAAUkB,OACL,GAAa,IAATlB,EACT,OAAUkB,OACL,GAAa,IAATlB,EACT,OAAUkB,OAEV,MAAMtG,MAAM,cAAcjD,eAAeqI,0BAE7C,UK5FgB6+B,GACZlnC,EAAe2G,EAAe4b,EAAwBuU,EACtD2P,EAAoBC,GACtB,IAAM/Q,EAAQhvB,EAAEvB,MAAMjF,OAChBgnC,EAAch9B,eAAa+tB,mBAAmB,CAACpB,GAAOnB,GACxDyR,EAAYzgC,EACG,MAAfwgC,IACFC,EAAYzmB,GAAU,CAACjX,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkT,KAAMyR,MAE7D,IAAME,EAAel9B,eAAakuB,iBAAiB,EAAG1C,GAAO,GAE7D,GAAI0R,IAAiB1R,EAAQ,EAC3B,MAAM,IAAI1yB,MACN,qDACI0D,EAAEvB,MAAMjF,OAAS,GADrB,iBAEgB22B,GAStB,IAPA,IAAMp1B,EAAO0lC,EAAUhiC,MAAMiiC,GACzB5oC,EAASgoB,GAAS,CAAC/c,OAAQ,CAAC/C,EAAGygC,GAAY7kB,YAMtC9hB,EAAI,EAAGA,GAAKwE,KAAKuR,KAAKvR,KAAKqiC,KAAK5lC,IAAS,EAAGjB,IAAK,CACxD,IAAM+E,EAAU,IAAImhC,GAAW3mC,EAAIonC,EAAUhiC,OAAO,EAAOshC,GACrDa,EAAa9oC,EACbkkB,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACtM,KAC9ChC,EACI8jB,EAAQK,iBAAiBpd,EAAS,CAAC/G,GAASA,EAAO0H,MAAOwc,GAC9DJ,EAAQ7R,YAAY62B,EAAWr3B,QAIjC,GAAIu2B,EAAW,CACPjhC,EAAU,IAAImhC,GAAW3mC,EAAIonC,EAAUhiC,MAAOqhC,EAAWC,GACzDa,EAAa9oC,EACbkkB,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAAC,KAC9CtO,EACI8jB,EAAQK,iBAAiBpd,EAAS,CAAC/G,GAASA,EAAO0H,MAAOwc,GAC9DJ,EAAQ7R,YAAY62B,EAAWr3B,QAGjC,GAAmB,MAAfi3B,EAAqB,CACvB,IACMK,EAA0B7mB,GAC5B,CAACjX,OAAQ,CAAC/C,EAAGlI,GAAS8jB,UAASC,MAAO,CAACkT,KAFhBvrB,eAAas9B,uBAAuBN,MAO/D,OAHA5kB,EAAQ7R,YAAYjS,EAAOyR,QAC3BqS,EAAQ7R,YAAY02B,EAAUl3B,QAEvBs3B,EAGT,OAAO/oC,CACT,CChDO,IAAMipC,GAA8B,CACzC5kB,WAAY6kB,UACZ3kB,YAAa,SACbC,oBAXEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAmwB,SAAM2P,cAAWC,YACxB,OAAOQ,GAAQ1C,GAAUqC,KAAMlgC,EAAG4b,EAASuU,EAAM2P,EAAWC,EAC9D,GCEO,IAAMkB,GAA6B,CACxC9kB,WAAY+kB,SACZ7kB,YAAa,SACbC,oBAXEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAmwB,SAAM2P,cAAWC,YACxB,OAAOQ,GAAQ1C,GAAUsD,IAAKnhC,EAAG4b,EAASuU,EAAM2P,EAAWC,EAC7D,iBCCE,WAAYz6B,EAAuBy1B,GAVnC7hC,mBAAgB,CAAC,KAMjBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EACPA,cAAW,mBAGTA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKmY,UAAY,gBAAgB0pB,EACjC7hC,KAAK6hC,WAAaA,SAGpBqG,wBAAA,WAsBE,MArBiB,WACbnmB,EAAK,yJAIO/hB,KAAKmoC,+CACLnoC,KAAKooC,8CACLpoC,KAAKqoC,iTAOXroC,KAAKsoC,qFAGGtoC,KAAKuoC,2FAOjBL,iCAAA,WACN,MAAwB,SAApBloC,KAAK6hC,WACA,YAEA,aAIHqG,gCAAA,WACN,MAAwB,SAApBloC,KAAK6hC,WACA,YAEA,aAIHqG,gCAAA,WACN,MAAwB,SAApBloC,KAAK6hC,WACA,YAEA,aAIHqG,+BAAA,WACN,MAAwB,SAApBloC,KAAK6hC,WACA,uBAEA,wBAIHqG,mCAAA,WACN,MAAwB,SAApBloC,KAAK6hC,WACA,4BAEA,kCCjDN,IAAM2G,GAAmC,CAC9CvlB,WAAYwlB,eACZtlB,YAAa,SACbC,oBAjC2BX,GAKpB,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACA4hC,cAAW7G,eAEZrI,EAAY1yB,EAAEvB,MAAM,GAKpBojC,GAJ8B,SAAf9G,EAAyB/6B,EAAEvB,MAAM,GAAKuB,EAAEvB,MAAM,IAIhCmjC,EAC7BE,GAJ6B,SAAf/G,EAAyB/6B,EAAEvB,MAAM,GAAKuB,EAAEvB,MAAM,IAIjCmjC,EAC3BG,GAJ6B,SAAfhH,EAAyB/6B,EAAEvB,MAAM,GAAKuB,EAAEvB,MAAM,KAIhCmjC,EAAYA,GAMxC5lB,EAAc,CAClB,CAAC1Z,KAAM,QAAS8D,KAAM,CAACw7B,KAGnB/iC,EAAU,IAAIuiC,GARgB,SAAfrG,EACjB,CAACrI,EAAWmP,EAAcC,EAAaC,GACvC,CAACrP,EAAWqP,EAAaF,EAAcC,GAMU/G,GACrD,OAAOnf,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,EACzD,iBCbE,WACI1W,EAAuB2uB,EAAsBD,EAC7ClZ,EAAiB5C,EACjB8pB,gBADAlnB,mBAAiB5C,qBACjB8pB,MAZJ9oC,mBAAgB,CAAC,IAAK,KACtBA,cAAW,uCACXA,mBAA0C,CAAC,GAAI,GAAI,GAWjDA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,EAAG,IAC9CtI,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAE5C0b,GACF5hB,KAAK6G,cAActG,KAAK,QAEtBuoC,GACF9oC,KAAK6G,cAActG,KAAK,0BAG1BP,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAK8oC,mBAAqBA,EAC1B9oC,KAAK+6B,aAAeA,EACpB/6B,KAAK86B,YAAcA,EACnB96B,KAAKmY,UAAY,iBAAiBnY,KAAKgf,eAAchf,KAAK+6B,iBACtD/6B,KAAK86B,mBAGXiO,wBAAA,WACE,IAAM/M,EAAah8B,KAAK86B,YAAc96B,KAAK+6B,aACrC70B,EACFlG,KAAKkG,cAAc,GAAKlG,KAAKkG,cAAc,GAAKlG,KAAKkG,cAAc,GACjE8iC,EAAchpC,KAAKkG,cAAc,GAAKlG,KAAK+6B,aAAe,EAC1Dra,EAAa1gB,KAAKkG,cAAc,GAAKlG,KAAK86B,YAAc,EA+E9D,MA7EiB,WACb/b,EAAoB/e,KAAKgf,WAAYhf,KAAK8oC,oBAAoB,EAAO,0DAE3BpoB,QAAgBsoB,yDAChBhpC,KAAK86B,kBAC/C96B,KAAK+6B,aALQ,2zCAuCbiO,6BAAsChpC,KAAKkG,cAAc,8DAEzDwa,6BAAqC1gB,KAAKkG,cAAc,yVAUxD81B,EAAa91B,EACT,gBAAgB81B,MAChB,kBAAkBA,yBAAiC91B,uDAG/BlG,KAAK86B,gDACL96B,KAAK86B,gLAOL96B,KAAK+6B,kEACH/6B,KAAK86B,sNAM7Bxb,EAAsBtf,KAAK4hB,QAAS5hB,KAAKgf,0MCvGjD,WACIia,EAAmCrX,EACnC5C,EAA4C8pB,gBADTlnB,mBACnC5C,qBAA4C8pB,MAZhD9oC,mBAAgB,CAAC,IAAK,KACtBA,cAAW,uCACXA,mBAA0C,CAAC,EAAG,EAAG,GACjDA,mBAAgB,EAKhBA,aAAS,EAKPA,KAAKoM,YAAc6sB,EAAS/wB,SAC5BlI,KAAKmI,eAAiB,CAACrB,EAAG,CAAC,GAAI3H,EAAG,CAAC,GAAImJ,EAAG,CAAC,EAAG,IAC9CtI,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAC,EAAGlG,KAAKmgB,cAAe,IAE5BhW,OAAK2B,OACuB,iBAAxBmtB,EAAS4I,YACT,WAAM,MAAA,iCAENjgB,GACF5hB,KAAK6G,cAActG,KAAK,QAEtBuoC,GACF9oC,KAAK6G,cAActG,KAAK,0BAG1BP,KAAKi5B,SAAWA,EAChBj5B,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAK8oC,mBAAqBA,EAE1B9oC,KAAKmY,UACD,iBAAiB6G,MAAchf,KAAKi5B,SAAS8B,iBACzC/6B,KAAKi5B,SAAS6B,gBAAe96B,KAAKi5B,SAASsC,iBAC3Cv7B,KAAKi5B,SAASuC,gBAAex7B,KAAKmgB,qBAG5C8oB,wBAAA,WACE,IAAMC,GAAWlpC,KAAKmgB,cAAgB,GAAKngB,KAAKi5B,SAASuC,YACrDx7B,KAAKi5B,SAAS6B,YAwDlB,MAtDiB,WACb/b,EAAoB/e,KAAKgf,WAAYhf,KAAK8oC,oBAAoB,EAAM,6SAS/C9oC,KAAKi5B,SAASsC,6CACfv7B,KAAKi5B,SAASuC,YAXrB,2TAgBex7B,KAAKmgB,4QAMF+oB,gDACElpC,KAAKmgB,iDAChBngB,KAAKmgB,uLAKHngB,KAAKi5B,SAAS8B,yJAGZmO,8HAGElpC,KAAKi5B,SAAS6B,oHAEd96B,KAAKmgB,qMAOXngB,KAAKmgB,wMAIrBb,EAAsBtf,KAAK4hB,QAAS5hB,KAAKgf,6JCjFrD,WACIia,EAAmCrX,EACnC5C,EAA4C8pB,gBADTlnB,mBACnC5C,qBAA4C8pB,MAbhD9oC,mBAAgB,CAAC,IAAK,KACtBA,cAAW,+HAGXA,mBAA0C,CAAC,IAAK,EAAG,GAUjDA,KAAKoM,YAAc6sB,EAAS/wB,SAC5BlI,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAK4hC,eAAyC,iBAAxB3I,EAAS4I,WAE3BjgB,GACF5hB,KAAK6G,cAActG,KAAK,QAEtBuoC,GACF9oC,KAAK6G,cAActG,KAAK,0BAG1BP,KAAKi5B,SAAWA,EAChBj5B,KAAK4hB,QAAUA,EACf5hB,KAAKgf,WAAaA,EAClBhf,KAAK8oC,mBAAqBA,EAC1B9oC,KAAKmY,UAAY,aAAanY,KAAKgf,eAAchf,KAAK4hC,sBAGxDuH,wBAAA,WACE,IAAMxG,EAAc3iC,KAAK4hC,eAAiB,2BACA,2BAsE1C,MApEiB,WACb7iB,EAAoB/e,KAAKgf,WAAYhf,KAAK8oC,oBAAoB,EAAO,gBAErE/mB,8HAIA/hB,KAAK4hC,eAAiB,KAAO,uEACX5hC,KAAK4hC,eAAiB,EAAI,2uCA4BvBe,yqBAoBAA,8JAMjBrjB,EAAsBtf,KAAK4hB,QAAS5hB,KAAKgf,4LCnD9C,IAAMoqB,GAA4C,CACvDnmB,WAAYomB,wBACZlmB,YAAa,SACbC,oBAxDoCX,GAK7B,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MAAGgO,WACHrP,YAASw2B,QAAK4F,eAAYoC,cAAW/H,oBACtCgI,EAAc55B,eAAa65B,wBAAwBtC,GACrDyH,EAAarF,EACC,MAAdqF,IACFA,EAAa,CAAC,EAAG,IAGnB,IAUI3jC,EAVEszB,EAAW3uB,eAAa85B,kBAC1Bt9B,EAAEvB,MACFuP,EAAOvP,MAA2CE,EAAS6jC,EAC3DrN,EAAKC,GAAiB,EAAsBgI,GAC1Cr7B,EAAa,CACjB,CAACO,KAAM,QAAS8D,KAAM,CAAC+rB,EAASkC,QAAQM,IAAKxC,EAASkC,QAAQ9M,OAC9D,CAACjlB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASiC,SAAUjC,EAASgC,WAG/C2G,EAAyC,iBAAxB3I,EAAS4I,WA2BhC,OAxBKD,GAAkB3I,EAASiC,SAAW,IAAMjC,EAASgC,QAAU,IACtC,IAA1BhC,EAASsC,cAA+C,IAAzBtC,EAASuC,aACb,IAA3BvC,EAAS0C,eAAmD,IAA5B1C,EAASyC,gBACzCzC,EAAS6I,aAAe7I,EAAS+I,YACnCr8B,EAAU,IAAIojC,GACV9P,EAAS/wB,SAAU+wB,EAAS8B,aAAc9B,EAAS6B,aAErD8G,GAAkB3I,EAASiC,SAAW,GAAKjC,EAASgC,QAAU,GAC9DhC,EAASuC,aAAe,GACxBvC,EAAS6I,aAAe7I,EAAS+I,aACL,IAA5B/I,EAASyC,gBAAmD,IAA3BzC,EAAS0C,eAC1C1C,EAAS6I,WAAa,GAAM,EAC9Bn8B,EAAU,IAAIsjC,GAA2BhQ,IAEzCtzB,EAAU,IAAIwjC,GAAuBlQ,GACrCpwB,EAAWtI,KACP,CAAC6I,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS8B,eAChC,CAAC3xB,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS6B,cAChC,CAAC1xB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASsC,aAActC,EAASuC,cAAe,CACpEpyB,KAAM,QACN8D,KAAM,CAAC+rB,EAASyC,eAAgBzC,EAAS0C,kBAI1CjZ,EAAQK,iBAAiBpd,EAAS,CAACmB,EAAGgO,GAAShO,EAAER,MAAOuC,EACjE,GCpDa0gC,GAAqBzhB,GAAiB,CACjDP,OAAQ9M,EAAasB,IACrByL,cAAegiB,GACfzhB,iBAAiB,IAGN0hB,GAA+B,CAC1CxmB,WAAYymB,WACZvmB,YAAa,SACbC,WAAYmmB,aCVEn0B,GACZqN,GAEK,IAAA5Y,WAAQ6Y,YAASC,UAIxB,OAAOkU,yBAA0B,MAAOnU,EAC1C,CAEO,IAAMinB,GAA0B,CACrC1mB,WAAYglB,MACZ9kB,YAAa,SACbC,WAAYhO,IC+DP,IAAMw0B,GAA6B,CACxC3mB,WAAY4mB,SACZ1mB,YAAa,SACbC,oBA1EEX,eAEK5Y,WAAQ6Y,YACRonB,mBACDlT,EAAU/sB,EAEV6e,kDAACqhB,YAASC,eAAYC,WAE5B3/B,eAAa4/B,oBAAoBH,EAAQzpC,OAAQ2pC,EAAQrT,GAOzD,IANM,IAAA7S,2CAAComB,SAAMC,UAEPC,EAASD,EAAM9pC,OACjB0kB,EAAuB,KACvBslB,EAAmBP,EAAQzpC,OACzBiqC,EAAiC,GAC9B3pC,EAAI,EAAGA,EAAIypC,IAAUzpC,EAAG,KAC/B,IAAqB,IAAAojB,YAAA6B,EAAAukB,EAAMxpC,mCAAI,CAA1B,IAAM4pC,UACHrmB,8CAAC0R,uBAA0B4U,eAE7B3jC,SACAwD,eAAaogC,sBAAsB7U,GACrC/uB,EAAI8vB,EAAQ4T,IAEZ1jC,EAAIga,GAAU,CAACjX,OAAQ,CAAC/C,EAAG8vB,EAAQ4T,IAAU9nB,UAASC,MAAO,CAACkT,UAC9D0U,EAAiBhqC,KAAKuG,IAGxB,IADA,IAAM48B,EAAwB58B,EAAEvB,MAAM4B,QAC7BinB,EAAI,EAAGA,EAAIqc,EAAanqC,SAAU8tB,EACzCsV,EAAYrgC,OAAOonC,EAAarc,GAAI,EAAG,GAGpCjkB,OAAKC,YAAYtD,EAAEvB,MAAOm+B,KAC7B58B,EAAIuc,GAAQ,CAACxZ,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACpd,MAAOm+B,KAClD6G,EAAiBhqC,KAAKuG,IAEZ,OAARke,EACFA,EAAMle,GAGNke,EACIukB,GAAmB,CAAC1/B,OAAQ,CAACia,EAAGhd,EAAGzJ,EAAG2nB,GAAMtC,YAChD6nB,EAAiBhqC,KAAKykB,sGAGtBpkB,EAAIypC,EAAS,IACXF,EAAKvpC,IAAM,IACbokB,EAAM5P,GAAI,CACRvL,OAAQ,CAAC/C,EAAGke,GACZtC,UACAC,MAAO,CACLsU,KAAMkT,EAAKvpC,IAAMmpC,EAAQzpC,OAASgqC,GAClCxQ,UAAU,KAGdyQ,EAAiBhqC,KAAKykB,IAExBslB,SAKJ,IAAyB,IAAAK,EAAA9kB,EAAA0kB,iCAAkB,CAAtC,IAAM52B,UACLA,IAAeqR,GAGnBtC,EAAQ7R,YAAY8C,EAAWtD,0GAGjC,OAAO2U,CACT,GC3Ea4lB,GAAMtjB,GAAgB,CAACC,OAAQhM,EAAYiC,MAE3CqtB,GAA0B,CACrC5nB,WAAY6nB,MACZ3nB,YAAa,SACbC,WAAYwnB,ICHDG,GAAQjjB,GACjB,CAACP,OAAQ9M,EAAa2B,MAAO9V,MAAO,OAAQkhB,cAAewjB,KAElDC,GAA4B,CACvChoB,WAAYioB,QACZ/nB,YAAa,SACbC,WAAY2nB,ICPD3gB,GAAM9C,GAAgB,CACjCC,OAAQhM,EAAYkC,IACpB+J,cAAeqI,GACfvpB,MAAO,YAGI6kC,GAA0B,CACrCloB,WAAYmoB,MACZjoB,YAAa,SACbC,WAAYgH,aCTEihB,GAAW5oB,GAKlB,IAAA5Y,WAAQ8Y,UAAOD,YACf5U,QACAmK,UAEDqzB,EAAYrzB,EAAM1S,MAAMjF,OACxBwoB,EAAW7Q,EAAM1S,MAAM4B,QACzBokC,EAAOz9B,EAWX,OAVIA,EAAM,IAER3D,OAAK2B,SACCw/B,EAAY,IAAMx9B,GACpB,WAAM,MAAA,mCAAoCw9B,EAAY,QAClDA,SACRC,EAAOD,EAAYx9B,EAAM,GAE3Bgb,EAASzlB,OAAOkoC,EAAM,EAAG,GAElBloB,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGmR,GAAQyK,UAASC,MAAO,CAACpd,MAAOujB,IAC9D,CAEO,ICjBH0iB,GDiBSC,GAAiC,CAC5CxoB,WAAYyoB,aACZvoB,YAAa,SACbC,WAAYioB,IE5BD/gB,GACThD,GAAgB,CAACC,OAAQhM,EAAYmC,MAAO8J,cAAesI,KAElD6b,GAA4B,CACvC1oB,WAAY2oB,QACZzoB,YAAa,SACbC,WAAYkH,kBCCZ,WAAYuhB,GARZ7rC,iBAAwB,GAIxBA,mBAAgB,CAAC,KACjBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAcy/B,EACnB7rC,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKmY,UAAY,uBAGnB2zB,wBAAA,WAWE,MAViB,WACb/pB,EAAK,gUChBAgqB,GAAoC,CAC7C9oB,WAAY+oB,gBACZ7oB,YAAa,SACbC,WAAY,SAAChb,OAACyB,WAAQ6Y,YACb8jB,UACD/e,EAAgB/E,EAEhB/c,EAAU,IAAImmC,GAAsBtF,EAAmBjhC,OAG7D,OADIkiB,EAAc1E,iBAAiBpd,EAAS,CAAC6gC,GAAQA,EAAMlgC,SCTpDkkB,GACTlD,GAAgB,CAACC,OAAQhM,EAAYoC,MAAO6J,cAAeuI,KAElDkc,GAA4B,CACvChpB,WAAYipB,QACZ/oB,YAAa,SACbC,WAAYoH,ICPD2hB,GACTrkB,GAAiB,CAACP,OAAQ9M,EAAamC,QAAStW,MAAO,UAE9C8lC,GAA+B,CAC1CnpB,WAAYopB,WACZlpB,YAAa,SACbC,WAAY+oB,kBCGZ,WAAY//B,EAAuBkgC,EAAqBC,gBAAAA,MARxDvsC,mBAAe,EACfA,iBAAwB,CAAC,GAGzBA,mBAA0B,GAC1BA,mBACI,CAAC,IAAK,EAAG,GAGXA,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAComC,EAAa,EAAG,IAErBtsC,KAAKusC,YAAcA,EACnBvsC,KAAKmY,UAAY,cAAcnY,KAAKusC,mBAGtCC,wBAAA,WACE,IAAMC,EAAczsC,KAAKusC,YACrB,0CACA,4CAGJ,MAAO,2CADHvsC,KAAKusC,YAAc,mBAAqB,+BAGxCxqB,EAAK,+LAIY0qB,uLN9BZC,GAAiC,CAC5CzpB,WAAY0pB,aACZxpB,YAAa,SACbC,oBAOyBX,GAKlB,IAAA5Y,WAAQ6Y,YAASC,UACnBiqB,WACEN,gBAEP,GAAc,MAAVM,EACF,MAAM,IAAIxpC,MAAM,4DAGlB,IAAMypC,EAAwC,sCAC1CD,aAAkBE,iBAChBC,EAAwC,sCAC1CH,aAAkBI,iBAChBC,EAA2C,uCAC/BL,aAAkBM,mBACF,qCAC7BN,aAAkBO,gBACjBC,EACuB,iCAAeR,aAAkBS,YAExDjlC,yDAACpE,OAAOC,OAMRmI,EAAc,CAACnI,EAAQD,EAAOsoC,GAM9BgB,EAAiBT,GAAWE,EAClC,GAAIK,GAAiBH,GAAYK,EAAgB,CAC/C,IAAIh8B,EAmBF,GAAIg8B,EAAgB,CAClB,IAAMC,EACFlsC,QAAM8N,QAAQ,yCACS,MAAvBq8B,IACA+B,IAA0BC,KAC5BA,GAAqBD,EACrB/B,GACIn8B,SAASC,cAAc,UAAUE,WAC7B,KAAM,CAACg+B,yBAEjBhC,GAAoBiC,OAAOzpC,MAAQA,EACnCwnC,GAAoBiC,OAAOxpC,OAASA,EACpCunC,GAAoBkC,UAChBd,EAA+C,EAAG,EAAG5oC,EAAOC,GAChE2oC,EAASpB,GAAoBiC,OAG/B,IAAM3rC,EAAQ6rC,gBAAgBv9B,SAC1Bu9B,gBAAgBC,kBAAoBD,gBAAgBE,gBAClD3pC,EAAS,aACTW,EAAU6d,EAAQ9T,eAAek/B,eACnC1hC,EAAY,GAAIA,EAAY,GAAIlI,EAAQpC,GAC5C4gB,EAAQpU,MAAMy/B,2BACV,CAACjoC,OAAQ8mC,GAA4C,CAAC/nC,WACtD,CAACuH,EAAY,GAAIA,EAAY,KACjCkF,EAAc,CAACtN,QAAOC,SAAQC,SAAQpC,QAAO+C,WAG/C,IAAMhD,EAAOsI,OAAK6L,cAAc5J,GAC1B3G,EAAU0E,OAAKyB,eAAeQ,GAC9BzG,EACF,IAAI6mC,GAAkBpgC,EAAakgC,GArDrC,GAuDIxpB,EAAc,CAClB,CAAC1Z,KAAM,SAAU8D,KAAM,CAACrL,IAAQ,CAACuH,KAAM,SAAU8D,KAAM,CAACo/B,IACxD,CAACljC,KAAM,SAAU8D,OAAUzH,KAEvBwS,EAAQyK,EAAQ9O,eAAe,CAAC3P,EAAQD,GAAQ,SACzC0e,EAAQ7T,UAAUpM,IAAIwV,EAAM5H,QACpCgB,aAAeC,EAEpB,IAAM1S,EACF8jB,EAAQK,iBAAiBpd,EAAS,CAACsS,GAAQ,QAAS6K,GAExD,OADAJ,EAAQ7R,YAAYoH,EAAM5H,QACnBzR,EAKT,IAAMovC,EAAapB,EAA8C1/B,KAC7D+gC,EAAaD,EACjB,GAAmB,MAAf1B,GAAuC,IAAhBA,EAAmB,CAC5C2B,EAAa,IAAI5gC,WAAWu/B,EAAO5oC,MAAQ4oC,EAAO3oC,OAASqoC,GAI3D,IAFA,IAAMxe,EAAakgB,EAAU1tC,OACzBsI,EAAI,EACChI,EAAI,EAAGA,EAAIktB,EAAYltB,IAC1BA,EAAI,EAAI0rC,IACV2B,EAAWrlC,KAAOolC,EAAUptC,IAKlC,IAAMiF,EACF6c,EAAQ9O,eAAexH,EAAa,QAAS,IAAIgB,WAAW6gC,IAEhE,OADAvrB,EAAQrL,YAAYxR,EAAOwK,QACpBxK,CACT,GA/HI2nC,GAAqBnsC,QAAM8N,QAAQ,yCOVvC,kBAcE,WACI6iB,EAAkBkc,EAAqBC,EACvCC,EAA4BC,GAVhCruC,cAAW,yBAEXA,mBAA0C,CAAC,IAAK,EAAG,GAInDA,WAAO,EAKLA,KAAK6G,cAAgB,CAAC,IAAK,OAAQ,YACnCyD,eAAaya,2BAA2BiN,EAAQkc,GAChD5jC,eAAaya,2BAA2BiN,EAAQmc,GAChDnuC,KAAKoM,YAAc4lB,EACnBhyB,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAE7B,MAAfkoC,IACF9jC,eAAaya,2BAA2BiN,EAAQoc,GAChDpuC,KAAK6G,cAActG,KAAK,WAER,MAAd8tC,IACF/jC,eAAaya,2BAA2BiN,EAAQqc,GAChDruC,KAAK6G,cAActG,KAAK,UAE1BP,KAAKouC,YAAcA,EACnBpuC,KAAKquC,WAAaA,EAClBruC,KAAKmY,UAAY,mBAGnBm2B,wBAAA,WACE,IAAIC,EAAgB,MACI,MAApBvuC,KAAKouC,cACPG,EAAgB,iCAGlB,IAAIC,EAAe,MAmBnB,OAlBuB,MAAnBxuC,KAAKquC,aACPG,EAAe,gCAGA,WACbzsB,EAAK,mQAMiBwsB,mCACDC,8OCvDhBC,GAAqC,CAChDxrB,WAAYyrB,iBACZvrB,YAAa,SACbC,WAAY,SAAChb,OAACyB,WAAQ8Y,UAAOD,YACpB5b,MAAG6nC,UAAO94B,WAAQ6kB,SAAMkU,aACxBC,oBACDC,EAAgBpsB,EAChBqsB,EAAkB,CAACjoC,EAAa4zB,EAAgBkU,GAClDR,EAAc,KACJ,MAAVv4B,IACFu4B,EAAcv4B,EAAOtQ,MACrBwpC,EAAgBxuC,KAAKsV,IAEvB,IAAIw4B,EAAa,KACJ,MAATM,IACFN,EAAaM,EAAMppC,MACnBwpC,EAAgBxuC,KAAKouC,IAEvB,IAAMhpC,EAAU,IAAI2oC,GAChBxnC,EAAEvB,MAAOm1B,EAAKn1B,MAAOqpC,EAASrpC,MAAO6oC,EAAaC,GAChDvrB,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAAC2hC,KAC9C,OAAOC,EAAc/rB,iBACjBpd,EAASopC,EAAiBjoC,EAAER,MAAOwc,KCapC,IAAMksB,GAAkC,CAC7C/rB,WAAYgsB,cACZ9rB,YAAa,SACbC,oBAtC0BX,GAKnB,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MAAGgO,WAAQ2M,SAAMC,2BAEtBjc,YACAw2B,QACA4F,eACAoC,cACA/H,oBACAld,eACAkF,mBAGIggB,EAAc55B,eAAa65B,wBAAwBtC,GAMzD,OAAOwB,GAAW,CAChBv8B,IACAgO,SACAmkB,SARe3uB,eAAa85B,kBAC1Bt9B,EAAEvB,MACFuP,EAAOvP,MAA2CE,EAASw+B,EAAWhI,EACtEC,GAAiB,EAAuBgI,GAM1CxhB,UACAjB,OACAC,yBACAwC,iBACAlF,cAEJ,GCsCO,IAAMkwB,GAA2C,CACtDjsB,WAAYksB,uBACZhsB,YAAa,SACbC,oBA1EmCX,GAK5B,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MAAGgO,WAAQ2M,SAAMC,2BACjBjc,YAASw2B,QAAKgI,cAAW/H,oBAAiBld,eAAYkF,mBAGzDolB,EAAarF,EACC,MAAdqF,IACFA,EAAa,CAAC,EAAG,IAGnBn/B,OAAK2B,OACDxB,eAAa8kC,+BAA+B3pC,EAAS6jC,IACrD,WAAM,MAAA,gFACgB7jC,qBAA0B6jC,SAEpD,IAAMrQ,EAAW3uB,eAAa85B,kBAC1Bt9B,EAAEvB,MACFuP,EAAOvP,MAA2CE,EAAS6jC,EAC3DrN,EAAKC,GAAiB,GAEpBmT,EAA8B,CAACvoC,EAAGgO,GAElCyK,EAAkB,MAARkC,EACVxC,EAAsD,MAA1ByC,EAE9BnC,GACF8vB,EAAc9uC,KAAKkhB,GAEjBxC,GACFowB,EAAc9uC,KAAKmhB,GAGrB,IAKI/b,EALEkD,EAAa,CACjB,CAACO,KAAM,QAAS8D,KAAM,CAAC+rB,EAASkC,QAAQM,IAAKxC,EAASkC,QAAQ9M,OAC9D,CAACjlB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASiC,SAAUjC,EAASgC,WA6BrD,OAzBIhC,EAASiC,SAAW,GAAKjC,EAASgC,QAAU,GAC5ChC,EAASuC,aAAe,GACxBvC,EAAS6I,aAAe7I,EAAS+I,aACL,IAA5B/I,EAASyC,gBAAmD,IAA3BzC,EAAS0C,eAC1C1C,EAAS6I,WAAa,GAAM,EAC9Bn8B,EAAU,IAAIsjC,GACVhQ,EAAU1Z,EAASP,EAAYC,IAEnCtZ,EAAU,IAAIwjC,GACVlQ,EAAU1Z,EAASP,EAAYC,GACnCpW,EAAWtI,KACP,CAAC6I,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS8B,eAChC,CAAC3xB,KAAM,QAAS8D,KAAM,CAAC+rB,EAAS6B,cAChC,CAAC1xB,KAAM,QAAS8D,KAAM,CAAC+rB,EAASsC,aAActC,EAASuC,cAAe,CACpEpyB,KAAM,QACN8D,KAAM,CAAC+rB,EAASyC,eAAgBzC,EAAS0C,kBAG9B,cAAf3c,IACFnW,EAAWtI,KAAK,CAAC6I,KAAM,UAAW8D,KAAM,CAACgX,KACzCve,EAAQ2B,UAAY,iBAGlBob,EAAQK,iBAAiBpd,EAAS0pC,EAAe,UAAWxmC,EAGlE,iBC9DE,WAAYymC,EAAkB/pC,GAL9BvF,mBAA0B,CAAC,IAAK,WAEhCA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKmY,UAAY,YAAYm3B,EAC7BtvC,KAAKsvC,SAAWA,EAChBtvC,KAAKsH,SAAW,6BAA6BN,EAAkBsoC,cAGjEC,wBAAA,WACE,IAAIC,EAqBJ,OAnBEA,EADExvC,KAAKsvC,SAAW,EACH,sBAEA,mBAEA,WACbvtB,EAAK,wSAMiBytB,4LCgBvB,IAAMC,GAA+B,CAC1CxsB,WAAYysB,WACZvsB,YAAa,SACbC,oBA/CEX,GACK,IAAA5Y,WAAQ6Y,YACR/W,WAAQunB,YAETyc,EAAezc,EAAQ3tB,MACvB6qB,EAAYuf,EAAaA,EAAarvC,OAAS,GAC/CiwB,EAAapmB,OAAK6L,cAAcrK,EAAOpG,OAEvC6C,8CAAC+2B,OAAahP,OAAWE,OAAW5qB,OAGpCmqC,EAAiBvsB,GACnB,CAACxZ,OAAQ,CAAC/C,EAAGosB,GAAUxQ,UAASC,MAAO,CAACpd,MAAO,CAAC4qB,EAAWC,MACzDyf,EAAWxsB,GAAQ,CACvBxZ,OAAQ,CAAC/C,EAAG6E,GACZ+W,UACAC,MAAO,CAACpd,MAAO,CAAE4E,OAAK6L,cAAcrK,EAAOpG,OAAS8qB,EAAYA,MAElE,GAAI3N,EAAQiF,mBAAmB,CAAChc,EAAQunB,KACnB,WAAjBvnB,EAAOrF,MAAoB,CAC7B,IAAM2pB,EAAcvN,EAAQzO,SAASif,EAAQ7iB,QACvC6f,EAAYxN,EAAQotB,WAA4BnkC,GAChDokC,EAAW/f,GACbC,EAAaC,EAAWvkB,EAAOrF,MAAO6pB,EAAWC,EAAWC,EAC5D5qB,EAASkG,EAAOpG,MAAOgrB,GAE3B,OAAO7N,EAAQ9O,eAAeurB,EAAaxzB,EAAOrF,MAAOypC,EAASr+B,QAEpE,IAAM/L,EAAU,IAAI4pC,GAAgBnf,EAAW,CAACD,EAAWE,IACrDvN,EACF,CAAC,CAAC1Z,KAAM,QAAS8D,KAAM,CAACkjB,IAAa,CAAChnB,KAAM,QAAS8D,KAAMzH,IACzD+D,EAAMkZ,EAAQK,iBAChBpd,EAAS,CAACkqC,EAAUD,GAAiBC,EAASvpC,MAAOwc,GAEnDya,EACFla,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG0C,GAAMkZ,UAASC,MAAO,CAACpd,MAAO45B,KAMvD,OAJAzc,EAAQ7R,YAAY++B,EAAev/B,QACnCqS,EAAQ7R,YAAYg/B,EAASx/B,QAC7BqS,EAAQ7R,YAAYrH,EAAI6G,QAEjBktB,CACT,iBCtCE,WAAY/b,EAAkBpV,GAL9BpM,mBAA0B,CAAC,IAAK,WAChCA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAGLA,KAAKoM,YAAcoV,EAAOra,QAC1BnH,KAAKwhB,OAASA,EACdxhB,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKmY,UAAY,gBAGnB63B,wBAAA,WACE,IAAMvT,EAgBV,SAAyBjb,GAGvB,IAFA,IAAMyuB,EAAgB,CAAC,UAAW,UAAW,UAAW,WAClDxT,EAAe,GACZ77B,EAAI,EAAGA,EAAI4gB,EAAOlhB,OAAQM,IACvB,IAANA,EACF67B,EAAal8B,KAAK,UAElBk8B,EAAal8B,KAAK,GAAG0vC,EAAcrvC,IAGvC,OAAO67B,EAAah2B,MACtB,CA3ByBypC,CAAgBlwC,KAAKwhB,QAW1C,MAViB,WACbO,EAAK,6SAKuC0a,iDCvBpC0T,GACZ1tB,GAGK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MAAGosB,YACH+D,SAAMmZ,cAIPC,EAAalmC,OAAKguB,eAAelB,EAAMnwB,EAAEvB,OAAO,GAEhD+qC,EAAYhmC,eAAaimC,aAAaC,yBACxC1pC,EAAGosB,EAASmd,EAAYD,GAEtBK,EAActmC,OAAK6L,cAAckd,EAAQ3tB,OAEzCw0B,EAAY,GAEZ8V,EAAWxsB,GAAQ,CACvBxZ,OAAQ,CAAC/C,KACT4b,UACAC,MAAO,CACLpd,MAAO,CACL+qC,EAAU9W,UAAW8W,EAAUI,UAAWJ,EAAUK,QACpDL,EAAUjgB,cAKVI,EAAepN,GAAQ,CAC3BxZ,OAAQ,CAAC/C,EAAGosB,GACZxQ,UACAC,MAAO,CAACpd,MAAO,CAAC+qC,EAAU9W,UAAWiX,EAAcH,EAAU9W,cAG/DO,EAAUx5B,KAAKsvC,GACf9V,EAAUx5B,KAAKkwB,GAEf,IAAMI,EAAqB,CACzByf,EAAU9W,UAAW8W,EAAUI,UAAWD,EAAcH,EAAU9W,UAClE8W,EAAUjgB,WAGZ,GAAI3N,EAAQiF,mBAAmB,CAAC7gB,EAAGosB,IAAW,CAC5C,IACM0d,EADoBluB,EAAQ7T,UAAUpM,IAAIguB,EAAapgB,QACrBqB,OAClCkf,EACF5tB,SAAOytB,EAAalrB,MAAOkrB,EAAanqB,MAAOsqC,GAG7CC,EADcnuB,EAAQ7T,UAAUpM,IAAIotC,EAASx/B,QACvBqB,OACtBif,EACF3tB,SAAO6sC,EAAStqC,MAAOsqC,EAASvpC,MAAOuqC,GACrCrgB,EAASE,GAAgBC,EAAMC,EAAYC,GAIjD,OAFAkJ,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEtCqS,EAAQ9O,eACX08B,EAAUlkC,YAAaokB,EAAOlqB,MAAOkqB,EAAO9e,QAGlD,IAAM/L,EAAU,IAAIqqC,GAAcH,EAAStqC,MAAOsrB,GAC5CrnB,EAAMkZ,EAAQK,iBAChBpd,EAAS,CAACkqC,EAAUpf,GAAeof,EAASvpC,OAChDyzB,EAAUx5B,KAAKiJ,GAEf,IAAM+zB,EAAWla,GACb,CAACxZ,OAAQ,CAAC/C,EAAG0C,GAAMkZ,UAASC,MAAO,CAACpd,MAAO+qC,EAAUlkC,eAEzD,OADA2tB,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WACtCktB,CACT,CAEO,IAAMuT,GAA+B,CAC1C7tB,WAAY8tB,WACZ5tB,YAAa,SACbC,WAAY+sB,IC9EDa,GAAUlpB,GAAiB,CACtCP,OAAQ9M,EAAa4B,QACrBmL,cAAeypB,GACf3qC,MAAO,SAGI4qC,GAA8B,CACzCjuB,WAAYkuB,UACZhuB,YAAa,SACbC,WAAY4tB,ICTDI,GAAetpB,GAAiB,CAC3CP,OAAQ9M,EAAa6B,cACrBhW,MAAO,OACPkhB,cAAe6pB,KAGJC,GAAmC,CAC9CruB,WAAYsuB,eACZpuB,YAAa,SACbC,WAAYguB,ICXD1f,GACTpK,GAAgB,CAACC,OAAQhM,EAAYqC,OAAQtX,MAAO,SAE3CkrC,GAA4B,CACvCvuB,WAAYwuB,QACZtuB,YAAa,SACbC,WAAYsO,ICUP,IAAMggB,GAAgC,CAC3CzuB,WAAY0uB,YACZxuB,YAAa,SACbC,oBAjBwBX,GAKjB,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MAEDgc,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,YACvCvH,EAAU,IAAI0hB,GAAevgB,EAAEvB,MAAOgW,EAAY0C,WAExD,OADAtY,EAAQ2B,SAAW,eACZob,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAI,UAAWgc,EAC3D,GCZa8uB,GAAO9pB,GAChB,CAACP,OAAQ9M,EAAa8B,KAAMjW,MAAO,OAAQkhB,cAAeqqB,KAEjDC,GAA2B,CACtC7uB,WAAY8uB,OACZ5uB,YAAa,SACbC,WAAYwuB,ICNDI,GAAYlqB,GAAiB,CACxCP,OAAQ9M,EAAa+B,WACrBlW,MAAO,OACPkhB,cAAeyqB,KAGJC,GAAgC,CAC3CjvB,WAAYkvB,YACZhvB,YAAa,SACbC,WAAY4uB,ICVDlnB,GACTxD,GAAgB,CAACC,OAAQhM,EAAYuC,IAAK0J,cAAe+J,KAEhD6gB,GAA0B,CACrCnvB,WAAYovB,MACZlvB,YAAa,SACbC,WAAY0H,ICNDwnB,GACTxqB,GAAiB,CAACP,OAAQ9M,EAAagC,YAAanW,MAAO,SAElDisC,GAAiC,CAC5CtvB,WAAYuvB,aACZrvB,YAAa,SACbC,WAAYkvB,ICLDG,GAAanrB,GAAgB,CAACC,OAAQhM,EAAYwC,cAElD20B,GAAiC,CAC5CzvB,WAAY0vB,aACZxvB,YAAa,SACbC,WAAYqvB,ICLDG,GAAU9qB,GAAiB,CACtCP,OAAQ9M,EAAaqC,IACrB0K,cAAeqrB,KAGJC,GAA8B,CACzC7vB,WAAY8vB,UACZ5vB,YAAa,SACbC,WAAYwvB,ICIP,IAAMI,GAA8B,CACzC/vB,WAAYgwB,UACZ9vB,YAAa,SACbC,oBAhBEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAk1B,eAAYv2B,YAASw2B,QAAKC,oBAMjC,OAAOrB,GAAS/zB,EAJCwD,eAAa6xB,kBAC1Br1B,EAAEvB,MAA2Cy2B,EAAYv2B,EAF3C,EAGHw2B,EAAKC,GAES,MAAOxZ,EACtC,GCDO,IAAMwwB,GAA0B,CACrCjwB,WAAYkwB,MACZhwB,YAAa,SACbC,oBAZEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UAIxB,OAAOkU,yBAA0B,MAAOnU,EAC1C,GCPa0wB,GAAUtrB,GAAiB,CACtCP,OAAQ9M,EAAasC,IACrByK,cAAe6rB,KAGJC,GAA8B,CACzCrwB,WAAYswB,UACZpwB,YAAa,SACbC,WAAYgwB,kBCCZ,WACIphB,EAAkBwhB,EAClBC,GAFJ,WATAzzC,cAAW,GAGXA,mBAAgB,CAAC,KACjBA,mBAA0C,CAAC,GAAI,EAAG,GAGlDA,WAAO,EAKLA,KAAKoM,YAAconC,EAAShuC,KACxB,SAAC9H,EAAGkD,GAAM,OAAAlD,EAAE,GAAqBs0B,EAAOpxB,GAAKlD,EAAE,MACnDsC,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKgyB,OAASA,EACdwhB,EAAShuC,KAAI,SAAClG,EAAGsB,GACf6C,EAAK6D,UAAY,OAAO1G,qBAE1BZ,KAAK6V,OAAkB,YAAT49B,EAAqB,EAAI,EACvCzzC,KAAKmY,UAAY,aAAas7B,SAGhCC,wBAAA,WACE,IAAMlrC,EAAOxI,KAAKgyB,OAAO1xB,OAEnBsyB,EAAQ5yB,KAAKgyB,OAAOxsB,KAAI,SAAClG,EAAGsB,GAAM,MAAA,eAAeA,WAAQ6F,KAAK,KAC9D2L,EAAMpS,KAAKgyB,OACAxsB,KACG,SAAClG,EAAGsB,GAAM,MAAA,eAAeA,2BACrB4H,EAAO,EAAI,IAAI5H,MAAO,OAC7B6F,KAAK,KAEhBktC,EAAuB,IAATnrC,EAAa,QAAU,WACrCorC,EAAqB,IAATprC,EAAa,MAAQ,SACjCqrC,EAAsB,IAATrrC,EAAa,OAAS,UACnClC,EAAQU,EAAkBwB,GAC1BsrC,EAAiBtrC,EAAO,EAC1B,CAAC,YAAa,YAAa,YAAa,aAAarB,MAAM,EAAGqB,GAC9D,SAEJ,MAAO,WACHuZ,EAAK,4EAEWzb,MAASssB,6BACXtsB,MAAS8L,wFAEC5J,qCACdqrC,QAAgBF,wBAClBE,QAAgBF,YAAqBE,QAC7C7zC,KAAK6V,mCACWg+B,SAAiBD,wBACzBC,SAAiBD,iBAAwBC,QACjD7zC,KAAK6V,sHAI4Bi+B,wCC5D5BC,GAAgC,CAC3C9wB,WAAY+wB,YACZ7wB,YAAa,SACbC,WAAY,SAAChb,OAACyB,WAAQ8Y,UAAOD,YACpB5b,MACA0sC,aAAUC,SACX3E,EAAgBpsB,EAEhBI,EAAc0wB,EAAShuC,KAAI,SAAA9H,GAC/B,MAAO,CAAC0L,KAAM,QAAS8D,KAAM,CAACxP,EAAE,GAAIA,EAAE,QAElCiI,EAAU,IAAI+tC,GAAiB5sC,EAAEvB,MAAOiuC,EAAUC,GAIxD,OAFI3E,EAAc/rB,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,KCQvD,IAAMmxB,GAA0B,CACrChxB,WAAYixB,MACZ/wB,YAAa,SACbC,oBApBkBX,GAEX,IAAA5Y,WAAQ6Y,YACR5b,MAEP,GAAI4b,EAAQiF,mBAAmB,CAAC7gB,IAAK,CACnC,IAAM8gB,EAAQlF,EAAQ7T,UAAUpM,IAAIqE,EAAEuJ,QAChCjI,oCAACyf,OAAWiB,OAElB,OAAOpG,EAAQ9O,eAAekV,EAAUhiB,EAAER,MAAOuhB,GAGnD,IAAMliB,EAAU,IAAI0hB,GAAevgB,EAAEvB,MAAOgW,EAAYyC,KAExD,OAAO0E,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAClD,GCCO,IAAM6tC,GAA0C,CACrDlxB,WAAYmxB,sBACZjxB,YAAa,SACbC,oBA1BkCX,GAKlCrO,QAAQC,KACJ,kGAGG,IAAAxK,WAAQ6Y,YAASC,UACjB8jB,UAAO4N,WACPC,kBAAeC,iBAAcC,mBAE9BC,EAAY/xB,EAAQzO,SAASwyB,EAAMp2B,QACnCqkC,EAAahyB,EAAQzO,SAASogC,EAAOhkC,QAEpCskC,oEAGP,OAAOjyB,EAAQ9O,eACX,CAAC+gC,EAAgBr0C,QAAS,QAAS,IAAI8M,WAAWunC,GACxD,GCcO,IAAMC,GAA0C,CACrD3xB,WAAY4xB,sBACZ1xB,YAAa,SACbC,oBArCkCX,GAKlCrO,QAAQC,KACJ,kGAGG,IAAAxK,WAAQ6Y,YAASC,UACjB8jB,UAAO4N,WACPC,kBAAeC,iBAAcC,mBAAgBM,iBAE9CL,EAAY/xB,EAAQzO,SAASwyB,EAAMp2B,QACnCqkC,EAAahyB,EAAQzO,SAASogC,EAAOhkC,QAErC0kC,EAAmBT,EACnBU,EAAkBT,EAClBU,EAAoBT,EACpBU,EAAkBJ,EAElB1sC,sDAACusC,oBAAiBQ,mBAKxB,MAAO,CACLzyB,EAAQ9O,eACJ,CAAC+gC,EAAgBr0C,QAAS,QAAS,IAAI8M,WAAWunC,IACtDjyB,EAAQ9O,eACJ,CAACuhC,EAAe70C,QAAS,UAAW,IAAI6M,aAAagoC,IAE7D,YC3BgBC,GACZ3yB,GACK,IAAA5Y,WAAQ6Y,YACR5b,MACP,GAAgB,cAAZA,EAAER,MAAuB,CAC3B,IAAM24B,EAAWnuB,GAAK,CAACjH,OAAQ,CAACoO,MAAOnR,GAAI4b,YACrC5hB,EAAIs0C,GAAU,CAACvrC,OAAQ,CAAC/C,EAAGm4B,GAAWvc,YACtC2yB,EAAWtkC,GAAK,CAAClH,OAAQ,CAACoO,MAAOnR,GAAI4b,YACrC9hB,EAAIw0C,GAAU,CAACvrC,OAAQ,CAAC/C,EAAGuuC,GAAW3yB,YAEtC9jB,EAASmoB,GAAQ,CAACld,OAAQ,CAACiH,KAAMhQ,EAAGiQ,KAAMnQ,GAAI8hB,YAOpD,OALAA,EAAQ7R,YAAYouB,EAAS5uB,QAC7BqS,EAAQ7R,YAAY/P,EAAEuP,QACtBqS,EAAQ7R,YAAYwkC,EAAShlC,QAC7BqS,EAAQ7R,YAAYjQ,EAAEyP,QAEfzR,EAEP,OAAO4jB,GAAK,CACVG,MAAO,CACLpd,MAAOuB,EAAEvB,MACTe,MAAOQ,EAAER,MACT/H,MAAmB,WAAZuI,EAAER,MAAqB,GAAK,GAErCoc,WAGN,CAEO,IAAM4yB,GAAgC,CAC3CryB,WAAYsyB,YACZpyB,YAAa,SACbC,WAAYgyB,ICNP,IAAMI,GAA+B,CAC1CvyB,WAAYwyB,WACZtyB,YAAa,SACbC,oBA7BcsyB,EACZjzB,GACK,IAAA5Y,WAAQ6Y,YACR5b,MAEP,GAAgB,WAAZA,EAAER,MACJ,MAAM,IAAIlD,MAAM,gDACX,GAAgB,cAAZ0D,EAAER,MAAuB,CAClC,IAAM24B,EAAWnuB,GAAK,CAACjH,OAAQ,CAACoO,MAAOnR,GAAI4b,YACrC5hB,EAAI40C,EAAS,CAAC7rC,OAAQ,CAAC/C,EAAGm4B,GAAWvc,YACrC2yB,EAAWtkC,GAAK,CAAClH,OAAQ,CAACoO,MAAOnR,GAAI4b,YACrC9hB,EAAIw0C,GAAU,CAACvrC,OAAQ,CAAC/C,EAAGuuC,GAAW3yB,YAEtC9jB,EAASmoB,GAAQ,CAACld,OAAQ,CAACiH,KAAMhQ,EAAGiQ,KAAMnQ,GAAI8hB,YAOpD,OALAA,EAAQ7R,YAAYouB,EAAS5uB,QAC7BqS,EAAQ7R,YAAY/P,EAAEuP,QACtBqS,EAAQ7R,YAAYwkC,EAAShlC,QAC7BqS,EAAQ7R,YAAYjQ,EAAEyP,QAEfzR,EAEP,OAAO4jB,GAAK,CAACG,MAAO,CAACpd,MAAOuB,EAAEvB,MAAOe,MAAOQ,EAAER,MAAO/H,MAAO,GAAImkB,WAEpE,GCUO,IAAMizB,GAA2B,CACtC1yB,WAAY2yB,OACZzyB,YAAa,SACbC,oBAxCEX,GAEK,IAAA5Y,WAAQ6Y,YACRuU,eAEP,GAAsB,IAAlBptB,EAAOvJ,OACT,OAAO+qC,GACH,CAACxhC,OAAQ,CAACoO,MAAOpO,EAAO,IAAK6Y,UAASC,MAAO,CAAC7U,IAAKmpB,KAGzD,IAAM1xB,EAAQsE,EAAO,GAAGtE,MAClBe,EAAQuD,EAAO,GAAGvD,MAExBuD,EAAOjG,SAAQ,SAAAxE,GACb+K,OAAK0rC,kBACDtwC,EAAOnG,EAAEmG,MACT,yDACJ4E,OAAK2B,OACDxF,IAAUlH,EAAEkH,OACZ,WAAM,MAAA,8DAGZ,IAAMiyB,EAAwC,GAQxC35B,EAASuC,GAAO,CAAC0I,OAPCA,EAAOrE,KAAI,SAAApG,GACjC,IAAM02C,EACFzK,GAAW,CAACxhC,OAAQ,CAACoO,MAAO7Y,GAAIsjB,UAASC,MAAO,CAAC7U,IAAKmpB,KAE1D,OADAsB,EAAwBh4B,KAAKu1C,GACtBA,KAGuCpzB,UAASC,MAAO,CAACsU,UAIjE,OAFAsB,EAAwB30B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEpDzR,CACT,iBC5BE,WAAYozB,EAAkBwhB,GAA9B,WANAxzC,mBAAgB,CAAC,KACjBA,cAAW,uBACXA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAGLA,KAAKoM,YAAconC,EAAShuC,KACxB,SAAC9H,EAAGkD,GAAM,OAAAlD,EAAE,GAAqBs0B,EAAOpxB,GAAKlD,EAAE,MACnDsC,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDstC,EAAShuC,KAAI,SAAClG,EAAGsB,GACf6C,EAAK6D,UAAY,OAAO1G,qBAE1BZ,KAAKgyB,OAASA,EACdhyB,KAAKmY,UAAY,aAGnB49B,wBAAA,WACE,IAAMvtC,EAAOxI,KAAKgyB,OAAO1xB,OACnB8I,EAAOpC,EAAkBwB,GAEzBoqB,EAAQ5yB,KAAKgyB,OAAOxsB,KAAI,SAAClG,EAAGsB,GAAM,MAAA,eAAeA,WAAQ6F,KAAK,KAC9D2L,EAAMpS,KAAKgyB,OACAxsB,KACG,SAAClG,EAAGsB,GAAM,MAAA,eAAeA,2BACrB4H,EAAO,EAAI,IAAI5H,MAAO,OAC7B6F,KAAK,KAChBuvC,EAAaxtC,EAAO,EAAOY,MAAQwpB,MAAW,GAAGA,EACjDqjB,EAAWztC,EAAO,EAAOY,MAAQgJ,MAAS,GAAGA,EAE7C8jC,EAAmB1tC,EAAO,EAAI,oBAAsB,eACpD2tC,EAAoB3tC,EAAO,EAAI,mBAAqB,cAEpDsrC,EAAiBtrC,EAAO,EAC1B,CAAC,YAAa,YAAa,YAAa,aAAarB,MAAM,EAAGqB,GAC9D,SAkBJ,MAhBiB,WACbuZ,EAAK,4EAEWi0B,4BACFC,yEAGNC,SAAuBC,6KAIIrC,qDCnD9BsC,GACT,SAAC3zB,GAGQ,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACA0sC,aAAU6C,kBACjB,GAAI7C,EAAS3lC,OAAM,SAAAnQ,GAAK,OAAAyM,OAAKC,YAAY1M,EAAG,CAAC,EAAG,OAC9C,OAAOkpB,GAAS,CAAC/c,OAAQ,CAAC/C,KAAI4b,YAEhC,GAAoC,IAAhCvY,OAAK6L,cAAclP,EAAEvB,OAMvB,OAAOid,GAAK,CACVE,UACAC,MAAO,CAACpd,MALUiuC,EAAShuC,KACzB,SAAC9H,EAAGkD,GACA,OAAAlD,EAAE,GAAqBoJ,EAAEvB,MAAM3E,GAAKlD,EAAE,MAGhBa,MAAO83C,EAAe/vC,MAAOQ,EAAER,SAG/D,IAAMwc,EAAc,CAAC,CAAC1Z,KAAM,UAAW8D,KAAM,CAACmpC,KAC9C7C,EAAShuC,KAAI,SAAA9H,GAAK,OAAAolB,EAAYviB,KAAK,CAAC6I,KAAM,QAAS8D,KAAM,CAACxP,EAAE,GAAIA,EAAE,SAClE,IAAMiI,EAAU,IAAIowC,GAAWjvC,EAAEvB,MAAOiuC,GACxC,OAAO9wB,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,EACzD,EAESwzB,GAA4B,CACvCrzB,WAAYszB,QACZpzB,YAAa,SACbC,WAAYgzB,IChCDI,GAAM1uB,GAAiB,CAClCP,OAAQ9M,EAAauC,MAGVy5B,GAA0B,CACrCxzB,WAAYyzB,MACZvzB,YAAa,SACbC,WAAYozB,ICIP,IAAMG,GAA4B,CACvC1zB,WAAY2zB,QACZzzB,YAAa,SACbC,oBAZoBX,GAEb,IAAA5Y,WAAQ6Y,YACR5b,MAAG+vC,UAEJlxC,EAAU,IAAI4gB,GAAgB9L,EAAaoC,MAAO/V,EAAEvB,MAAOsxC,EAAMtxC,OACvE,OAAOmd,EAAQK,iBAAiBpd,EAAS,CAACmB,EAAG+vC,GAAQ,UACvD,GCCO,IAAMC,GAA2B,CACtC7zB,WAAY+jB,OACZ7jB,YAAa,SACbC,oBAZEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UAIxB,OAAOkU,yBAA0B,OAAQnU,EAC3C,GCAaq0B,GAA4B,CACvC9zB,WAAY+zB,QACZ7zB,YAAa,SACbC,WAVE,SAACX,GACQ,IAAAC,YAASC,UACTiQ,UAAOC,SAAMr0B,SAAM8H,UACpBoL,EAASihB,GAAaC,EAAOC,EAAMr0B,EAAM8H,GAC/C,OAAOoc,EAAQ9O,eAAe,CAAClC,EAAOpR,QAASgG,EAAOoL,EACxD,GCNSulC,GAAUnvB,GAAiB,CAACP,OAAQ9M,EAAa0B,MAEjD+6B,GAA8B,CACzCj0B,WAAYk0B,UACZh0B,YAAa,SACbC,WAAY6zB,ICNDG,GAAa9vB,GAAgB,CAACC,OAAQhM,EAAY2C,aAElDm5B,GAAiC,CAC5Cp0B,WAAYq0B,aACZn0B,YAAa,SACbC,WAAYg0B,ICLDG,GAAOjwB,GAAgB,CAACC,OAAQhM,EAAY4C,OAE5Cq5B,GAA2B,CACtCv0B,WAAYw0B,OACZt0B,YAAa,SACbC,WAAYm0B,ICLDG,GAAQpwB,GAAgB,CAACC,OAAQhM,EAAY6C,QAE7Cu5B,GAA4B,CACvC10B,WAAY20B,QACZz0B,YAAa,SACbC,WAAYs0B,kBCIZ,WACI1gB,EAA8C6gB,EAC9CC,GAPJ93C,mBAAgB,CAAC,KACjBA,cAAW,yDACXA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAKLA,KAAKoM,YAAc,CAAC4qB,EAAW,GAAI6gB,EAAWC,EAAU9gB,EAAW,IAEnEh3B,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAE9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKmY,UAAY,wBAGnB4/B,wBAAA,WA6CE,MA5CiB,WACbh2B,EAAK,0vDCCN,IAAMi2B,GAAqC,CAChD/0B,WAAYg1B,iBACZ90B,YAAa,SACbC,oBA3B6BX,GAKtB,IAAA5Y,WAAQ6Y,YAASC,UACjBu1B,WACAC,iBAAct2C,SAAMu2C,qBAErBhwC,SAACyvC,OAAWC,OAIZh1B,EAAc,CAClB,CAAC1Z,KAAM,UAAW8D,KAAM,CAJLirC,GAAgBN,EAAY,EAAI,EAAM,EACvCM,GAAgBL,EAAW,EAAI,EAAM,IAIvD,CAAC1uC,KAAM,UAAW8D,KAAM,CAHIkrC,EAAmB,GAAM,KAMjDzyC,EAAU,IAAIoyC,GAChBG,EAAO3yC,MAA2CsyC,EAAWC,GAEjE,OAAOp1B,EAAQK,iBAAiBpd,EAAS,CAACuyC,GAAS,UAAWp1B,EAChE,iBCbE,WACIkU,EAA8C6gB,EAC9CC,EAAkBM,GARtBp4C,mBAAgB,CAAC,KACjBA,cAAW,kDACXA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAKLA,KAAKoM,YAAc,CAAC4qB,EAAW,GAAI6gB,EAAWC,EAAU9gB,EAAW,IAEnEh3B,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAE9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKo4C,iBAAmBA,EACxBp4C,KAAKmY,UAAY,iBAAiBigC,SAGpCC,wBAAA,WACE,IAAIC,EAyCJ,OAvCEA,EADEt4C,KAAKo4C,iBAEH,0FAGgB,kDAGL,WACbr2B,EAAK,2uBAmBuBu2B,icC5B7B,IAAMC,GAA4C,CACvDt1B,WAAYu1B,wBACZr1B,YAAa,SACbC,oBA5BoCX,GAK7B,IAAA5Y,WAAQ6Y,YAASC,UACjBu1B,WACAC,iBAAcC,qBAEfhwC,cAACyvC,OAAWC,OAKZh1B,EAAc,CAClB,CAAC1Z,KAAM,UAAW8D,KAAM,CALLirC,GAAgBN,EAAY,EAAI,EAAM,EACvCM,GAAgBL,EAAW,EAAI,EAAM,IAKvD,CAAC1uC,KAAM,UAAW8D,KAAM,CAHRirC,EAAe,GAAM,KAMjCxyC,EAAU,IAAI0yC,GAChBH,EAAO3yC,MAA2CsyC,EAAWC,EAC7DM,GACJ,OAAO11B,EAAQK,iBAAiBpd,EAAS,CAACuyC,GAASA,EAAO5xC,MAAOwc,EACnE,iBCdE,WACI+oB,EACA4M,GAZJz4C,iBAAwB,GAIxBA,mBAAgB,CAAC,KAEjBA,mBAA0C,CAAC,GAAI,EAAG,GAElDA,WAAO,EAKLA,KAAKoM,YAAcy/B,EACnB7rC,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKsH,SAAW,+EAEhBtH,KAAKmY,UAAY,SACjBnY,KAAKoM,YAAcy/B,EAEM,iBAAd4M,GACTz4C,KAAKsH,UAAY,oBACjBtH,KAAK04C,YAAc,wCACnB14C,KAAKmY,WAAa,WAElBnY,KAAKsH,UAAY,0BACjBtH,KAAK04C,YAAc,mDACnB14C,KAAKmY,WAAa,gBAItBwgC,wBAAA,WAsBE,MArBiB,aACX52B,EAAK,qnBAWD/hB,KAAK04C,6TC5CNE,GAAuC,CAChD31B,WAAY41B,mBACZ11B,YAAa,SACbC,WAAY,SAAChb,OAACyB,WAAQ8Y,UAAOD,YACpB8jB,UACAsS,YAASL,cAAWM,WACrBtxB,EAAgB/E,EAEhB/c,EAAU,IAAIgzC,GAAenS,EAAmBjhC,MAAOkzC,GACvDpwC,8DAEAya,EAAc,CACd,CAAC1Z,KAAM,UAAW8D,KAAM,QACxB,CAAC9D,KAAM,UAAW8D,KAAM,QACxB,CAAC9D,KAAM,UAAW8D,KAAM,CAAC9H,KAAK4zC,IAAIF,KAClC,CAAC1vC,KAAM,UAAW8D,KAAM,CAAC9H,KAAK2/B,IAAI+T,MAYxC,MATyB,iBAAdL,EACT31B,EAAYviB,KACR,CAAC6I,KAAM,UAAW8D,KAAM,CAACiN,OAAO8+B,WAAWR,EAAUS,QAAQ,OAEjEp2B,EAAYviB,KAAK,CAAC6I,KAAM,UAAW8D,KAAMurC,IAG5BhxB,EAAc1E,iBACzBpd,EAAS,CAAC6gC,GAAQA,EAAMlgC,MAAOwc,KC3B5Bq2B,GACT7xB,GAAgB,CAACC,OAAQhM,EAAY8C,MAAOmJ,cAAewL,KAElDomB,GAA4B,CACvCn2B,WAAYo2B,QACZl2B,YAAa,SACbC,WAAY+1B,kBCQZ,WACIG,EAAyBhK,EAAkBiK,EAC3CC,EAAqB/zC,EAAmBF,EACxC2R,EAAuBqc,gBAAAA,MAjB3BvzB,mBAAgB,CAAC,UAAW,WAO5BA,mBAA0C,CAAC,GAAI,EAAG,GAIlDA,aAAS,EAOPA,KAAKoM,YAAc7G,EACnBvF,KAAKoJ,KAAO8N,EACZlX,KAAKuzB,eAAiBA,EACtBvzB,KAAKmI,eAAiB4E,EAAmBusC,GAEzCt5C,KAAK+L,SACDI,EAAgBnM,KAAKmI,eAAgBmxC,EAAet5C,KAAKkG,eAC7DlG,KAAKy5C,uBAAyBnK,EAAW,EACzCtvC,KAAKmY,UAAY,WAAWohC,MAAeC,MACvCx5C,KAAKy5C,2BAA0BviC,MAAeqc,EAClD,IAAMmmB,EAAc1yC,EAAkBvB,EAAQnF,QAC9CN,KAAKsH,SAAW,4BAA4BoyC,iBAC5C15C,KAAKw5C,YAAcA,EACnBx5C,KAAKu5C,YAAcA,SAGrBI,wBAAA,WAAA,WACMC,EAAgB,GACK,IAArB55C,KAAKu5C,YACPK,EAAgB,YACc,IAArB55C,KAAKu5C,cACdK,EAAgB,gBAElB,IAAMC,EAAiB,cAAcD,MAE/BpK,EAAexvC,KAAKy5C,uBAAyB,sBACA,mBAE/CK,EAAkB,GAClBC,EAAgC,GACC,IAAjC/5C,KAAKmI,eAAerB,EAAExG,QACxBw5C,EAAkB,iBAClBC,EAAgC,0GAKU,IAAjC/5C,KAAKmI,eAAerB,EAAExG,SAC/Bw5C,EAAkB,uCAClBC,EAAgC,ugBAalC,IAEMC,EAAiB,cADnBv8C,MAAM6P,KAAK,CAAChN,OAAQN,KAAKw5C,cAAc,SAACl6C,EAAGk1B,GAAQ,MAAA,UAAUA,SACd/tB,KAAK,UA4CxD,MAlBiB,SACfszC,eAEEh4B,EAAK,8PAK6B83B,sEACoBrK,gEAG9CnpC,EAAerG,KAAKoJ,MAAM,OAAU4wC,4DACCF,qBArC/B,SAACG,EAAahT,GAC9B,IAAI5kB,EAAmB,aAAa43B,oBAAqBhT,OACvC,YAAdxjC,EAAK2F,OACPiZ,EAAmB,uFAGc4kB,gFAEYgT,gPAMVhT,+FAMrC,IAAMiT,EAAqB,eAAeD,oBAAqBhT,QAC/D,OAAOxjC,EAAK8vB,eAAiBlR,EAAmB63B,EAkB1CC,CAAU,qBAAsB,6CChErC,IAAMC,GAAgC,CAC3Cn3B,WAAYo3B,YACZl3B,YAAa,SACbC,oBAjDwBX,GAKjB,IAAA5Y,WAAQ6Y,YAASC,UACjBuQ,YAASC,YACT5tB,UAED6C,wCAACgoB,cAAWiD,eAAYhD,cAAW5qB,YAAS2tB,eAG5CI,EAAe,CAACJ,EAAa/C,EAAWA,GAE9C,GAAmB,IAAf+C,EACF,OAAO1Q,EAAQ9O,eAAerO,EAAO2tB,EAAQ5sB,OAG/C,IAAMspC,EAAiBvsB,GACnB,CAACxZ,OAAQ,CAAC/C,EAAGosB,GAAUxQ,UAASC,MAAO,CAACpd,MAAO,CAAC8tB,EAAYjD,MAC1Dyf,EAAWxsB,GACb,CAACxZ,OAAQ,CAAC/C,EAAGqsB,GAAUzQ,UAASC,MAAO,CAACpd,MAAO,CAAC8tB,EAAYhD,MAE1DjnB,EAAOymC,EAASvpC,MAChBT,EACF2c,GAAK,CAACE,UAASC,MAAO,CAACpd,MAAOiuB,EAAcj1B,MAAO,EAAG+H,MAAO8C,KAE3D0Z,EAAc,CAClB,CAAC1Z,KAAM,QAAS8D,KAAM,CAACkjB,IAAa,CAAChnB,KAAM,QAAS8D,KAAMzH,GAC1D,CAAC2D,KAAM,QAAS8D,KAAM,CAHX/C,OAAK6L,cAAc65B,EAAStqC,UAKnCI,EAAU,IAAIg0C,GAChB9J,EAAStqC,MAAO6qB,EAAWwf,EAAerqC,MAAMjF,OAChDuvC,EAAStqC,MAAMjF,OAAQmF,EAAS+tB,EAAcpqB,GAC5CI,EAAMkZ,EAAQK,iBAChBpd,EAAS,CAACkqC,EAAUD,GAAiBxmC,EAAM0Z,EAAajd,GAEtD03B,EAAWla,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG0C,GAAMkZ,UAASC,MAAO,CAACpd,WAM7D,OAJAmd,EAAQ7R,YAAY++B,EAAev/B,QACnCqS,EAAQ7R,YAAYg/B,EAASx/B,QAC7BqS,EAAQ7R,YAAYrH,EAAI6G,QAEjBktB,CACT,iBCtCE,WAAY+c,EAAe/0C,EAAiBiD,GAV5CxI,mBAAgB,CAAC,IAAK,IAAK,KAK3BA,mBAA0C,CAAC,GAAI,EAAG,GAGlDA,WAAO,EAGLA,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAEhDlG,KAAKs6C,MAAQA,EACbt6C,KAAKwI,KAAOA,EACZxI,KAAKmY,UAAY,gBAGnBoiC,wBAAA,WAEE,IAAIC,EACAC,EACJ,GAAIz6C,KAAKwI,KAAO,EACd,MAAMpF,MAAM,kBAAkBpD,KAAKwI,8BAGrC,GAAkB,IAAdxI,KAAKwI,KACPiyC,EAAW,QACXD,EAAU,YACL,CAIL,IAHA,IAAMvK,EAAgB,CAAC,UAAW,UAAW,UAAW,WAClDyK,EAAa,GACbC,EAAc,GACX/5C,EAAI,EAAGA,EAAIZ,KAAKoM,YAAY9L,OAAQM,IAC3C+5C,EAAYp6C,KAAK,GAAG0vC,EAAcrvC,IAC9BA,EAAIZ,KAAKs6C,OACXI,EAAWn6C,KAAK,GAAG0vC,EAAcrvC,IAGrC45C,EAAUE,EAAWj0C,OACrBg0C,EAAWE,EAAYl0C,OAgBzB,MAbiB,WACbsb,EAAK,kIAGey4B,gFAEeC,uEAEAA,qDC1CpC,IAAMG,GAA6B,CACxC33B,WAAY43B,SACZ13B,YAAa,SACbC,oBAdqBX,GAEd,IAAA5Y,WAAQ6Y,YACRwkB,cAAW9nC,MAAGV,MAEfiH,EACF,IAAI40C,GAAcrT,EAAU3hC,MAAMjF,OAAQlB,EAAEmG,MAAOnG,EAAEmG,MAAMjF,QAC/D,OAAOoiB,EAAQK,iBACXpd,EAAS,CAACuhC,EAAW9nC,EAAGV,GAAI4pB,aAAWlpB,EAAEkH,MAAO5H,EAAE4H,OACxD,GCVaw0C,GAAUxzB,GAAgB,CAACC,OAAQhM,EAAY+C,UAE/Cy8B,GAA8B,CACzC93B,WAAY+3B,UACZ73B,YAAa,SACbC,WAAY03B,ICHD9B,GAAM1xB,GAAgB,CAACC,OAAQhM,EAAYgD,MAE3C08B,GAA0B,CACrCh4B,WAAYi4B,MACZ/3B,YAAa,SACbC,WAAY41B,ICLDmC,GAAO7zB,GAAgB,CAACC,OAAQhM,EAAYiD,OAE5C48B,GAA2B,CACtCn4B,WAAYo4B,OACZl4B,YAAa,SACbC,WAAY+3B,ICLDG,GAAMxzB,GACf,CAACP,OAAQ9M,EAAayB,IAAKsL,cAAe+zB,GAAQxzB,iBAAiB,IAE1DyzB,GAA0B,CACrCv4B,WAAYw4B,MACZt4B,YAAa,SACbC,WAAYk4B,ICsCP,IAAMI,GAA8B,CACzCz4B,WAAY04B,UACZx4B,YAAa,SACbC,oBAzCEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjBi5B,WACA9tC,QAEDqpB,EAAOhtB,OAAKguB,eAAe,CAACrqB,GAAM8tC,EAAOr2C,OAEzCs2C,EAAWx2C,GAAI,CACnBwE,OAAQ,CAAC/C,EAAG80C,GACZl5B,UACAC,MAAO,CAAC2Y,iBAAkBnE,EAAM2C,UAAU,KAGtCgiB,EAAgBxxC,eAAa6vB,qBAAqB0hB,EAASt2C,MAAO4xB,GAElE4kB,EACF14B,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG+0C,GAAWn5B,UAASC,MAAO,CAACpd,MAAOu2C,KACtDh4B,EACFw3B,GAAI,CAACzxC,OAAQ,CAACia,EAAG83B,EAAQv+C,EAAG0+C,GAAoBr5B,YAC9CrlB,EAAI+sB,GAAI,CAACvgB,OAAQ,CAAC/C,EAAGgd,GAAIpB,YACzBs5B,EACF5mC,GAAI,CAACvL,OAAQ,CAAC/C,EAAGzJ,GAAIqlB,UAASC,MAAO,CAACsU,KAAME,EAAM2C,UAAU,KAC1DmiB,EACF54B,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGk1C,GAASt5B,UAASC,MAAO,CAACpd,MAAOu2C,KACpDtyC,EACFytC,GAAQ,CAACptC,OAAQ,CAACia,EAAGzmB,EAAGA,EAAG4+C,GAAiBv5B,YAShD,OAPAA,EAAQ7R,YAAYgrC,EAASxrC,QAC7BqS,EAAQ7R,YAAYkrC,EAAkB1rC,QACtCqS,EAAQ7R,YAAYiT,EAAEzT,QACtBqS,EAAQ7R,YAAYxT,EAAEgT,QACtBqS,EAAQ7R,YAAYmrC,EAAO3rC,QAC3BqS,EAAQ7R,YAAYorC,EAAe5rC,QAE5B7G,CACT,GCoBa0yC,GAAqC,CAChDj5B,WAAYk5B,iBACZh5B,YAAa,SACbC,WA/D4B,SAACX,GAKtB,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAs2B,eAAYoW,aAEnBrpC,OAAK2B,OACDhF,EAAEvB,MAAMjF,QAAU,GAClB,WAAM,MAAA,2EAGV,IAAMg9B,EAAOF,EAAWvG,QAAO,SAAC/S,EAAGzmB,GAAM,OAAAymB,EAAIzmB,KAEvC++C,EAA4C,CAAC,CAAC,EAAG,IACvDA,EAAiB77C,WAAjB67C,IAAyB5I,IACzB,IAAK,IAAI5yC,EAAI,EAAIw8B,EAAW98B,OAAQM,EAAIkG,EAAEvB,MAAMjF,SAAUM,EACxDw7C,EAAiB77C,KAAK,CAAC,EAAG,IAG5B,IAAMw5B,EAAY,GAEZsiB,EAAUjG,GAAM,CACpBvsC,OAAQ,CAAC/C,KACT4b,UACAC,MAAO,CAAC6wB,SAAU4I,EAAkB/F,cAAe,KAG/CiG,EACFhyC,eAAakzB,YAAY6e,EAAQ92C,MAAO63B,EAAYE,GAAM,GAExDif,EAAoCjyC,eAAaozB,YACnD4e,EAAoBh8C,OAAQ88B,EAAW98B,QAAQ,GAE7CkzB,EACFlpB,eAAaszB,oBAAoBye,EAAQ92C,MAAO63B,EAAYE,GAAM,GAEhEkf,EAAkBn5B,GACpB,CAACxZ,OAAQ,CAAC/C,EAAGu1C,GAAU35B,UAASC,MAAO,CAACpd,MAAO+2C,KAE7CG,EAAW37B,GAAU,CACzBjX,OAAQ,CAAC/C,EAAG01C,GACZ95B,UACAC,MAAO,CAACkT,KAAM0mB,KAGV39C,EACFykB,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG21C,GAAW/5B,UAASC,MAAO,CAACpd,MAAOiuB,KAQ5D,OANAuG,EAAUx5B,KAAK87C,GACftiB,EAAUx5B,KAAKi8C,GACfziB,EAAUx5B,KAAKk8C,GAEf1iB,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WAEtCzR,CACT,iBCrDE,WAAY4iB,EAAkBwT,GAT9Bh1B,mBAAgB,CAAC,KAKjBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAKL,IADA,IAAMoM,EAAwB,IAAI3O,MAAM+jB,EAAOlhB,QACtCM,EAAI,EAAGA,EAAIwL,EAAY9L,OAAQM,IACtCwL,EAAYxL,GAAK4gB,EAAO5gB,GAAKo0B,EAAKp0B,GAEpCZ,KAAKoM,YAAcA,EACnBpM,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKwI,KAAOxI,KAAKoM,YAAY9L,OAC7BN,KAAKmY,UAAY,cAGnBukC,wBAAA,WACE,IAAMjgB,EAcV,SAAyBj0B,EAAcm0C,gBAAAA,MACrC,GAAIn0C,GAAQ,EACV,MAAMpF,MAAM,iBAAiBoF,2BAE/B,GAAa,IAATA,EACF,MAAO,YAAYm0C,YAKrB,IAFA,IAAM1M,EAAgB,CAAC,UAAW,UAAW,UAAW,WAClDxT,EAAe,GACZ77B,EAAI,EAAGA,EAAI4H,EAAM5H,IACxB67B,EAAal8B,KAAK,IAAI0vC,EAAcrvC,SAAQ+7C,YAAuB/7C,QAErE,OAAO67B,EAAah2B,MACtB,CA5ByBypC,CAAgBlwC,KAAKwI,KAAM,aAUhD,MARiB,WACbuZ,EAAK,+IAG4B0a,iDC3BzBmgB,GACZjxC,GAEK,IAAA9B,WAAQ6Y,YAASC,UACjB7b,MACAkuB,SAGP,GAAItS,EAAQiF,mBAAmB,CAAC7gB,KAAmB,WAAZA,EAAER,OACrCQ,EAAEvB,MAAMjF,QAAU,EAAG,CAGvB,IAAM4M,EAAOwV,EAAQzO,SAASnN,EAAEuJ,QAC1B9R,EAAoB,WAAZuI,EAAER,MACX4G,EAAsB1H,KAAI,SAAApI,GAAK,OAAA+M,OAAKgK,aAAa/W,MAClD8P,EACE2vC,EAAM75C,SAAO8D,EAAEvB,MAAOuB,EAAER,MAAO/H,GAC/BiyB,EAASuE,GAAY8nB,EAAK7nB,GAChC,OAAOtS,EAAQ9O,eAAe4c,EAAOjrB,MAAOirB,EAAOlqB,MAAOkqB,EAAO9e,QAGnE,IAAM/L,EAAU,IAAI+2C,GAAY51C,EAAEvB,MAAOyvB,GAGzC,OAFetS,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAG1D,CAEO,IAAMw2C,GAA2B,CACtC75B,WAAY85B,OACZ55B,YAAa,SACbC,WAAYw5B,IC8EP,IAAMI,GAAoC,CAC/C/5B,WAAYg6B,gBACZ95B,YAAa,SACbC,oBA3G4BX,GAKrB,IAAA5Y,WAAQ6Y,YAASC,UACjBu6B,kBAAeC,iBAAc7pB,iBAC7BlnB,gBAEDhE,wCAACgoB,cAAWiD,eAAYhD,cAAW5qB,YAAS2tB,eAG5CG,GAAiB,EACvB,GAA2B,WAAvB4pB,EAAa72C,MAAoB,CACnC,IAAMsqB,EAAalO,EAAQotB,WAA0BoN,GAC/CE,EAAa16B,EAAQotB,WAA2BqN,GAChDE,EAAgBlzC,OAAKgK,aACvBuO,EAAQzO,SAASqf,EAAajjB,QAAQ,IACpCmgB,EAASyC,GACXrC,EAAYwsB,EAAYhxC,EAAagnB,EAAY/C,EAAWgD,EAC5DjD,EAAW3qB,EAAS43C,EAAe9pB,GACvC,OAAO7Q,EAAQ9O,eAAexH,EAAaokB,EAAOlqB,MAAOkqB,EAAO9e,QAGlE,IAAM8hB,EAAe,CAACJ,EAAa/C,EAAWA,GAExCitB,EAAiBj6B,GAAQ,CAC7BxZ,OAAQ,CAAC/C,EAAGo2C,GACZx6B,UACAC,MAAO,CAACpd,MAAO,CAAC8tB,EAAYjD,MAExBmtB,EAAgBJ,EAAa53C,MAAMjF,OACrC+iB,GAAQ,CACNxZ,OAAQ,CAAC/C,EAAGq2C,GACZz6B,UACAC,MAAO,CAACpd,MAAO,CAAC8tB,EAAYhD,MAE9BzJ,GAAS,CAAC/c,OAAQ,CAAC/C,EAAGq2C,GAAez6B,YAEnCtZ,EAAOm0C,EAAcj3C,MACrBwoB,EACFpM,EAAQ9O,eAAe,GAAIxK,EAAMe,OAAKsoB,oBAAoB,EAAGrpB,IAG3Do0C,EAAgBn6B,GAAQ,CAC5BxZ,OAAQ,CAAC/C,EAAGwsB,GACZ5Q,UACAC,MAAO,CAACpd,MAAO9H,MAAM+1B,EAAalzB,QAAQkiB,KAAK,MAE3Ci7B,EACFb,GAAK,CAAC/yC,OAAQ,CAAC/C,EAAG02C,GAAgB96B,UAASC,MAAO,CAACqS,KAAMxB,KAGvD1Q,EAAc,CAClB,CAAC1Z,KAAM,QAAS8D,KAAM,CAACkjB,IACvB,CAAChnB,KAAM,QAAS8D,KAAMzH,GACtB,CAAC2D,KAAM,QAAS8D,KAAM,CAJX/C,OAAK6L,cAAc,CAACqd,EAAYhD,OAO7C,OAAQgD,GACN,KAAK,EACH,MACF,KAAK,EAED,IAAM1tB,EAAU,IAAIg0C,GAChB,CAACtmB,EAAYhD,GAAYD,EAAWktB,EAAe/3C,MAAMjF,OACzDi9C,EAAch4C,MAAMjF,OAAQmF,EAAS+tB,EAAcpqB,EACnDmqB,GACJ7Q,EAAQK,iBACJpd,EAAS,CAAC43C,EAAeD,GAAiBl0C,EAAM0Z,EAChD26B,GAEN,MACF,QAGU93C,EAAU,IAAIg0C,GAChB,CAACtmB,EAAYhD,GAAYD,EAAWktB,EAAe/3C,MAAMjF,OACzDwuB,EAAKvpB,MAAMjF,OAAQmF,EAAS+tB,EAAcpqB,EAAMmqB,GACpD7Q,EAAQK,iBACJpd,EAAS,CAACmpB,EAAMwuB,GAAiBl0C,EAAM0Z,EAAa26B,GAIlD93C,EAAU,IAAIg0C,GAChB,CAACtmB,EAAYhD,GAAYD,EAAWktB,EAAe/3C,MAAMjF,OACzDi9C,EAAch4C,MAAMjF,OAAQmF,EAAS+tB,EAAcpqB,GACvDsZ,EAAQK,iBACJpd,EAAS,CAAC43C,EAAeD,GAAiBl0C,EAAM0Z,EAChD26B,GAIV,IAAMC,EAAcr6B,GAChB,CAACxZ,OAAQ,CAAC/C,EAAG22C,GAAe/6B,UAASC,MAAO,CAACpd,MAAO6G,KAOxD,OALAsW,EAAQ7R,YAAYysC,EAAejtC,QACnCqS,EAAQ7R,YAAY0sC,EAAcltC,QAClCqS,EAAQ7R,YAAY2sC,EAAcntC,QAClCqS,EAAQ7R,YAAYie,EAAKze,QACzBqS,EAAQ7R,YAAY4sC,EAAaptC,QAC1BqtC,CACT,GCnFO,IAAMC,GAA6B,CACxC16B,WAAY26B,SACZz6B,YAAa,SACbC,oBA1BEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACA+2C,oBAAiB5mB,SAElBsK,EAAQp3B,OAAKguB,eAAelB,EAAMnwB,EAAEvB,OAAO,GAC3Cu4C,EAAaxzC,eAAayzC,iBAAiBj3C,EAAG+2C,EAAiBtc,GAE/DzL,EAAQhvB,EAAEvB,MAAMjF,OAChBuzB,EAAQ,IAAIp2B,MAAMq4B,GAAOtT,KAAK,GAC9B3gB,EAAOiF,EAAEvB,MAAM4B,QAErB,OAAO22C,EAAWt4C,KAAI,SAAA9E,GACpB,IAAM2vB,IAAgBxuB,GACtBwuB,EAAUkR,GAAS7gC,EACnB,IAAMs9C,EACF72C,GAAM,CAAC0C,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkR,QAAOhyB,KAAMwuB,KAEtD,OADAwD,EAAM0N,IAAU7gC,EACTs9C,IAEX,GCvBatmC,GAAO4P,GAAgB,CAACC,OAAQhM,EAAYkD,OAE5Cw/B,GAA2B,CACtCh7B,WAAYi7B,OACZ/6B,YAAa,SACbC,WAAY1L,ICJDymC,GAA6B,CACxCl7B,WAAYm7B,SACZj7B,YAAa,SACbC,WAAY,SAAChb,OAACyB,WAAQ6Y,YACb5b,MACDgoC,EAAgBpsB,EAChB/c,EAAU,IAAI0hB,GAAevgB,EAAEvB,MAAOgW,EAAYmD,QACxD,OAAOowB,EAAc/rB,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,SCP7C+3C,GAAoBv2B,GAAiB,CAChDP,OAAQ9M,EAAakC,qBAGV2hC,GAAwC,CACnDr7B,WAAYs7B,oBACZp7B,YAAa,SACbC,WAAYi7B,kBCGZ,WAAY/hB,GAXZt8B,mBAAgB,CAAC,KAOjBA,mBAAgB,EAChBA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAckwB,EACnBt8B,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,cAC5C,CAAClG,KAAKmgB,cAAe,EAAG,IAE5B,IAAM7Z,EAAQU,EAAkBhH,KAAKoM,YAAY9L,QACjDN,KAAKsH,SAAW,WAAWhB,kBAAqBA,OAChDtG,KAAKmY,UAAY,sBAGnBqmC,wBAAA,WAAA,WAEMC,EAAY,GAChB,GAAa,IAFAz+C,KAAKoM,YAAY9L,OAG5Bm+C,EAAY,iDACP,CACL,IAAIC,EAAa,EACjBD,EACIz+C,KAAKoM,YACA5G,KAAI,SAAClG,EAAGsB,GAEP,OADA89C,IACmC,IAA5Bj7C,EAAK2I,YAAY9L,OACpB,6BAA6BM,wBAAuBA,MACpD,WAAU89C,EAAa,2BACnB99C,wBAAuBA,SAEhC6F,KAAK,KAWhB,MARiB,YACZsb,EAAK,mJAG4B08B,2CC+BnC,IAAME,GAAmC,CAC9C17B,WAAY27B,eACZz7B,YAAa,SACbC,oBA3E2BX,GAKpB,IA2BH7jB,EA3BGiL,WAAQ6Y,YAASC,UACjB7b,MAEL+sB,UACAzhB,QACA3M,YACAo5C,cACAC,YACAC,iBACAC,gBACAC,mBAGI72C,kDACJ82C,qBACAC,eACAC,eACAC,cACAC,kBACA1iB,UACA2iB,QACAC,YAQF,GAAIJ,EAEFxgD,EAASykB,GAAQ,CAACxZ,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACpd,MAAO45C,UAClD,GAAIE,GAAaC,EAAe,CAErCn1C,OAAK2B,OACDhF,EAAEvB,MAAMjF,QAAU,GAClB,WAAM,MAAA,yCAAyCwG,EAAEvB,MAAMjF,UAE3D,IAAMuB,EAAOkyB,aAAW8L,gBAAgBjD,EAAQ2iB,EAAMC,GAEhDrhB,EAASh3B,GAAM,CAAC0C,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkR,MAAO+I,EAAQ/6B,UACnEjD,EACIykB,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGq3B,GAASzb,UAASC,MAAO,CAACpd,MAAO45C,KAC1Dz8B,EAAQ7R,YAAYstB,EAAO9tB,YACtB,CAEL,GAD2BqS,EAAQiF,mBAAmB,CAAC7gB,IAC/B,CACtB,IAAM4K,EAASgR,EAAQzO,SAASnN,EAAEuJ,QAC5BsgB,EAAO3tB,SAAO8D,EAAEvB,MAAOuB,EAAER,MAAOoL,GAChCiiB,EACFe,GAAoBwqB,EAAkBvuB,EAAM6uB,EAAU5iB,GAC1Dh+B,EAAS8jB,EAAQ9O,eAAeurC,EAAYr4C,EAAER,MAAOqtB,EAAajiB,YAC7D,CACL,IAAM/L,EAAU,IAAI64C,GAAoBU,GAClCp8B,EACF,CAAC,CAAC1Z,KAAM,QAAS8D,KAAM0vB,GAAS,CAACxzB,KAAM,QAAS8D,KAAMsyC,IAG1D5gD,EAASykB,GACL,CAACxZ,OAAQ,CAAC/C,EAHR6sB,EACFjR,EAAQK,iBAAiBpd,EAAS,CAACmB,GAAIA,EAAER,MAAOwc,IAEpBJ,UAASC,MAAO,CAACpd,MAAO45C,KACxDz8B,EAAQ7R,YAAY8iB,EAAatjB,SAIrC,OAAOzR,CACT,GC/CO,IAAM6gD,GAAmC,CAC9Cx8B,WAAYy8B,eACZv8B,YAAa,SACbC,oBA9B2BX,GAKpB,IAAA5Y,WAAQ6Y,YAASC,UAEtB6I,cACAC,gBACAC,YACAC,aACAC,aACAC,2BAEK3e,SAAM2nB,eACP8qB,EAAQj9B,EAAQzO,SAAS/G,EAAKmD,QAC9BuvC,EAAcl9B,EAAQzO,SAAS4gB,EAAWxkB,QAE1CjI,2BAACslB,OAAQJ,OAGf,MAAO,CACL5K,EAAQ9O,eAAe,CAAC8Z,EAAOptB,QAAS,SAAUotB,GAClDhL,EAAQ9O,eAAeihB,EAAWtvB,MAAO,QAAS+nB,GAEtD,GC1BauyB,GAAOv4B,GAAgB,CAACC,OAAQhM,EAAYoD,OAE5CmhC,GAA2B,CACtC78B,WAAY88B,OACZ58B,YAAa,SACbC,WAAYy8B,kBCcZ,WAAYt6C,GALZvF,mBAAgB,CAAC,IAAK,WAEtBA,mBAA0C,CAAC,IAAK,EAAG,GACnDA,WAAO,EAGLA,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKsH,SAAW,sFAEhBtH,KAAKmY,UAAY,cAGnB6nC,wBAAA,WAqEE,MApEiB,aACXj+B,EAAK,s/EAiFb,WAAYxc,GALZvF,mBAAgB,CAAC,IAAK,WAEtBA,mBAA0C,CAAC,IAAK,EAAG,GACnDA,WAAO,EAGLA,KAAKoM,YAAc7G,EACnBvF,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAKhDlG,KAAKsH,SAAW,6CAChBtH,KAAKmY,UAAY,eAGnB8nC,wBAAA,WA6DE,MA5DiB,aACXl+B,EAAK,q4ECzHf,SAASm+B,GACLx9B,EAAwB/O,GACP,OAAfA,GACF+O,EAAQ7R,YAAY8C,EAAWtD,OAEnC,CAEA,SAAS8vC,GAAcC,GAErB,IADA,IAAIC,EAAO,EACJA,EAAOD,GACZC,GAAQ,EAEV,OAAOA,CACT,CAuIO,IAAMC,GAA2B,CACtCr9B,WAAYs9B,OACZp9B,YAAa,SACbC,oBArIEX,GAEK,IAAA5Y,WAAQ6Y,YAASC,UACjB7b,MACAsnB,MAAG8G,WAEJlD,EAASlrB,EAAEvB,MACX4vB,EAAUnD,EAAOA,EAAO1xB,OAAS,GAEvC,GAAIoiB,EAAQiF,mBAAmB,CAAC7gB,IAAK,CACnC,IAAMirB,EAAQrP,EAAQzO,SAASnN,EAAEuJ,QAC3BjI,2BAACitB,OAAaC,OAGpB,MAAO,CACL5S,EAAQ9O,eACJyhB,EAAY9vB,MAAO8vB,EAAY/uB,MAAO+uB,EAAY3jB,QACtDgR,EAAQ9O,eACJ0hB,EAAe/vB,MAAO+vB,EAAehvB,MAAOgvB,EAAe5jB,SAInE,GAAU,IAAN0c,EAEF,OADA4D,EAAOA,EAAO1xB,OAAS,GAAK,EACrB,CACLoiB,EAAQ9O,eAAeoe,EAAQlrB,EAAER,MAAO,IACxCoc,EAAQ9O,eAAeoe,EAAQ,QAAS,KAI5C,GAAgB,IAAZmD,EACF,MAAO,CACLruB,EAAG0b,GAAK,CAACG,MAAO,CAACpd,MAAOysB,EAAQ1rB,MAAO,QAAS/H,MAAO,GAAImkB,aAyC/D,IApCA,IACM0S,EADQjrB,OAAK6L,cAAcgc,GACXmD,EAChBqrB,EAAMn9B,GAAQ,CAACxZ,OAAQ,CAAC/C,KAAI6b,MAAO,CAACpd,MAAO,CAAC6vB,EAAOD,IAAWzS,YAE9D+9B,EAAQN,GAAc/xB,GACtBsyB,EAAcP,GAAchrB,GAM9BjC,EAAsB,KAKpBytB,EAAY,WAAM,OAAY,OAAZztB,EAAmB,CAACstB,EAAKA,GAAO,CAACA,EAAKttB,IAExD0tB,EAAU,SAACC,EAAaC,EAAav7C,GACzC,IAAMsE,EAAS82C,IACTh7C,EAAU,IAAIq6C,GAAYz6C,GAE1Bw7C,EAAkB,CACpB,CAAC33C,KAAM,QAAS8D,KAAM,CAACioB,IACvB,CAAC/rB,KAAM,QAAS8D,KAAM,CAHI,OAAZgmB,EAAmB,EAAI,IAIrC,CAAC9pB,KAAM,UAAW8D,KAAM,CAACiN,OAAOue,oBAChC,CAACtvB,KAAM,QAAS8D,KAAM,CAAC2zC,IACvB,CAACz3C,KAAM,QAAS8D,KAAM,CAAC4zC,KAErBE,EAAc9tB,EACpBA,EAAUxQ,EAAQK,iBACdpd,EAASkE,EAAQ,QAASk3C,GAC9Bb,GAAoCx9B,EAASs+B,IAItCC,EAAM,EAAGA,EAAMR,EAAOQ,GAAO,EAEpC,IADA,IAAMJ,EAAY,EAANI,EACHH,EAAMG,EAAKH,GAAO,EAAGA,GAAO,EACnCF,EAAQC,EAAKC,EAAK,CAAC1rB,EAAOsrB,IAK9B,IAAK,IAAIjQ,EAAciQ,EAAajQ,EAAcgQ,EAAOhQ,GAAe,EAAG,CACzE,IAAMroB,EAASu4B,IACTO,EAAe,IAAIjB,GAAa,CAAC7qB,EAAOqb,EAAc,IAEtD0Q,EAAmB,CACrB,CAAC/3C,KAAM,QAAS8D,KAAM,CAACioB,IACvB,CAAC/rB,KAAM,QAAS8D,KAAM,CAHI,OAAZgmB,EAAmB,EAAI,IAIrC,CAAC9pB,KAAM,QAAS8D,KAAM,CAACuzC,KAErBW,EAAcluB,EACpBA,EAAUxQ,EAAQK,iBACdm+B,EAAc94B,EAAQ,QAAS+4B,GACnCjB,GAAoCx9B,EAAS0+B,GAK7C,IADMP,EAAY,GADZI,EAAMR,EAAQ,GAEXK,EAAMG,EAAKH,GAAO,EAAGA,GAAO,EACnCF,EAAQC,EAAKC,EAAK5tB,EAAQ3tB,OAK9B,IAAIy7C,EAAc9tB,EAClBA,EAAU/rB,GACN,CAAC0C,OAAQ,CAAC/C,EAAGosB,GAAUxQ,UAASC,MAAO,CAACkR,MAAO,EAAGhyB,KAAM,CAACuzB,EAAOhH,MACpE8xB,GAAoCx9B,EAASs+B,GAG7C,IAAItvC,EAASy+B,GACT,CAACtmC,OAAQ,CAAC/C,EAAG05C,EAAKttB,WAAUxQ,UAASC,MAAO,CAACsU,KAAM,EAAGmZ,UAAW,KACrE8P,GAAoCx9B,EAAS89B,GAI7C,IAAM13B,EAAWkJ,EAAO7qB,MAAM,GAAI,GAClC2hB,EAASvoB,KAAK6tB,GAEd4yB,EAAc9tB,EACdA,EAAU7P,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGosB,GAAUvQ,MAAO,CAACpd,MAAOujB,GAAWpG,YACnEw9B,GAAoCx9B,EAASs+B,GAE7C,IAAMK,EAAa3vC,EAInB,OAHAA,EAAS2R,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAG4K,GAASiR,MAAO,CAACpd,MAAOujB,GAAWpG,YACjEw9B,GAAoCx9B,EAAS2+B,GAEtC,CAAC3vC,EAAQwhB,EAClB,iBC/IE,WAAYhrB,GATZlI,mBAAgB,CAAC,QAAS,cAE1BA,cAAW,gEAIXA,mBAA0C,CAAC,GAAI,EAAG,GAClDA,WAAO,EAGLA,KAAKoM,YAAclE,EACnBlI,KAAKmI,eAAiB4E,EAAmB/M,KAAKoM,aAC9CpM,KAAK+L,SAAWI,EACZnM,KAAKmI,eAAgBnI,KAAKoM,YAAapM,KAAKkG,eAChDlG,KAAKmY,UAAY,mBAGnBmpC,wBAAA,WAsHE,MArHiB,w5EAgETv/B,EAAK,+2ECrCV,IAAMw/B,GAAgC,CAC3Ct+B,WAAYu+B,YACZr+B,YAAa,SACbC,oBA/CwBX,GAKjB,IAaHg/B,EAbG53C,WAAQ6Y,YAASC,UACjB6jB,UAAOkb,eACPC,kBAAeC,aAAUnJ,cAAWrsC,gBAErChE,eAACgtB,OAAOysB,OAAaC,OAAYxV,OACjCjkC,uBAACw7B,OAAW9B,OAMZp8B,EAAU,IAAI27C,GAHhB,CAAClsB,EAAOyO,EAAW9B,EAClBuK,IAGCyV,EAAwC,YAAlBJ,EAA8B,EAAI,EAE9D,OAAQC,GACN,IAAK,WAYL,QACEH,EAAa,EACb,MAXF,IAAK,UACHA,EAAa,EACb,MACF,IAAK,OACHA,EAAa,EACb,MACF,IAAK,UACHA,EAAa,EAMjB,IAAM3+B,EAAc,CAClB,CAAC1Z,KAAM,QAAS8D,KAAM,CAAC60C,IACvB,CAAC34C,KAAM,QAAS8D,KAAM,CAACu0C,IAAc,CAACr4C,KAAM,UAAW8D,KAAM,CAACurC,KAEhE,OAAO/1B,EAAQK,iBACXpd,EAAS,CAAC6gC,EAAOkb,GAAa,UAAW5+B,EAC/C,GCIO,UCwDDk/B,GAAgC,CACpC/7B,GACA+P,GACAI,GACAM,GACAuB,GACAU,GACAI,GACA+C,GACAM,GACAc,GACAwB,GACAa,GACAI,GACAxY,GACAua,GACAqC,GACAa,GACAI,GACAG,GACAmB,GACAuB,GACAE,GACAS,GACAY,GACAQ,GACAiB,GACAI,GACAE,GACAM,GACAE,GACA3oB,GACA+oB,GACAW,GACAT,GACAG,GACAqC,GACAO,GACAE,GACAO,GACAqB,GACAI,GACAI,GACAzqB,GACAqZ,GACAsR,GACAE,GACAI,GACAI,GACAE,GACAG,GACAG,GACAlY,GACAsY,GACAE,GACArY,GACAuY,GACAI,GACAS,GACAtK,GACAwK,GACAE,GACAS,GACAtW,GACAkX,GACAG,GACAW,GACAG,GACAE,GACAG,GACAC,GACAvY,GACA0Y,GACAG,GACAG,GACAG,GACAh0B,GACAq0B,GACAO,GACAK,GACAQ,GACAgB,GACAQ,GACAG,GACAE,GACAG,GACApe,GACA2hB,GACAc,GACA/D,GACAQ,GACAc,GACAW,GACAM,GACAE,GACAG,GACA9C,GACA7R,GACAmW,GACAhD,GACAwD,GACAiB,GACAxpB,GD9JwC,CACxC9U,WAAYg/B,SACZ9+B,YAAa,SACbC,oBA9CEX,GAGK,IAAA5Y,WAAQ6Y,YAASC,UACjBpkB,UACF04B,SAEDA,EAAO,IACTA,GAAQ14B,EAAMgH,MAAMjF,QAStB,IANA,IAAMwG,EAAIvI,EACJu3B,EAAQhvB,EAAEvB,MAAMjF,OAEhB8/C,EAAM7hD,EAAMgH,MAAM0xB,GAClB/uB,EAAqB,IAAIzK,MAAMq4B,EAAQ,GACzCosB,EAAW,EACNthD,EAAI,EAAGA,EAAIk1B,EAAOl1B,IACrBA,IAAMq2B,IACR/uB,EAASg6C,KAAcp7C,EAAEvB,MAAM3E,IAInC,IAAMm5B,EAAY,GAEZlG,EAAQ,IAAIp2B,MAAMq4B,GAAOtT,KAAK,GAC9B3gB,EAAOiF,EAAEvB,MAAM4B,QACrBtF,EAAKo1B,GAAQ,EACb,IAAMztB,EAAoB,IAAI/L,MAAM2iD,GACpC,IAASx/C,EAAI,EAAGA,EAAI4I,EAAIlJ,OAAQM,IAAK,CACnCizB,EAAMoD,GAAQr2B,EACd,IAAMu9B,EAASh3B,GAAM,CAAC0C,OAAQ,CAAC/C,KAAI4b,UAASC,MAAO,CAACkR,QAAOhyB,UACrD07B,EACFla,GAAQ,CAACxZ,OAAQ,CAAC/C,EAAGq3B,GAASzb,UAASC,MAAO,CAACpd,MAAO2C,KAC1DsB,EAAI5I,GAAK28B,EAETxD,EAAUx5B,KAAK49B,GAIjB,OADApE,EAAUn2B,SAAQ,SAAAxE,GAAK,OAAAsjB,EAAQ7R,YAAYzR,EAAEiR,WACtC7G,CACT,GCkKE8rC,QAGF,IAA2B,IAAA6M,GAAAt8B,EAAAm8B,uCAAe,CAArC,IAAMI,YACTC,iBAAeD"}