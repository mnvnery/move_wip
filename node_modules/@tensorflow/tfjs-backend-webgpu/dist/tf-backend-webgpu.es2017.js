/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
(function (global, factory) {
    typeof exports === 'object' && typeof module !== 'undefined' ? factory(exports, require('@tensorflow/tfjs-core')) :
    typeof define === 'function' && define.amd ? define(['exports', '@tensorflow/tfjs-core'], factory) :
    (global = typeof globalThis !== 'undefined' ? globalThis : global || self, factory(global.tf = global.tf || {}, global.tf));
}(this, (function (exports, tf) { 'use strict';

    function _interopNamespace(e) {
        if (e && e.__esModule) return e;
        var n = Object.create(null);
        if (e) {
            Object.keys(e).forEach(function (k) {
                if (k !== 'default') {
                    var d = Object.getOwnPropertyDescriptor(e, k);
                    Object.defineProperty(n, k, d.get ? d : {
                        enumerable: true,
                        get: function () {
                            return e[k];
                        }
                    });
                }
            });
        }
        n['default'] = e;
        return n;
    }

    var tf__namespace = /*#__PURE__*/_interopNamespace(tf);

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const ENV = tf.env();
    /** The batched dispatching calls size in the device queue. */
    ENV.registerFlag('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE', () => 15);
    /**
     * Whether we forward execution to the CPU backend if tensors are small and
     * reside on the CPU.
     */
    ENV.registerFlag('WEBGPU_CPU_FORWARD', () => true);
    /**
     * This flag is used to test different types of matmul programs.
     *
     * See MatMulProgramType in webgpu_util.ts for a list of available values.
     */
    ENV.registerFlag('WEBGPU_MATMUL_PROGRAM_TYPE', () => -1);
    /**
     * Whether to use conv2dTranspose_naive which directly implement the
     * conv2dTranspose logic rather than using a matmul to simulate.
     */
    ENV.registerFlag('WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE', () => false);
    /**
     * Whether we use low power GPU. Otherwise, a high performance GPU will be
     * requested.
     */
    ENV.registerFlag('WEBGPU_USE_LOW_POWER_GPU', () => false);
    /**
     * Threshold for input tensor size that determines whether WebGPU backend will
     * delegate computation to CPU.
     *
     * Default value is 1000.
     */
    ENV.registerFlag('WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD', () => 1000);
    /**
     * Whether to use a dummy canvas to make profiling tools like PIX work with
     * TFJS webgpu backend.
     */
    ENV.registerFlag('WEBGPU_USE_PROFILE_TOOL', () => false);
    /**
     * Whether to use import API.
     */
    ENV.registerFlag('WEBGPU_IMPORT_EXTERNAL_TEXTURE', () => true);
    /**
     * Whether to use conv2dNaive for debugging.
     */
    ENV.registerFlag('WEBGPU_USE_NAIVE_CONV2D_DEBUG', () => false);

    /**
     * @license
     * Copyright 2022 Google LLC.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class AdapterInfo {
        constructor(adapterInfo) {
            if (adapterInfo) {
                this.vendor = adapterInfo.vendor;
            }
        }
        isIntel() {
            return this.vendor === 'intel';
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class BufferManager {
        constructor(device) {
            this.device = device;
            this.numUsedBuffers = 0;
            this.numFreeBuffers = 0;
            this.freeBuffers = new Map();
            this.usedBuffers = new Map();
            this.numBytesUsed = 0;
            this.numBytesAllocated = 0;
        }
        acquireUploadBuffer(size, usage) {
            return this.acquireBuffer(size, usage, true);
        }
        acquireBuffer(size, usage, mappedAtCreation = false) {
            const key = getBufferKey(size, usage);
            if (!this.freeBuffers.has(key)) {
                this.freeBuffers.set(key, []);
            }
            if (!this.usedBuffers.has(key)) {
                this.usedBuffers.set(key, []);
            }
            this.numBytesUsed += size;
            this.numUsedBuffers++;
            if (this.freeBuffers.get(key).length > 0) {
                this.numFreeBuffers--;
                const newBuffer = this.freeBuffers.get(key).shift();
                this.usedBuffers.get(key).push(newBuffer);
                return newBuffer;
            }
            this.numBytesAllocated += size;
            const newBuffer = this.device.createBuffer({ size, usage, mappedAtCreation });
            this.usedBuffers.get(key).push(newBuffer);
            return newBuffer;
        }
        releaseBuffer(buffer, size, usage) {
            if (this.freeBuffers.size === 0) {
                return;
            }
            const key = getBufferKey(size, usage);
            if (!this.freeBuffers.has(key)) {
                this.freeBuffers.set(key, []);
            }
            this.freeBuffers.get(key).push(buffer);
            this.numFreeBuffers++;
            this.numUsedBuffers--;
            const bufferList = this.usedBuffers.get(key);
            const bufferIndex = bufferList.indexOf(buffer);
            if (bufferIndex < 0) {
                throw new Error('Cannot release a buffer that was never provided by this ' +
                    'buffer manager');
            }
            bufferList.splice(bufferIndex, 1);
            this.numBytesUsed -= size;
        }
        releaseUploadBuffer(buffer, size, usage) {
            buffer.mapAsync(GPUMapMode.WRITE)
                .then(() => {
                this.releaseBuffer(buffer, size, usage);
            }, (err) => {
                // Do nothing;
            });
        }
        getNumUsedBuffers() {
            return this.numUsedBuffers;
        }
        getNumFreeBuffers() {
            return this.numFreeBuffers;
        }
        dispose() {
            this.freeBuffers.forEach((buffers, key) => {
                buffers.forEach(buffer => {
                    buffer.destroy();
                });
            });
            this.usedBuffers.forEach((buffers, key) => {
                buffers.forEach(buffer => {
                    buffer.destroy();
                });
            });
            this.freeBuffers = new Map();
            this.usedBuffers = new Map();
            this.numUsedBuffers = 0;
            this.numFreeBuffers = 0;
            this.numBytesUsed = 0;
            this.numBytesAllocated = 0;
        }
    }
    function getBufferKey(size, usage) {
        return `${size}_${usage}`;
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class TextureManager {
        constructor(device) {
            this.device = device;
            this.numUsedTextures = 0;
            this.numFreeTextures = 0;
            this.freeTextures = new Map();
            this.usedTextures = new Map();
            this.numBytesUsed = 0;
            this.numBytesAllocated = 0;
        }
        acquireTexture(width, height, format, usage) {
            const bytesPerElement = getBytesPerElement(format);
            const byteSize = width * height * bytesPerElement;
            const key = getTextureKey(width, height, format, usage);
            if (!this.freeTextures.has(key)) {
                this.freeTextures.set(key, []);
            }
            if (!this.usedTextures.has(key)) {
                this.usedTextures.set(key, []);
            }
            this.numBytesUsed += byteSize;
            this.numUsedTextures++;
            if (this.freeTextures.get(key).length > 0) {
                this.numFreeTextures--;
                const newTexture = this.freeTextures.get(key).shift();
                this.usedTextures.get(key).push(newTexture);
                return newTexture;
            }
            this.numBytesAllocated += byteSize;
            const newTexture = this.device.createTexture({
                size: [width, height],
                format,
                usage,
            });
            this.usedTextures.get(key).push(newTexture);
            return newTexture;
        }
        releaseTexture(texture, width, height, format, usage) {
            if (this.freeTextures.size === 0) {
                return;
            }
            const key = getTextureKey(width, height, format, usage);
            if (!this.freeTextures.has(key)) {
                this.freeTextures.set(key, []);
            }
            this.freeTextures.get(key).push(texture);
            this.numFreeTextures++;
            this.numUsedTextures--;
            const textureList = this.usedTextures.get(key);
            const textureIndex = textureList.indexOf(texture);
            if (textureIndex < 0) {
                throw new Error('Cannot release a texture that was never provided by this ' +
                    'texture manager');
            }
            textureList.splice(textureIndex, 1);
            const bytesPerElement = getBytesPerElement(format);
            const byteSize = width * height * bytesPerElement;
            this.numBytesUsed -= byteSize;
        }
        getNumUsedTextures() {
            return this.numUsedTextures;
        }
        getNumFreeTextures() {
            return this.numFreeTextures;
        }
        dispose() {
            this.freeTextures.forEach((textures, key) => {
                textures.forEach(texture => {
                    texture.destroy();
                });
            });
            this.usedTextures.forEach((textures, key) => {
                textures.forEach(texture => {
                    texture.destroy();
                });
            });
            this.freeTextures = new Map();
            this.usedTextures = new Map();
            this.numUsedTextures = 0;
            this.numFreeTextures = 0;
            this.numBytesUsed = 0;
            this.numBytesAllocated = 0;
        }
    }
    function getTextureKey(width, height, format, usage) {
        return `${width}_${height}_${format}_${usage}`;
    }
    function getBytesPerElement(format) {
        if (format === 'rgba8unorm') {
            return 16;
        }
        else {
            throw new Error(`${format} is not supported!`);
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // Generates GLSL that computes strides.
    function symbolicallyComputeStrides(indicesArr, variableName) {
        if (Math.max(...indicesArr) > 3) {
            throw new Error('Cannot symbolically compute strides for rank > 4 tensor.');
        }
        const numCoords = indicesArr.length;
        const shape = indicesArr.map(d => `${variableName}[${d}]`);
        const strides = new Array(numCoords - 1);
        strides[numCoords - 2] = shape[numCoords - 1];
        for (let i = numCoords - 3; i >= 0; --i) {
            strides[i] = `(${strides[i + 1]} * ${shape[i + 1]})`;
        }
        return strides;
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const compileProgram = (device, program, inputsData, output) => {
        const outputData = { dtype: output.dtype, shape: output.shape };
        const source = makeShader(inputsData, outputData, program);
        const module = device.createShaderModule({ code: source, label: program.constructor.name });
        const pipeline = device.createComputePipeline({
            compute: { module, entryPoint: '_start' },
            label: program.constructor.name,
            layout: 'auto'
        });
        return pipeline;
    };
    function getCoordsDataType(rank) {
        if (rank <= 1) {
            return 'i32';
        }
        else if (rank === 2) {
            return `vec2<i32>`;
        }
        else if (rank === 3) {
            return `vec3<i32>`;
        }
        else if (rank === 4) {
            return `vec4<i32>`;
        }
        else if (rank === 5) {
            return `vec5`;
        }
        else if (rank === 6) {
            return `vec6`;
        }
        else {
            throw Error(`GPU for rank ${rank} is not yet supported`);
        }
    }
    function getCoordsXYZ(index) {
        if (index === 0) {
            return 'x';
        }
        else if (index === 1) {
            return 'y';
        }
        else if (index === 2) {
            return 'z';
        }
        else if (index === 3) {
            return 'w';
        }
        else if (index === 4) {
            return 'u';
        }
        else if (index === 5) {
            return 'v';
        }
        else {
            throw Error(`Index ${index} is not yet supported`);
        }
    }
    function getMainHeaderString(...params) {
        let snippet;
        switch (params.length) {
            case 0:
                snippet = `
        ${getWorkGroupSizeString()}
        fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
                  @builtin(global_invocation_id) GlobalId : vec3<u32>,
                  @builtin(num_workgroups) NumWorkgroups : vec3<u32>) {
          localId = LocalId;
          globalId = GlobalId;
          numWorkgroups = NumWorkgroups;
          main();
        }

        fn main()
      `;
                break;
            case 1:
                snippet = `
        ${getWorkGroupSizeString()}
        fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
                  @builtin(global_invocation_id) GlobalId : vec3<u32>,
                  @builtin(num_workgroups) NumWorkgroups : vec3<u32>) {
          localId = LocalId;
          globalId = GlobalId;
          numWorkgroups = NumWorkgroups;
          main(getGlobalIndex());
        }

        fn main(${params[0]} : i32)
      `;
                break;
            default:
                throw Error('Unreachable');
        }
        return snippet;
    }
    function getWorkGroupSizeString() {
        return `
  @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)
`;
    }
    function makeShader(inputInfo, outputData, program) {
        const prefixSnippets = [];
        prefixSnippets.push(`
      const workGroupSizeX = ${program.workGroupSize[0]}u;
      const workGroupSizeY = ${program.workGroupSize[1]}u;
      const workGroupSizeZ = ${program.workGroupSize[2]}u;

      var<private> localId: vec3<u32>;
      var<private> globalId: vec3<u32>;
      var<private> numWorkgroups: vec3<u32>;

      // Only used when the y/z dimension of workgroup size is 1.
      fn getGlobalIndex() -> i32 {
        ${isFlatDispatch(program) ?
        `  return i32(globalId.x);` :
        `  let localInvocationIndex = localId.z * workGroupSizeX * workGroupSizeY +
                   localId.y * workGroupSizeX + localId.x;
               let workGroupID = (globalId - localId)/vec3<u32>(
                   workGroupSizeX, workGroupSizeY, workGroupSizeZ);

               return i32((workGroupID.z * numWorkgroups.x * numWorkgroups.y +
                   workGroupID.y * numWorkgroups.x + workGroupID.x) *
                   (workGroupSizeX * workGroupSizeY * workGroupSizeZ) +
                   localInvocationIndex);
        `}
      }
    `);
        if (program.isFromPixels) {
            prefixSnippets.push(`
        struct Uniform {
          size            : i32,
          numChannels     : i32,
          outShapeStrides : vec2<i32>,
        };

        @group(0) @binding(0) var<storage, read_write> result: array<${mapToWgslTypes(outputData.dtype, program.isVec4)}>;
        @group(0) @binding(2) var<uniform> uniforms: Uniform;
      `);
            return [
                commonSnippet,
                prefixSnippets.join('\n'),
                getCoordsFromIndexSnippet(outputData.shape),
                program.getUserCode(),
            ].join('\n');
        }
        let uniformDeclaration = 'struct Uniforms { NAN : f32, ';
        program.variableNames.forEach((x, i) => {
            const perDataType = getCoordsDataType(inputInfo[i].shape.length);
            uniformDeclaration +=
                `${x.charAt(0).toLowerCase() + x.slice(1)}Shape : ${perDataType}, `;
        });
        const outputDataType = getCoordsDataType(outputData.shape.length);
        uniformDeclaration += `outShape : ${outputDataType}, `;
        const stridesLength = outputData.shape.length - 1;
        const stridesDataType = getCoordsDataType(stridesLength);
        uniformDeclaration += `
         outShapeStrides: ${stridesDataType}, `;
        if (program.size) {
            uniformDeclaration += 'size : i32, ';
        }
        if (program.uniforms) {
            uniformDeclaration += program.uniforms;
        }
        uniformDeclaration += '};';
        uniformDeclaration = insertAlignment(uniformDeclaration);
        prefixSnippets.push(uniformDeclaration);
        // Output buffer.
        if (program.atomic) {
            prefixSnippets.push(`
      @group(0) @binding(0) var<storage, read_write> result: array<atomic<i32>>;
    `);
        }
        else {
            prefixSnippets.push(`
      @group(0) @binding(0) var<storage, read_write> result: array<${mapToWgslTypes(outputData.dtype, program.isVec4)}>;
    `);
        }
        program.variableNames.forEach((x, i) => {
            prefixSnippets.push(`
      @group(0) @binding(${1 + i}) var<storage, read> ${x}: array<${program.variableTypes ?
            program.variableTypes[i] :
            mapToWgslTypes(inputInfo[i].dtype, program.isVec4)}>;
        `);
        });
        if (uniformDeclaration !== '') {
            prefixSnippets.push(`
      @group(0) @binding(${1 + program.variableNames.length}) var<uniform> uniforms: Uniforms;
      `);
        }
        const coordsSnippet = getOutputCoordsSnippet(outputData.shape, program.dispatchLayout);
        const sources = [
            commonSnippet, prefixSnippets.join('\n'),
            getCoordsFromIndexSnippet(outputData.shape), coordsSnippet,
            getOutputIndexFromCoordsSnippet(outputData.shape.length)
        ];
        if (!program.atomic) {
            sources.push(setOutputSnippet(outputData.shape, outputData.dtype, program.isVec4));
        }
        const inputSnippet = inputInfo
            .map((x, i) => getInputSnippet(x, outputData.shape, program.variableTypes ?
            (program.variableTypes[i] === 'vec4<f32>') :
            program.isVec4, program.dispatchLayout.x.length === outputData.shape.length))
            .join('\n');
        sources.push(inputSnippet);
        sources.push(program.getUserCode());
        const source = sources.join('\n');
        return source;
    }
    function makeShaderKey(program, shapes, inputsData, output) {
        let key = program.shaderKey;
        if (program.isFromPixels) {
            return key;
        }
        const types = inputsData.map(d => d.dtype).concat(output.dtype);
        const broadcastDims = inputsData.map(d => tf.backend_util.getBroadcastDims(d.shape, output.shape));
        const inputShapesEqualsOutShape = inputsData.map(d => tf.util.arraysEqual(d.shape, output.shape)).join('_');
        const broadcastDimsKey = broadcastDims.map(d => d.join('_')).join(';');
        const flatDispatchString = isFlatDispatch(program) ? 'flatDispatch' : '';
        key += '_' + (program.workGroupSize ? program.workGroupSize.join(',') : '') +
            shapes.map(shape => shape.length).join(',') + types.join(',') +
            program.variableNames.join(',') + broadcastDimsKey +
            inputShapesEqualsOutShape + flatDispatchString;
        return key;
    }
    const commonSnippet = `
  struct vec5 {x: i32, y: i32, z: i32, w: i32, u: i32};
  struct vec6 {x: i32, y: i32, z: i32, w: i32, u: i32, v: i32};

  // Checks whether coordinates lie within the bounds of the shape.
  fn coordsInBounds2D(coord : vec2<i32>, shape : vec2<i32>) -> bool {
    return all(coord >= vec2<i32>(0)) && all(coord < shape);
  }
  fn coordsInBounds3D(coord : vec3<i32>, shape : vec3<i32>) -> bool {
    return all(coord >= vec3<i32>(0)) && all(coord < shape);
  }
  fn coordsInBounds4D(coord : vec4<i32>, shape : vec4<i32>) -> bool {
    return all(coord >= vec4<i32>(0)) && all(coord < shape);
  }

  fn getIndexFromCoords1D(coord : i32, shape : i32) -> i32 {
    return coord;
  }
  fn getIndexFromCoords2D(coords : vec2<i32>, shape : vec2<i32>) -> i32 {
    return dot(coords, vec2<i32>(shape.y, 1));
  }
  fn getIndexFromCoords3D(coords : vec3<i32>, shape : vec3<i32>) -> i32 {
    return dot(coords, vec3<i32>(shape.y * shape.z, shape.z, 1));
  }
  fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {
    return dot(coords, vec4<i32>(
        shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));
  }
  fn getIndexFromCoords5D(coords : vec5, shape : vec5) -> i32 {
    let shapeStrides: vec5 = vec5(shape.y * shape.z * shape.w * shape.u, shape.z * shape.w * shape.u, shape.w * shape.u, shape.u, 1);
    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u;
  }
  fn getIndexFromCoords6D(coords : vec6, shape : vec6) -> i32 {
    let shapeStrides: vec6 = vec6(shape.y * shape.z * shape.w * shape.u * shape.v, shape.z * shape.w * shape.u * shape.v, shape.w * shape.u * shape.v, shape.u * shape.v, shape.v, 1);
    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u + coords.v*shapeStrides.v;
  }

  fn idiv(a: i32, b: i32, sign: f32) -> i32 {
    var res: i32 = a / b;
    let modulo: i32 = a % b;
    if (sign < 0. && modulo != 0) {
      res = res - 1;
    }
    return res;
  }

  // NaN defination in IEEE 754-1985 is :
  //   - sign = either 0 or 1.
  //   - biased exponent = all 1 bits.
  //   - fraction = anything except all 0 bits (since all 0 bits represents infinity).
  // https://en.wikipedia.org/wiki/IEEE_754-1985#Representation_of_non-numbers
  fn isnan(val: f32) -> bool {
    let floatToUint: u32 = bitcast<u32>(val);
    return (floatToUint & 0x7fffffffu) > 0x7f800000u;
  }
  fn isnanVec4(val : vec4<f32>) -> vec4<bool> {
    return vec4<bool>(isnan(val[0]), isnan(val[1]), isnan(val[2]), isnan(val[3]));
  }
`;
    /**
     * Derives logical coordinates from a flat index. Performs integer division
     * with each stride and decrements the index until the index equals the final
     * dimension coordinate.
     */
    function getCoordsFromIndexSnippet(shape) {
        const rank = shape.length;
        if (rank <= 1) {
            return `fn getCoordsFromIndex(index : i32) -> i32 { return index; }`;
        }
        const strides = tf.util.computeStrides(shape);
        const dtype = getCoordsDataType(rank);
        const coords = [];
        for (let i = 0; i < rank; i++) {
            coords.push(`d${i}`);
        }
        if (strides.length === 1) {
            return `    fn getCoordsFromIndex(index : i32) -> vec2<i32> {
      let d0 = index / uniforms.outShapeStrides; let d1 = index - d0 * uniforms.outShapeStrides;
      return vec2<i32>(d0, d1);
    }`;
        }
        let snippet;
        snippet = 'var index2 = index;' +
            strides
                .map((_, i) => {
                const line1 = `let ${coords[i]} = index2 / uniforms.outShapeStrides.${getCoordsXYZ(i)}`;
                const line2 = i === strides.length - 1 ?
                    `let ${coords[i + 1]} = index2 - ${coords[i]} * uniforms.outShapeStrides.${getCoordsXYZ(i)}` :
                    `index2 = index2 - ${coords[i]} * uniforms.outShapeStrides.${getCoordsXYZ(i)}`;
                return `${line1}; ${line2};`;
            })
                .join('');
        return `
    fn getCoordsFromIndex(index : i32) -> ${dtype} {
      ${snippet}
      return ${dtype}(${coords.join(',')});
    }
  `;
    }
    function getInputAtCoordsSnippet(inputInfo, isVec4) {
        const texName = inputInfo.name;
        const rank = inputInfo.shape.length;
        const type = getCoordsDataType(rank);
        const funcName = 'get' + texName.charAt(0).toUpperCase() + texName.slice(1);
        const dims = ['d0', 'd1', 'd2', 'd3', 'd4', 'd5'].slice(0, rank);
        const inputs = dims.map(d => `${d} : i32`).join(', ');
        if (rank < 1) {
            if (isVec4) {
                return `
        fn ${funcName}() -> vec4<f32> {
          return vec4<f32>(${texName}[0]);
        }
      `;
            }
            return `
      fn ${funcName}() ->f32 {
        return f32(${texName}[0]);
      }
    `;
        }
        const shapeStr = `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;
        let rankStr = `${rank}D`;
        if (rank === 0) {
            rankStr = '1D';
        }
        if (isVec4) {
            return `
      fn ${funcName}(${inputs}) -> vec4<f32> {
        return vec4<f32>(${texName}[getIndexFromCoords${rankStr}(${type}(${dims.join(',')}),
          ${shapeStr}) / 4]);
      }
      `;
        }
        return `
    fn ${funcName}(${inputs}) -> f32 {
      return f32(${texName}[getIndexFromCoords${rankStr}(${type}(${dims.join(',')}),
        ${shapeStr})]);
    }
   `;
    }
    function getInputByOutputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout) {
        const texName = inputInfo.name;
        const texFuncSnippet = texName.charAt(0).toUpperCase() + texName.slice(1);
        const funcName = 'get' + texFuncSnippet + 'ByOutput';
        const inRank = inputInfo.shape.length;
        const outRank = outShape.length;
        const type = getCoordsDataType(outRank);
        // If the inShape equals the outShape and the dispatch layout is flat, we can
        // directly use |gl_GlobalInvocationID.x| as the index and don't need coords
        // conversion between these two shapes.
        if (tf.util.arraysEqual(inputInfo.shape, outShape) && isFlatDispatchLayout) {
            if (isVec4) {
                return `
      fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
        return vec4<f32>(${texName}[globalIndex]);
      }

      fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {
        return vec4<f32>(${texName}[${outRank > 1 ? 'getOutputIndexFromCoords(coords)' : 'coords'} / 4]);
      }
      `;
            }
            else {
                return `
    fn ${funcName}Index(globalIndex : i32) -> f32 {
      return f32(${texName}[globalIndex]);
    }

    fn ${funcName}Coords(coords : ${type}) -> f32 {
      return f32(${texName}[${outRank > 1 ? 'getOutputIndexFromCoords(coords)' : 'coords'}]);
    }
    `;
            }
        }
        const broadcastDims = tf.backend_util.getBroadcastDims(inputInfo.shape, outShape);
        const rankDiff = outRank - inRank;
        let coordsSnippet = '';
        if (inRank === 0) {
            if (isVec4) {
                return `
    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
      return get${texFuncSnippet}();
    }

    fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {
      return get${texFuncSnippet}();
    }
  `;
            }
            return `
    fn ${funcName}Index(globalIndex : i32) -> f32{
      return get${texFuncSnippet}();
    }

    fn ${funcName}Coords(coords : ${type}) -> f32{
      return get${texFuncSnippet}();
    }
  `;
        }
        else {
            if (outRank < 2 && broadcastDims.length >= 1) {
                coordsSnippet = 'coords = 0;';
            }
            else {
                coordsSnippet =
                    broadcastDims.map(d => `coords.${getCoordsXYZ(d + rankDiff)} = 0;`)
                        .join('\n');
            }
        }
        let unpackedCoordsSnippet = '';
        if (outRank < 2 && inRank > 0) {
            unpackedCoordsSnippet = 'coords';
        }
        else {
            if (outRank > 1) {
                const coordsType = getCoordsDataType(inRank);
                const coordsValues = inputInfo.shape.map((s, i) => `coords.${getCoordsXYZ(i + rankDiff)}`)
                    .join(', ');
                unpackedCoordsSnippet = `${coordsType}(${coordsValues})`;
            }
            else {
                unpackedCoordsSnippet = 'coords';
            }
        }
        const shapeStr = `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;
        const rankStr = `${inRank}D`;
        if (isVec4) {
            return `
    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
      var coords = getCoordsFromIndex(globalIndex);
      ${coordsSnippet}
      return ${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr}) / 4];
    }

    fn ${funcName}Coords(coordsIn : ${type}) -> vec4<f32> {
      var coords = coordsIn;
      ${coordsSnippet}
      return ${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr}) / 4];
    }
  `;
        }
        return `
  fn ${funcName}Index(globalIndex : i32) -> f32 {
    var coords = getCoordsFromIndex(globalIndex);
    ${coordsSnippet}
    return f32(${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr})]);
  }

  fn ${funcName}Coords(coordsIn : ${type}) -> f32 {
    var coords = coordsIn;
    ${coordsSnippet}
    return f32(${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr})]);
  }
`;
    }
    function getInputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout) {
        let res = getInputAtCoordsSnippet(inputInfo, isVec4);
        const inShape = inputInfo.shape;
        if (inShape.length <= outShape.length) {
            res += getInputByOutputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout);
        }
        return res;
    }
    /**
     * Generates getOutputCoords() function that computes output coordinates from
     * dispatch geometry to reduce arithmetic.
     */
    function getOutputCoordsSnippet(outShape, dispatchLayout) {
        const { x, y = [], z = [] } = dispatchLayout;
        const outRank = outShape.length;
        const rank = x.length + y.length + z.length;
        // getOutputCoords is only meaningful when the output rank is same with
        // dispatch layout rank.
        if (rank !== outRank) {
            return '';
        }
        if (x.length === outRank) {
            const dtype = getCoordsDataType(outRank);
            const snippet = `fn getOutputCoords() -> ${dtype}{
    let globalIndex = getGlobalIndex();
    return getCoordsFromIndex(globalIndex);
  }
  `;
            return snippet;
        }
        let gatherDimensionsStr = '';
        const dims = [x, y, z];
        for (let i = 0; i < dims.length; i++) {
            const arr = dims[i];
            if (arr.length === 0) {
                continue;
            }
            if (arr.length === 1) {
                gatherDimensionsStr += `let d${arr[0]} = i32(globalId[${i}]);`;
            }
            else {
                const strides = symbolicallyComputeStrides(arr, 'uniforms.outShape');
                gatherDimensionsStr += `var index${i} = i32(globalId[${i}]);`;
                for (let j = 0; j < strides.length; j++) {
                    gatherDimensionsStr += `let d${arr[j]} = index${i} / ${strides[j]};`;
                    if (j === strides.length - 1) {
                        gatherDimensionsStr += `let d${arr[j + 1]} = ` +
                            `index${i} - d${arr[j]} * ${strides[j]};`;
                    }
                    else {
                        gatherDimensionsStr +=
                            `index${i} = index${i} - d${arr[j]} * ${strides[j]};`;
                    }
                }
            }
        }
        const dimensions = [];
        for (let i = 0; i < rank; i++) {
            dimensions.push(`d${i}`);
        }
        const dtype = getCoordsDataType(rank);
        let snippet = `fn getOutputCoords() -> ${dtype} {
  ${gatherDimensionsStr}
`;
        if (dimensions.length === 0) {
            snippet += `return ${dtype}(0); }`;
        }
        else {
            snippet += `return ${dtype}(${dimensions.join(',')}); }`;
        }
        return snippet;
    }
    function getOutputIndexFromCoordsSnippet(outRank) {
        let snippet = '';
        switch (outRank) {
            case 0:
            case 1:
                snippet += `
        fn getOutputIndexFromCoords(coords : i32) -> i32 {
          return coords;
        }
        `;
                break;
            case 2:
                snippet += `
        fn getOutputIndexFromCoords(coords : vec2<i32>) -> i32 {
          return dot(coords, vec2<i32>(uniforms.outShapeStrides, 1));
        }
        `;
                break;
            case 3:
                snippet += `
        fn getOutputIndexFromCoords(coords : vec3<i32>) -> i32 {
          return dot(coords, vec3<i32>(uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, 1));
        }
        `;
                break;
            case 4:
                snippet += `
        fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {
          return dot(coords, vec4<i32>(
            uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, uniforms.outShapeStrides.z, 1));
        }
        `;
                break;
            case 5:
                snippet += `
        fn getOutputIndexFromCoords(coords : vec5) -> i32 {
          return coords.x * uniforms.outShapeStrides.x +
              coords.y * uniforms.outShapeStrides.y +
              coords.z * uniforms.outShapeStrides.z +
              coords.w * uniforms.outShapeStrides.w +
              coords.u;
        }
        `;
                break;
            case 6:
                snippet += `
        fn getOutputIndexFromCoords(coords : vec6) -> i32 {
          return coords.x * uniforms.outShapeStrides.x +
              coords.y * uniforms.outShapeStrides.y +
              coords.z * uniforms.outShapeStrides.z +
              coords.w * uniforms.outShapeStrides.w +
              coords.u * uniforms.outShapeStrides.u +
              coords.v;
        }
        `;
                break;
            default:
                tf.util.assert(false, () => `Unsupported ${outRank}D shape`);
                break;
        }
        return snippet;
    }
    function isFlatDispatch(program) {
        return program.dispatch[1] === 1 && program.dispatch[2] === 1;
    }
    function mapToWgslTypes(type, isVec4) {
        if (type === 'float32') {
            return isVec4 ? 'vec4<f32>' : 'f32';
        }
        else if (type === 'int32') {
            return isVec4 ? 'vec4<i32>' : 'i32';
        }
        else if (type === 'bool') {
            // Type 'bool' cannot be used in storage class,
            // https://www.w3.org/TR/WGSL/#host-shareable-types.
            return isVec4 ? 'vec4<i32>' : 'i32';
        }
        return type;
    }
    function setOutputSnippet(outShape, outBufferType, isVec4) {
        const outRank = outShape.length;
        const wgslType = mapToWgslTypes(outBufferType, isVec4);
        let snippet;
        if (isVec4) {
            snippet = `fn setOutputAtIndex(flatIndex : i32, value : vec4<f32>) {
      result[flatIndex] = ${wgslType}(value);
    }
    fn setOutputAtIndexI32(flatIndex : i32, value : vec4<i32>) {
      result[flatIndex] = ${wgslType}(value);
    }`;
        }
        else {
            snippet = `fn setOutputAtIndex(flatIndex : i32, value : f32) {
      result[flatIndex] = ${wgslType}(value);
    }
    fn setOutputAtIndexI32(flatIndex : i32, value : i32) {
      result[flatIndex] = ${wgslType}(value);
    }`;
        }
        if (outRank >= 2) {
            const dims = ['d0', 'd1', 'd2', 'd3', 'd4', 'd5'].slice(0, outRank);
            const type = getCoordsDataType(outRank);
            if (isVec4) {
                snippet += `
      fn setOutputAtCoords(${dims.map(d => `${d} : i32`).join(', ')}, value : vec4<f32>) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));
        setOutputAtIndex(flatIndex / 4, value);
      }
      fn setOutputAtCoordsI32(${dims.map(d => `${d} : i32`).join(', ')}, value : vec4<i32>) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));
        setOutputAtIndexI32(flatIndex / 4, value);
      }
    `;
            }
            else {
                snippet += `
      fn setOutputAtCoords(${dims.map(d => `${d} : i32`).join(', ')}, value : f32) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));
        setOutputAtIndex(flatIndex, value);
      }
      fn setOutputAtCoordsI32(${dims.map(d => `${d} : i32`).join(', ')}, value : i32) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(', ')}));
        setOutputAtIndexI32(flatIndex, value);
      }
    `;
            }
        }
        return snippet;
    }
    function insertAlignment(uniformShader) {
        // insert alignment when current pattern is vec5 or vec6
        const curInsertRe = /(\w+)\s*:\s*vec(5|6)/g;
        uniformShader = uniformShader.replace(curInsertRe, (match) => {
            return '@align(16) ' + match;
        });
        // insert alignment when previous pattern is vec5 or vec6
        const preInsertRe = /vec(5|6)\s*,\s*(\w+)/g;
        uniformShader = uniformShader.replace(preInsertRe, (_, p1, p2) => {
            return `vec${p1}, @align(16) ${p2}`;
        });
        return uniformShader;
    }

    const arrayProduct = (arr) => {
        let product = 1;
        for (let i = 0; i < arr.length; i++) {
            product *= arr[i];
        }
        return product;
    };
    function tilesFitEvenlyIntoShape(tileSize, shape) {
        if (tileSize.length !== shape.length) {
            throw new Error(`Cannot compute whether rank ${tileSize.length}` +
                ` tiles fit evenly into rank ${shape.length} shape` +
                ` - ranks must match.`);
        }
        return shape.every((dim, dimIdx) => dim % tileSize[dimIdx] === 0);
    }
    // Computes dispatch geometry based on layout of output dimensions and
    // workGroupSize.
    function computeDispatch(layout, outputShape, workGroupSize = [1, 1, 1], elementsPerThread = [1, 1, 1]) {
        const [dispatchX, dispatchY, dispatchZ] = [
            Math.ceil(arrayProduct(layout.x.map(d => outputShape[d])) /
                (workGroupSize[0] * elementsPerThread[0])),
            layout.y ? Math.ceil(arrayProduct(layout.y.map(d => outputShape[d])) /
                (workGroupSize[1] * elementsPerThread[1])) :
                1,
            layout.z ? Math.ceil(arrayProduct(layout.z.map(d => outputShape[d])) /
                (workGroupSize[2] * elementsPerThread[2])) :
                1
        ];
        return [dispatchX, dispatchY, dispatchZ];
    }
    function computeWorkGroupInfoForMatMul(dimAOuter, dimInner, dimBOuter, transposeA = false) {
        // These are experimental values. Usually, we need to adjust the work group
        // size based on the input shapes to improve the EU occupancy.
        // TODO: WebGPU limits the maximum allowed shared memory size as 16K. To make
        // sure it doesn't exceed this limitations. Temporarily reduce the work group
        // size to [8, 8, 1] and the work per thread size is [4, 4, 1]. But we should
        // revisit it and find the balance between work group size and work per thread
        // size.
        const workGroupSize = [8, 8, 1];
        const elementsPerThread = [4, 4, 1];
        if (!transposeA) {
            if (dimAOuter <= 8) {
                elementsPerThread[1] = 1;
            }
            if (dimInner <= 16 && dimBOuter <= 16) {
                workGroupSize[0] = 4;
            }
        }
        return { workGroupSize, elementsPerThread };
    }
    function computeWorkGroupSizeForConv2d(layout, outputShape, isVec4 = false) {
        if (isVec4) {
            return [8, 8, 1];
        }
        const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));
        const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));
        // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.
        // These are experimental values. Usually, we need to adjust the work group
        // size based on the output shape. For example, when one dimension is smaller
        // than 4, it will be wasteful if we assign a larger size for this dimension,
        // which results lots of threads doing useless work and reduces parallelism
        // of hardware threads. But it is always a balance between work group size
        // and shared memory. If one dimension is too small, such as 1, shared memory
        // will won't be fully utilized.
        if (dim0 <= 4) {
            return [4, 16, 1];
        }
        if (dim1 <= 4) {
            return [16, 4, 1];
        }
        return [16, 16, 1];
    }
    function computeWorkPerThreadForConv2d(layout, outputShape, isVec4 = false) {
        if (isVec4) {
            return [4, 4, 1];
        }
        const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));
        const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));
        // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.
        // The following conditions correspond to the values set in
        // computeWorkGroupSizeForConv2d.
        if (dim0 <= 4) {
            return [1, 2, 1];
        }
        if (dim1 <= 4) {
            return [2, 1, 1];
        }
        return [2, 2, 1];
    }
    function flatDispatchLayout(shape) {
        return { x: shape.map((d, i) => i) };
    }
    function GPUBytesPerElement(dtype) {
        if (dtype === 'float32' || dtype === 'int32' || dtype === 'bool' ||
            dtype === 'string') {
            return 4;
        }
        else if (dtype === 'complex64') {
            return 8;
        }
        else {
            throw new Error(`Unknown dtype ${dtype}`);
        }
    }
    function ArrayBufferToTypedArray(data, dtype) {
        if (dtype === 'float32') {
            return new Float32Array(data);
        }
        else if (dtype === 'int32') {
            return new Int32Array(data);
        }
        else if (dtype === 'bool' || dtype === 'string') {
            return Uint8Array.from(new Int32Array(data));
        }
        else {
            throw new Error(`Unknown dtype ${dtype}`);
        }
    }
    function isWebGPUSupported() {
        return ((typeof window !== 'undefined') ||
            //@ts-ignore
            (typeof WorkerGlobalScope !== 'undefined')) &&
            !!navigator.gpu;
    }
    var MatMulProgramType;
    (function (MatMulProgramType) {
        MatMulProgramType[MatMulProgramType["MatMulReduceProgram"] = 0] = "MatMulReduceProgram";
        MatMulProgramType[MatMulProgramType["MatMulSplitKProgram"] = 1] = "MatMulSplitKProgram";
        MatMulProgramType[MatMulProgramType["MatMulSmallOutputSizeProgram"] = 2] = "MatMulSmallOutputSizeProgram";
        MatMulProgramType[MatMulProgramType["MatMulPackedProgram"] = 3] = "MatMulPackedProgram";
        MatMulProgramType[MatMulProgramType["MatMulMax"] = 4] = "MatMulMax";
    })(MatMulProgramType || (MatMulProgramType = {}));

    var webgpu_util = {
        __proto__: null,
        tilesFitEvenlyIntoShape: tilesFitEvenlyIntoShape,
        computeDispatch: computeDispatch,
        computeWorkGroupInfoForMatMul: computeWorkGroupInfoForMatMul,
        computeWorkGroupSizeForConv2d: computeWorkGroupSizeForConv2d,
        computeWorkPerThreadForConv2d: computeWorkPerThreadForConv2d,
        flatDispatchLayout: flatDispatchLayout,
        GPUBytesPerElement: GPUBytesPerElement,
        ArrayBufferToTypedArray: ArrayBufferToTypedArray,
        isWebGPUSupported: isWebGPUSupported,
        get MatMulProgramType () { return MatMulProgramType; }
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // Empirically determined constant used to determine size threshold for handing
    // off execution to the CPU.
    const CPU_HANDOFF_SIZE_THRESHOLD = tf.env().getNumber('WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD');
    // Reshape dispatch, not to exceed device limits.
    const reshapeDispatch = (device, program) => {
        const MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE = device.limits.maxComputeWorkgroupsPerDimension;
        const layout = program['dispatchLayout'];
        const dispatch = program['dispatch'];
        if (dispatch.every((d) => d <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE)) {
            return dispatch;
        }
        tf.util.assert(dispatch[0] > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE &&
            layout.y === undefined && layout.z === undefined, () => 'Dispatch size exceeds WebGPU limits in Y or Z dimension.');
        let dispatchAverage = Math.ceil(Math.sqrt(dispatch[0]));
        if (dispatchAverage > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE) {
            dispatchAverage = Math.ceil(Math.cbrt(dispatch[0]));
            tf.util.assert(dispatchAverage <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE, () => 'Total dispatch size exceeds WebGPU maximum.');
            return [dispatchAverage, dispatchAverage, dispatchAverage];
        }
        else {
            return [dispatchAverage, dispatchAverage, 1];
        }
    };
    class WebGPUBackend extends tf.KernelBackend {
        constructor(device, adapterInfo) {
            super();
            this.commandQueueOwnedIds = new WeakSet();
            this.dispatchNumberInEncoder = 0;
            this.disposed = false;
            this.downloadWaitMs = 0;
            this.tensorDataPendingDisposal = [];
            this.stagingPendingDisposal = [];
            this.uniformPendingDisposal = [];
            this.uploadWaitMs = 0;
            if (!isWebGPUSupported()) {
                throw new Error('WebGPU is not supported on this device');
            }
            this.pipelineCache = {};
            this.device = device;
            this.queue = device.queue;
            this.currentCommandEncoder = null;
            this.currentComputePass = null;
            this.supportTimeQuery = device.features.has('timestamp-query');
            this.adapterInfo = new AdapterInfo(adapterInfo);
            this.bufferManager = new BufferManager(this.device);
            this.textureManager = new TextureManager(this.device);
            this.tensorMap = new tf.DataStorage(this, tf.engine());
            if (this.supportTimeQuery) {
                this.querySet = this.device.createQuerySet({
                    type: 'timestamp',
                    count: 2,
                });
            }
            // Profiling tools like PIX needs this dummy canvas to
            // trigger capturing a frame.
            if (tf.env().getBool('WEBGPU_USE_PROFILE_TOOL')) {
                this.dummyCanvas = document.createElement('canvas');
                this.dummyCanvas.width = 1;
                this.dummyCanvas.height = 1;
                this.dummyContext = this.dummyCanvas.getContext('webgpu');
                this.dummyContext.configure({
                    device,
                    format: 'bgra8unorm',
                });
                document.body.appendChild(this.dummyCanvas);
            }
        }
        nextDataId() {
            return WebGPUBackend.nextDataId++;
        }
        floatPrecision() {
            return 32;
        }
        defaultGpuBufferUsage() {
            return GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC |
                GPUBufferUsage.COPY_DST;
        }
        /**
         * Dispose the memory if the dataId has 0 refCount. Return true if the memory
         * is released or memory is not managed in this backend, false if memory is
         * not cleared.
         * @param dataId
         * @oaram force Optional, remove the data regardless of refCount
         */
        disposeData(dataId, force = false) {
            if (this.tensorDataPendingDisposal.indexOf(dataId) >= 0) {
                return false;
            }
            if (!this.tensorMap.has(dataId)) {
                return true;
            }
            const tensorData = this.tensorMap.get(dataId);
            this.decRef(dataId);
            if (!force && tensorData.refCount > 0) {
                return false;
            }
            // complex is never in commandQueueOwnedIds
            if (this.commandQueueOwnedIds.has(dataId)) {
                this.tensorDataPendingDisposal.push(dataId);
                return false;
            }
            const { complexTensorInfos } = this.tensorMap.get(dataId);
            if (complexTensorInfos != null) {
                this.disposeData(complexTensorInfos.real.dataId, force);
                this.disposeData(complexTensorInfos.imag.dataId, force);
            }
            this.releaseResource(dataId);
            this.tensorMap.delete(dataId);
            return true;
        }
        memory() {
            return {
                numBytesInGPU: this.bufferManager.numBytesUsed,
                numBytesAllocatedInGPU: this.bufferManager.numBytesAllocated,
                unreliable: false
            };
        }
        releaseResource(dataId) {
            const tensorData = this.tensorMap.get(dataId);
            if (!tensorData || !tensorData.resourceInfo) {
                return;
            }
            if ('texture' in tensorData.resourceInfo) {
                const textureInfo = tensorData.resourceInfo;
                if (textureInfo.texture instanceof GPUTexture) {
                    this.textureManager.releaseTexture(textureInfo.texture, textureInfo.width, textureInfo.height, textureInfo.format, textureInfo.usage);
                }
                textureInfo.texture = null;
            }
            else {
                const bufferInfo = tensorData.resourceInfo;
                this.bufferManager.releaseBuffer(bufferInfo.buffer, bufferInfo.size, bufferInfo.usage);
                bufferInfo.buffer = null;
            }
            tensorData.resourceInfo = null;
        }
        /** Return refCount of a `TensorData`. */
        refCount(dataId) {
            if (this.tensorMap.has(dataId)) {
                const tensorData = this.tensorMap.get(dataId);
                return tensorData.refCount;
            }
            return 0;
        }
        /** Increase refCount of a `TensorData`. */
        incRef(dataId) {
            const tensorData = this.tensorMap.get(dataId);
            tensorData.refCount++;
        }
        /** Decrease refCount of a `TensorData`. */
        decRef(dataId) {
            if (this.tensorMap.has(dataId)) {
                const tensorData = this.tensorMap.get(dataId);
                tensorData.refCount--;
            }
        }
        write(values, shape, dtype) {
            if (dtype === 'complex64' && values != null) {
                throw new Error(`Cannot write to a complex64 dtype. ` +
                    `Please use tf.complex(real, imag).`);
            }
            const dataId = { id: this.nextDataId() };
            this.tensorMap.set(dataId, { dtype, shape, values, refCount: 1 });
            return dataId;
        }
        move(dataId, values, shape, dtype, refCount) {
            if (dtype === 'complex64') {
                throw new Error(`Cannot write to a complex64 dtype. ` +
                    `Please use tf.complex(real, imag).`);
            }
            this.tensorMap.set(dataId, { dtype, shape, values, refCount });
        }
        submitQueue() {
            this.ensureComputePassEnded();
            this.queue.submit([this.currentCommandEncoder.finish()]);
            this.currentCommandEncoder = null;
            this.dispatchNumberInEncoder = 0;
            this.commandQueueOwnedIds = new WeakSet();
            this.tensorDataPendingDisposal.forEach(d => {
                this.releaseResource(d);
                this.tensorMap.delete(d);
            });
            this.uniformPendingDisposal.forEach(d => this.bufferManager.releaseBuffer(d.buffer, d.size, d.usage));
            this.stagingPendingDisposal.forEach(d => this.bufferManager.releaseUploadBuffer(d.buffer, d.size, d.usage));
            this.tensorDataPendingDisposal = [];
            this.uniformPendingDisposal = [];
            this.stagingPendingDisposal = [];
        }
        ensureCommandEncoderReady() {
            if (!this.currentCommandEncoder) {
                this.currentCommandEncoder = this.device.createCommandEncoder();
            }
        }
        ensureComputePassEnded() {
            if (this.currentComputePass) {
                this.currentComputePass.end();
                this.currentComputePass = null;
            }
        }
        getComputePass() {
            if (!this.currentComputePass) {
                this.currentComputePass = this.currentCommandEncoder.beginComputePass();
            }
            return this.currentComputePass;
        }
        async getBufferData(buffer, size) {
            const staging = this.bufferManager.acquireBuffer(size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);
            this.ensureCommandEncoderReady();
            this.ensureComputePassEnded();
            this.currentCommandEncoder.copyBufferToBuffer(buffer, 0, staging, 0, size);
            this.submitQueue();
            await staging.mapAsync(GPUMapMode.READ);
            const values = staging.getMappedRange().slice(0);
            staging.unmap();
            if (staging != null) {
                this.bufferManager.releaseBuffer(staging, size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);
            }
            // Need to get texture from swapChain to enable profiling tool
            // to capture a frame
            if (tf.env().getBool('WEBGPU_USE_PROFILE_TOOL')) {
                tf.util.assert(this.dummyContext !== undefined, () => `Fail to get context for profiling tool`);
                this.dummyContext.getCurrentTexture();
            }
            return values;
        }
        convertAndCacheOnCPU(dataId, data) {
            const tensorData = this.tensorMap.get(dataId);
            this.releaseResource(dataId);
            tensorData.values = data;
            return tensorData.values;
        }
        // TODO: Remove once this is fixed:
        // https://github.com/tensorflow/tfjs/issues/1595
        readSync(dataId) {
            const tensorData = this.tensorMap.get(dataId);
            const { values } = tensorData;
            if (values == null) {
                throw new Error('WebGPU readSync is only available for CPU-resident tensors.');
            }
            return values;
        }
        async read(dataId) {
            if (!this.tensorMap.has(dataId)) {
                throw new Error(`Tensor ${dataId} was not registered!`);
            }
            const tensorData = this.tensorMap.get(dataId);
            const { values } = tensorData;
            if (values != null) {
                // TODO(xing.xu@intel.com): Merge backend_util.BackendValues and
                // backend_util.TypedArray.
                return this.convertAndCacheOnCPU(dataId, values);
            }
            // Download the values from the GPU.
            let vals;
            if (tensorData.dtype === 'complex64') {
                const ps = await Promise.all([
                    this.read(tensorData.complexTensorInfos.real.dataId),
                    this.read(tensorData.complexTensorInfos.imag.dataId)
                ]);
                const realValues = ps[0];
                const imagValues = ps[1];
                vals = tf.backend_util.mergeRealAndImagArrays(realValues, imagValues);
            }
            else {
                const bufferInfo = tensorData.resourceInfo;
                const data = await this.getBufferData(bufferInfo.buffer, bufferInfo.size);
                vals = ArrayBufferToTypedArray(data, tensorData.dtype);
            }
            this.convertAndCacheOnCPU(dataId, vals);
            return vals;
        }
        /**
         * Read tensor to a new GPUBuffer.
         * @param dataId The source tensor.
         */
        readToGPU(dataId) {
            const srcTensorData = this.tensorMap.get(dataId);
            const { values, dtype, shape, resourceInfo } = srcTensorData;
            if (dtype === 'complex64') {
                throw new Error('Does not support reading buffer for complex64 dtype.');
            }
            if (resourceInfo == null) {
                if (values != null) {
                    throw new Error('Data is not on GPU but on CPU.');
                }
                else {
                    throw new Error('There is no data on GPU or CPU.');
                }
            }
            const size = resourceInfo.size;
            const buffer = this.bufferManager.acquireBuffer(size, resourceInfo.usage);
            this.ensureCommandEncoderReady();
            this.ensureComputePassEnded();
            this.currentCommandEncoder.copyBufferToBuffer(resourceInfo.buffer, 0, buffer, 0, size);
            this.submitQueue();
            const tensorInfo = this.makeTensorInfo(shape, dtype);
            // Make engine track this tensor, so that we can dispose it later.
            const tensorRef = tf.engine().makeTensorFromTensorInfo(tensorInfo);
            const tensorData = this.tensorMap.get(tensorInfo.dataId);
            tensorData
                .resourceInfo = { size, usage: this.defaultGpuBufferUsage(), buffer };
            return { tensorRef, buffer, bufSize: size };
        }
        bufferSync(t) {
            const data = this.readSync(t.dataId);
            if (t.dtype === 'string') {
                try {
                    // Decode the bytes into string.
                    const strings = data.map(d => tf.util.decodeString(d));
                    return tf.buffer(t.shape, t.dtype, strings);
                }
                catch (_a) {
                    throw new Error('Failed to decode encoded string bytes into utf-8');
                }
            }
            return tf.buffer(t.shape, t.dtype, data);
        }
        async time(f) {
            if (!this.supportTimeQuery) {
                console.warn(`This device doesn't support timestamp-query extension. ` +
                    `Start Chrome browser with flag ` +
                    `--disable-dawn-features=disallow_unsafe_apis then try again. ` +
                    `Otherwise, zero will be shown for the kernel time when profiling ` +
                    `mode is enabled. Using performance.now is not workable for webgpu ` +
                    `since it doesn't support synchronous data read from GPU.`);
            }
            const oldActiveTimers = this.activeTimers;
            const newActiveTimers = [];
            let outerMostTime = false;
            if (this.programTimersStack == null) {
                this.programTimersStack = newActiveTimers;
                outerMostTime = true;
            }
            else {
                this.activeTimers.push(newActiveTimers);
            }
            this.activeTimers = newActiveTimers;
            f();
            const flattenedActiveTimerQueries = tf.util.flatten(this.activeTimers.map((d) => d.query))
                .filter(d => d != null);
            const flattenedActiveTimerNames = tf.util.flatten(this.activeTimers.map((d) => d.name))
                .filter(d => d != null);
            this.activeTimers = oldActiveTimers;
            if (outerMostTime) {
                this.programTimersStack = null;
            }
            const res = {
                uploadWaitMs: this.uploadWaitMs,
                downloadWaitMs: this.downloadWaitMs,
                kernelMs: null,
                wallMs: null
            };
            const kernelMs = await Promise.all(flattenedActiveTimerQueries);
            res['kernelMs'] = tf.util.sum(kernelMs);
            res['getExtraProfileInfo'] = () => kernelMs.map((d, i) => ({ name: flattenedActiveTimerNames[i], ms: d }))
                .map(d => `${d.name}: ${d.ms}`)
                .join(', ');
            this.uploadWaitMs = 0;
            this.downloadWaitMs = 0;
            return res;
        }
        makeTensorInfo(shape, dtype, values) {
            if (dtype === 'string' && values != null && values.length > 0 &&
                tf.util.isString(values[0])) {
                values = values.map(d => tf.util.encodeString(d));
            }
            const dataId = this.write(values, shape, dtype);
            return { dataId, shape, dtype };
        }
        tensorToBinding(tensor) {
            if (!tensor) {
                return null;
            }
            const tensorData = this.tensorMap.get(tensor.dataId);
            if ('texture' in tensorData.resourceInfo) {
                const info = tensorData.resourceInfo;
                if (info.texture instanceof GPUExternalTexture) {
                    return info.texture;
                }
                else {
                    return info.texture.createView();
                }
            }
            const bufferInfo = tensorData.resourceInfo;
            return { offset: 0, size: bufferInfo.size, buffer: bufferInfo.buffer };
        }
        async getQueryTime(query) {
            if (this.supportTimeQuery) {
                return this.getTimeFromQuerySet(query);
            }
            else {
                return 0;
            }
        }
        uploadToGPU(dataId) {
            const tensorData = this.tensorMap.get(dataId);
            // Already on the GPU.
            if (tensorData.resourceInfo) {
                return;
            }
            const size = GPUBytesPerElement(tensorData.dtype) *
                tf.util.sizeFromShape(tensorData.shape);
            const buffer = this.bufferManager.acquireBuffer(size, this.defaultGpuBufferUsage());
            tensorData
                .resourceInfo = { size, usage: this.defaultGpuBufferUsage(), buffer };
            if (tensorData.values) {
                const stagingBuffer = this.bufferManager.acquireUploadBuffer(size, GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC);
                const arrayBuffer = stagingBuffer.getMappedRange();
                if (tensorData.dtype === 'int32' || tensorData.dtype === 'bool') {
                    new Int32Array(arrayBuffer).set(tensorData.values);
                }
                else {
                    new Float32Array(arrayBuffer).set(tensorData.values);
                }
                stagingBuffer.unmap();
                this.ensureCommandEncoderReady();
                this.ensureComputePassEnded();
                this.currentCommandEncoder.copyBufferToBuffer(stagingBuffer, 0, buffer, 0, size);
                const stagingInfo = {
                    size,
                    usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC,
                    buffer: stagingBuffer
                };
                this.stagingPendingDisposal.push(stagingInfo);
                // TODO: WebGPU doesn't support read data synchronously from GPU to CPU.
                // So it will report error when switching backend from WebGPU to others.
                // There are two situations: 1) swithcing the backend after running a
                // model; 2) swithcing the backend within the model. Temporarilly keep the
                // values on CPU to solve the first issue.
                // tensorData.values = null;
            }
        }
        makeUniforms(programUniform) {
            let currentOffset = 0;
            let preLength = 0;
            const offsets = [];
            programUniform.forEach((d) => {
                if (d.data.length === 0) {
                    d.data = [1];
                }
                // https://www.w3.org/TR/WGSL/#alignof
                let baseAlignment;
                switch (d.data.length) {
                    case 1:
                        baseAlignment = 4;
                        break;
                    case 2:
                        baseAlignment = 8;
                        break;
                    case 3:
                        baseAlignment = 16;
                        break;
                    case 4:
                        baseAlignment = 16;
                        break;
                    case 5:
                        baseAlignment = 16;
                        break;
                    case 6:
                        baseAlignment = 16;
                        break;
                    default:
                        tf.util.assert(false, () => `Unsupported ${d.data.length}D shape`);
                }
                if (preLength === 5 || preLength === 6) {
                    baseAlignment = 16;
                }
                currentOffset = Math.ceil(currentOffset / baseAlignment) * baseAlignment;
                preLength = d.data.length;
                offsets.push(currentOffset);
                currentOffset += d.data.length * 4;
            });
            const arrayBuffer = new ArrayBuffer(currentOffset);
            programUniform.forEach((d, i) => {
                const offset = offsets[i];
                if (d.type === 'int32') {
                    new Int32Array(arrayBuffer, offset, d.data.length).set(d.data);
                }
                else if (d.type === 'uint32') {
                    new Uint32Array(arrayBuffer, offset, d.data.length).set(d.data);
                }
                else {
                    new Float32Array(arrayBuffer, offset, d.data.length).set(d.data);
                }
            });
            const uniformBuffer = this.bufferManager.acquireBuffer(currentOffset, GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM);
            this.queue.writeBuffer(uniformBuffer, 0, arrayBuffer, 0, currentOffset);
            const uniformInfo = {
                size: currentOffset,
                usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM,
                buffer: uniformBuffer
            };
            this.uniformPendingDisposal.push(uniformInfo);
            return { offset: 0, size: currentOffset, buffer: uniformBuffer };
        }
        runWebGPUProgram(program, inputs, outputDtype, programDefinedUniform, output) {
            if (!output) {
                output = this.makeTensorInfo(program.outputShape, outputDtype);
            }
            if (tf.util.sizeFromShape(output.shape) === 0) {
                // Short-circuit the computation since the result is empty (has 0 in its
                // shape).
                this.tensorMap.get(output.dataId).values =
                    tf.util.getTypedArrayFromDType(output.dtype, 0);
                return output;
            }
            this.uploadToGPU(output.dataId);
            program.dispatch = reshapeDispatch(this.device, program);
            // There are five kinds of uniforms: NAN, shapes, shape strides, program
            // size, program defined uniforms.
            let programUniform = [];
            let bufferShapes = [];
            if (!program.isFromPixels) {
                programUniform.push({ type: 'float32', data: [NaN] });
                bufferShapes = inputs.concat(output).map(d => d.shape);
                const uniformsType = 'int32';
                bufferShapes.map(d => {
                    programUniform.push({ type: uniformsType, data: d });
                });
                const strides = tf.util.computeStrides(output.shape);
                programUniform.push({ type: uniformsType, data: strides });
                if (program.size) {
                    const size = tf.util.sizeFromShape(program.outputShape);
                    programUniform.push({ type: uniformsType, data: [program.isVec4 ? size / 4 : size] });
                }
            }
            const inputsData = inputs.map((input, i) => {
                if (input.dtype === 'complex64') {
                    throw new Error(`GPGPUProgram does not support complex64 input. For complex64 ` +
                        `dtypes, please separate the program into real and imaginary ` +
                        `parts.`);
                }
                this.uploadToGPU(input.dataId);
                return {
                    // Returning dtype from tensorMap because it reflects dtype
                    // of underlying buffer, rather than abstract dtype.
                    dtype: this.tensorMap.get(input.dataId).dtype,
                    shape: input.shape,
                    name: program.variableNames[i]
                };
            });
            const key = makeShaderKey(program, bufferShapes, inputsData, output);
            let pipeline;
            if (key in this.pipelineCache) {
                pipeline = this.pipelineCache[key];
            }
            else {
                pipeline = compileProgram(this.device, program, inputsData, output);
                this.pipelineCache[key] = pipeline;
            }
            if (programDefinedUniform) {
                programUniform = [...programUniform, ...programDefinedUniform];
            }
            const bindings = [
                this.tensorToBinding(output), ...inputs.map(t => this.tensorToBinding(t)),
                this.makeUniforms(programUniform)
            ];
            const bindGroup = this.device.createBindGroup({
                layout: pipeline.getBindGroupLayout(0),
                entries: bindings.map((b, i) => ({ binding: i, resource: b })),
            });
            this.ensureCommandEncoderReady();
            const pass = this.getComputePass();
            const shouldTimeProgram = this.activeTimers != null;
            if (shouldTimeProgram) {
                if (this.supportTimeQuery) {
                    // tslint:disable-next-line:no-any
                    pass.writeTimestamp(this.querySet, 0);
                }
            }
            pass.setPipeline(pipeline);
            pass.setBindGroup(0, bindGroup);
            pass.dispatchWorkgroups(program.dispatch[0], program.dispatch[1], program.dispatch[2]);
            if (shouldTimeProgram) {
                if (this.supportTimeQuery) {
                    // tslint:disable-next-line:no-any
                    pass.writeTimestamp(this.querySet, 1);
                }
            }
            this.dispatchNumberInEncoder++;
            inputs.forEach(input => {
                this.commandQueueOwnedIds.add(input.dataId);
            });
            this.commandQueueOwnedIds.add(output.dataId);
            if (tf.env().get('WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE') <= this.dispatchNumberInEncoder) {
                this.submitQueue();
            }
            if (shouldTimeProgram) {
                this.activeTimers.push({
                    name: program.constructor.name,
                    query: this.getQueryTime(this.querySet)
                });
            }
            return output;
        }
        async getTimeFromQuerySet(querySet) {
            const queryBuffer = this.bufferManager.acquireBuffer(16, GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);
            const dst = this.bufferManager.acquireBuffer(16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);
            this.ensureCommandEncoderReady();
            this.ensureComputePassEnded();
            this.currentCommandEncoder.resolveQuerySet(querySet, 0, 2, queryBuffer, 0);
            this.currentCommandEncoder.copyBufferToBuffer(queryBuffer, 0, dst, 0, 16);
            this.submitQueue();
            await dst.mapAsync(GPUMapMode.READ);
            const arrayBuf = new BigUint64Array(dst.getMappedRange());
            const timeElapsedNanos = Number((arrayBuf[1] - arrayBuf[0]));
            dst.unmap();
            this.bufferManager.releaseBuffer(dst, 16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);
            this.bufferManager.releaseBuffer(queryBuffer, 16, GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);
            // Return milliseconds.
            return timeElapsedNanos / 1000000;
        }
        shouldExecuteOnCPU(inputs, sizeThreshold = CPU_HANDOFF_SIZE_THRESHOLD) {
            return tf.env().getBool('WEBGPU_CPU_FORWARD') &&
                inputs.every(input => this.tensorMap.get(input.dataId).resourceInfo == null &&
                    tf.util.sizeFromShape(input.shape) < sizeThreshold);
        }
        numDataIds() {
            return this.tensorMap.numDataIds() - this.tensorDataPendingDisposal.length;
        }
        dispose() {
            if (this.disposed) {
                return;
            }
            this.bufferManager.dispose();
            this.textureManager.dispose();
            this.disposed = true;
        }
    }
    WebGPUBackend.nextDataId = 0;

    /**
     * @license
     * Copyright 2022 Google Inc. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    if (isWebGPUSupported()) {
        tf.registerBackend('webgpu', async () => {
            // Remove it once we figure out how to correctly read the tensor data
            // before the tensor is disposed in profiling mode.
            tf.env().set('CHECK_COMPUTATION_FOR_ERRORS', false);
            const gpuDescriptor = {
                powerPreference: tf.env().get('WEBGPU_USE_LOW_POWER_GPU') ?
                    'low-power' :
                    'high-performance'
            };
            const adapter = await navigator.gpu.requestAdapter(gpuDescriptor);
            const adapterLimits = adapter.limits;
            const deviceDescriptor = {};
            const supportTimeQuery = adapter.features.has('timestamp-query');
            deviceDescriptor.requiredLimits = {
                'maxComputeWorkgroupStorageSize': adapterLimits.maxComputeWorkgroupStorageSize,
                'maxComputeWorkgroupsPerDimension': adapterLimits.maxComputeWorkgroupsPerDimension,
                'maxStorageBufferBindingSize': adapterLimits.maxStorageBufferBindingSize,
            };
            if (supportTimeQuery) {
                deviceDescriptor.requiredFeatures = ['timestamp-query'];
            }
            const device = await adapter.requestDevice(deviceDescriptor);
            // tslint:disable-next-line:no-any
            const adapterInfo = await adapter.requestAdapterInfo();
            return new WebGPUBackend(device, adapterInfo);
        }, 3 /*priority*/);
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    var BinaryOpType;
    (function (BinaryOpType) {
        BinaryOpType[BinaryOpType["MUL"] = 0] = "MUL";
        BinaryOpType[BinaryOpType["ADD"] = 1] = "ADD";
        BinaryOpType[BinaryOpType["ATAN2"] = 2] = "ATAN2";
        BinaryOpType[BinaryOpType["SUB"] = 3] = "SUB";
        BinaryOpType[BinaryOpType["DIV"] = 4] = "DIV";
        BinaryOpType[BinaryOpType["EQUAL"] = 5] = "EQUAL";
        BinaryOpType[BinaryOpType["GREATER"] = 6] = "GREATER";
        BinaryOpType[BinaryOpType["GREATER_EQUAL"] = 7] = "GREATER_EQUAL";
        BinaryOpType[BinaryOpType["LESS"] = 8] = "LESS";
        BinaryOpType[BinaryOpType["LESS_EQUAL"] = 9] = "LESS_EQUAL";
        BinaryOpType[BinaryOpType["LOGICAL_AND"] = 10] = "LOGICAL_AND";
        BinaryOpType[BinaryOpType["NOT_EQUAL"] = 11] = "NOT_EQUAL";
        BinaryOpType[BinaryOpType["SQUARED_DIFFERENCE"] = 12] = "SQUARED_DIFFERENCE";
        BinaryOpType[BinaryOpType["INT_DIV"] = 13] = "INT_DIV";
        BinaryOpType[BinaryOpType["POW"] = 14] = "POW";
        BinaryOpType[BinaryOpType["PRELU"] = 15] = "PRELU";
        BinaryOpType[BinaryOpType["MAX"] = 16] = "MAX";
        BinaryOpType[BinaryOpType["MIN"] = 17] = "MIN";
        BinaryOpType[BinaryOpType["COMPLEX_MULTIPLY_REAL"] = 18] = "COMPLEX_MULTIPLY_REAL";
        BinaryOpType[BinaryOpType["COMPLEX_MULTIPLY_IMAG"] = 19] = "COMPLEX_MULTIPLY_IMAG";
    })(BinaryOpType || (BinaryOpType = {}));
    const CHECK_NAN_SNIPPET = `
  if (isnan(a)) { return a; }
  if (isnan(b)) { return b; }
  `;
    const CHECK_NAN_SNIPPET_VEC4_INNER = `
  if (isNaN.r) {
    resultTemp.r = valueForNaN;
  }
  if (isNaN.g) {
    resultTemp.g = valueForNaN;
  }
  if (isNaN.b) {
    resultTemp.b = valueForNaN;
  }
  if (isNaN.a) {
    resultTemp.a = valueForNaN;
  }
  `;
    const CHECK_NAN_SNIPPET_VEC4 = `
  let isNaN = isnanVec4(a) | isnanVec4(b);
  ${CHECK_NAN_SNIPPET_VEC4_INNER}
  `;
    const ADD = 'return a + b;';
    // (Ar + Ai)(Br + Bi) =
    // ArBr + ArBi + AiBr + AiBi = ArBr - AB + ArBi + AiBr
    // Yr = ArBr - AB
    // Yi = ArBi + AiBr
    const COMPLEX_MULTIPLY_REAL = 'return areal * breal - aimag * bimag;';
    const COMPLEX_MULTIPLY_IMAG = 'return areal * bimag + aimag * breal;';
    const DIV = 'return a / b;';
    const MUL = 'return a * b;';
    const SQUARED_DIFFERENCE = 'return (a - b) * (a - b);';
    const SUB = 'return a - b;';
    const EQUAL = 'return f32(a == b);';
    const EQUAL_VEC4 = 'return vec4<f32>(a == b);';
    const GREATER = 'return f32(a > b);';
    const GREATER_VEC4 = 'return vec4<f32>(a > b);';
    const GREATER_EQUAL = 'return f32(a >= b);';
    const GREATER_EQUAL_VEC4 = 'return vec4<f32>(a >= b);';
    const LESS = 'return f32(a < b);';
    const LESS_VEC4 = 'return vec4<f32>(a < b);';
    const LESS_EQUAL = 'return f32(a <= b);';
    const LESS_EQUAL_VEC4 = 'return vec4<f32>(a <= b);';
    const LOGICAL_AND = 'return f32(f32(a) >= 1.0 && f32(b) >= 1.0);';
    const LOGICAL_AND_VEC4 = `return (vec4<f32>(a >= vec4<f32>(1.0)) *
  vec4<f32>(b >= vec4<f32>(1.0)));`;
    const INT_DIV = `
  let s = sign(a) * sign(b);
  let ia = i32(round(a));
  let ib = i32(round(b));
  return f32(idiv(ia, ib, s));
  `;
    const INT_DIV_VEC4 = `
  let ia = vec4<i32>(round(a));
  let ib = vec4<i32>(round(b));
  let cond = ib != vec4<i32>(0);
  var resultTemp = vec4<i32>(0);
  let s = sign(a) * sign(b);

  // Windows (D3D) wants guaranteed non-zero int division at compile-time.
  if (cond[0]) {
    resultTemp[0] = idiv(ia[0], ib[0], s[0]);
  }
  if (cond[1]) {
    resultTemp[1] = idiv(ia[1], ib[1], s[1]);
  }
  if (cond[2]) {
    resultTemp[2] = idiv(ia[2], ib[2], s[2]);
  }
  if (cond[3]) {
    resultTemp[3] = idiv(ia[3], ib[3], s[3]);
  }
  return vec4<f32>(resultTemp);
  `;
    const NOT_EQUAL = `
  if (isnan(a) || isnan(b)) {
    return 1.0;
  }
  return f32(a != b);
`;
    const NOT_EQUAL_VEC4 = `
  var resultTemp = vec4<f32>(a != b);
  let valueForNaN = 1.0;
  ${CHECK_NAN_SNIPPET_VEC4}

  return resultTemp;
`;
    const POW = `
  if(a < 0.0 && floor(b) < b) {
    return uniforms.NAN;
  }
  if (b == 0.0) {
    return 1.0;
  }
  if (round(abs(b) % 2.0) != 1.0) {
    return pow(abs(a), b);
  }
  return sign(a) * pow(abs(a), b);
  `;
    const POW_VEC4 = `
  let isModRound1Bool = vec4<i32>(round(abs(b) % vec4<f32>(2.0))) == vec4<i32>(1);
  let isModRound1 = vec4<f32>(isModRound1Bool);
  let multiplier = sign(a) * isModRound1 + (vec4<f32>(1.0) - isModRound1);
  var resultTemp = multiplier * pow(abs(a), b);

  // Ensure that a^0 = 1, including 0^0 = 1 as this correspond to TF and JS
  let isExpZero = b == vec4<f32>(0.0);
  if (isExpZero.r) {
    resultTemp.r = 1.0;
  }
  if (isExpZero.g) {
    resultTemp.g = 1.0;
  }
  if (isExpZero.b) {
    resultTemp.b = 1.0;
  }
  if (isExpZero.a) {
    resultTemp.a = 1.0;
  }
  let isNaN = (a < vec4<f32>(0.0)) & (floor(b) < b);
  let valueForNaN = uniforms.NAN;
  ${CHECK_NAN_SNIPPET_VEC4_INNER}
  return resultTemp;
  `;
    const PRELU = `if (a < 0.0) { return b * a; }  return a;`;
    const PRELU_VEC4 = `
  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));
  return (aLessThanZero * (b * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);
  `;
    function getBinaryWithNanString(op, useVec4, valueForNaN = 'uniforms.NAN') {
        const checkNanSnippet = useVec4 ? CHECK_NAN_SNIPPET_VEC4 : CHECK_NAN_SNIPPET;
        return useVec4 ? `
    let valueForNaN = ${valueForNaN};
    var resultTemp = vec4<f32>(${op}(a, b));
    ` + checkNanSnippet +
            `
    return resultTemp;
  ` :
            checkNanSnippet + `
    return ${op}(a, b);
  `;
    }
    function getBinaryOpString(type, useVec4) {
        switch (type) {
            case BinaryOpType.MUL:
                return MUL;
            case BinaryOpType.ADD:
                return ADD;
            case BinaryOpType.ATAN2:
                return getBinaryWithNanString('atan2', useVec4);
            case BinaryOpType.SUB:
                return SUB;
            case BinaryOpType.DIV:
                return DIV;
            case BinaryOpType.EQUAL:
                return useVec4 ? EQUAL_VEC4 : EQUAL;
            case BinaryOpType.GREATER:
                return useVec4 ? GREATER_VEC4 : GREATER;
            case BinaryOpType.GREATER_EQUAL:
                return useVec4 ? GREATER_EQUAL_VEC4 : GREATER_EQUAL;
            case BinaryOpType.LESS:
                return useVec4 ? LESS_VEC4 : LESS;
            case BinaryOpType.LESS_EQUAL:
                return useVec4 ? LESS_EQUAL_VEC4 : LESS_EQUAL;
            case BinaryOpType.LOGICAL_AND:
                return useVec4 ? LOGICAL_AND_VEC4 : LOGICAL_AND;
            case BinaryOpType.NOT_EQUAL:
                return useVec4 ? NOT_EQUAL_VEC4 : NOT_EQUAL;
            case BinaryOpType.SQUARED_DIFFERENCE:
                return SQUARED_DIFFERENCE;
            case BinaryOpType.INT_DIV:
                return useVec4 ? INT_DIV_VEC4 : INT_DIV;
            case BinaryOpType.PRELU:
                return useVec4 ? PRELU_VEC4 : PRELU;
            case BinaryOpType.MAX:
                return getBinaryWithNanString('max', useVec4);
            case BinaryOpType.MIN:
                return getBinaryWithNanString('min', useVec4);
            case BinaryOpType.POW:
                return useVec4 ? POW_VEC4 : POW;
            case BinaryOpType.COMPLEX_MULTIPLY_REAL:
                return COMPLEX_MULTIPLY_REAL;
            case BinaryOpType.COMPLEX_MULTIPLY_IMAG:
                return COMPLEX_MULTIPLY_IMAG;
            default:
                throw new Error(`BinaryType ${type} is not implemented!`);
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    var UnaryOpType;
    (function (UnaryOpType) {
        UnaryOpType[UnaryOpType["ABS"] = 0] = "ABS";
        UnaryOpType[UnaryOpType["CEIL"] = 1] = "CEIL";
        UnaryOpType[UnaryOpType["COS"] = 2] = "COS";
        UnaryOpType[UnaryOpType["COSH"] = 3] = "COSH";
        UnaryOpType[UnaryOpType["ELU"] = 4] = "ELU";
        UnaryOpType[UnaryOpType["EXP"] = 5] = "EXP";
        UnaryOpType[UnaryOpType["EXPM1"] = 6] = "EXPM1";
        UnaryOpType[UnaryOpType["FLOOR"] = 7] = "FLOOR";
        UnaryOpType[UnaryOpType["IS_NAN"] = 8] = "IS_NAN";
        UnaryOpType[UnaryOpType["LINEAR"] = 9] = "LINEAR";
        UnaryOpType[UnaryOpType["LOG"] = 10] = "LOG";
        UnaryOpType[UnaryOpType["LOGICAL_NOT"] = 11] = "LOGICAL_NOT";
        UnaryOpType[UnaryOpType["NEG"] = 12] = "NEG";
        UnaryOpType[UnaryOpType["RELU"] = 13] = "RELU";
        UnaryOpType[UnaryOpType["RELU6"] = 14] = "RELU6";
        UnaryOpType[UnaryOpType["LEAKYRELU"] = 15] = "LEAKYRELU";
        UnaryOpType[UnaryOpType["RECIPROCAL"] = 16] = "RECIPROCAL";
        UnaryOpType[UnaryOpType["RSQRT"] = 17] = "RSQRT";
        UnaryOpType[UnaryOpType["SIN"] = 18] = "SIN";
        UnaryOpType[UnaryOpType["SINH"] = 19] = "SINH";
        UnaryOpType[UnaryOpType["SIGMOID"] = 20] = "SIGMOID";
        UnaryOpType[UnaryOpType["SQRT"] = 21] = "SQRT";
        UnaryOpType[UnaryOpType["SQUARE"] = 22] = "SQUARE";
        UnaryOpType[UnaryOpType["TANH"] = 23] = "TANH";
        UnaryOpType[UnaryOpType["TO_INT"] = 24] = "TO_INT";
    })(UnaryOpType || (UnaryOpType = {}));
    const ABS = `return abs(a);`;
    const CEIL = `return ceil(a);`;
    const COS = `return cos(a);`;
    const COSH = `
  let e2x = exp(-a);
  return (e2x + 1.0 / e2x) / 2.0;
`;
    const EXPM1 = `return exp(a) - 1.0;`;
    const ELU = `if (a >= 0.0) { return a; }  return (exp(a) - 1.0);`;
    const ELU_VEC4 = `
  var resFloat = exp(a) - vec4<f32>(1.0);
  if (a.r >= 0.0) {
    resFloat.r = a.r;
  }
  if (a.g >= 0.0) {
    resFloat.g = a.g;
  }
  if (a.b >= 0.0) {
    resFloat.b = a.b;
  }
  if (a.a >= 0.0) {
    resFloat.a = a.a;
  }
  return resFloat;
`;
    const EXP = `return exp(a);`;
    const FLOOR = `return floor(a);`;
    const IS_NAN = `return f32(isnan(a));`;
    const LINEAR = `return a;`;
    const LOG = `if (a < 0.0) { return uniforms.NAN; }
  return log(a);`;
    const LOGICAL_NOT = `return f32(!(a >= 1.0));`;
    const NEG = `return -a;`;
    const LEAKYRELU = `if (a < 0.0) { return uniforms.alpha * a; } return a;`;
    const LEAKYRELU_VEC4 = `
  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));
  return (aLessThanZero * (uniforms.alpha * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);
`;
    const RECIPROCAL = `return 1.0 / a;`;
    const RELU = `return select(a, 0.0, a < 0.0);`;
    const RELU6 = 'return clamp(a, 0.0, 6.0);';
    const RELU6_VEC4 = 'return clamp(a, vec4<f32>(0.0, 0.0, 0.0, 0.0), vec4<f32>(6.0, 6.0, 6.0, 6.0));';
    const RELU_VEC4 = `
  return select(a, vec4<f32>(0.0), a < vec4<f32>(0.0));
`;
    const RSQRT = `return 1.0/sqrt(a);`;
    const SIGMOID = `return 1.0 / (1.0 + exp(-1.0 * a));`;
    const SIN = `return sin(a);`;
    const SINH = `
  let e2x = exp(a);
  return (e2x - 1.0 / e2x) / 2.0;
`;
    const SQRT = `return sqrt(a);`;
    const SQUARE = `return a * a;`;
    const TANH = `
  let e2x = exp(-2.0 * abs(a));
  return sign(a) * (1.0 - e2x) / (1.0 + e2x);
`;
    const TO_INT = `return f32(i32((a)));`;
    function getUnaryOpString(type, useVec4) {
        switch (type) {
            case UnaryOpType.ABS:
                return ABS;
            case UnaryOpType.COS:
                return COS;
            case UnaryOpType.COSH:
                return COSH;
            case UnaryOpType.CEIL:
                return CEIL;
            case UnaryOpType.ELU:
                return useVec4 ? ELU_VEC4 : ELU;
            case UnaryOpType.EXP:
                return EXP;
            case UnaryOpType.EXPM1:
                return EXPM1;
            case UnaryOpType.FLOOR:
                return FLOOR;
            case UnaryOpType.IS_NAN:
                return IS_NAN;
            case UnaryOpType.LINEAR:
                return LINEAR;
            case UnaryOpType.LOG:
                return LOG;
            case UnaryOpType.LOGICAL_NOT:
                return LOGICAL_NOT;
            case UnaryOpType.NEG:
                return NEG;
            case UnaryOpType.LEAKYRELU:
                return useVec4 ? LEAKYRELU_VEC4 : LEAKYRELU;
            case UnaryOpType.RECIPROCAL:
                return RECIPROCAL;
            case UnaryOpType.RELU:
                return useVec4 ? RELU_VEC4 : RELU;
            case UnaryOpType.RELU6:
                return useVec4 ? RELU6_VEC4 : RELU6;
            case UnaryOpType.RSQRT:
                return RSQRT;
            case UnaryOpType.SIGMOID:
                return SIGMOID;
            case UnaryOpType.SIN:
                return SIN;
            case UnaryOpType.SINH:
                return SINH;
            case UnaryOpType.SQRT:
                return SQRT;
            case UnaryOpType.SQUARE:
                return SQUARE;
            case UnaryOpType.TANH:
                return TANH;
            case UnaryOpType.TO_INT:
                return TO_INT;
            default:
                throw new Error(`BinaryType ${type} is not implemented!`);
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const typeSnippet = (component) => {
        switch (component) {
            case 1:
                return 'f32';
            case 2:
                return 'vec2<f32>';
            case 3:
                return 'vec3<f32>';
            case 4:
                return 'vec4<f32>';
            default:
                throw new Error(`${component}-component is not supported.`);
        }
    };
    function activationFnSnippet(activation, hasPreluActivationWeights = false, packed = false, coordsLength = 3) {
        if (activation === null) {
            return '';
        }
        let activationOpSnippet = '';
        if (activation === 'linear') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.LINEAR);
        }
        else if (activation === 'relu') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.RELU, packed);
        }
        else if (activation === 'elu') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.ELU, packed);
        }
        else if (activation === 'relu6') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.RELU6, packed);
        }
        else if (activation === 'prelu') {
            activationOpSnippet = getBinaryOpString(BinaryOpType.PRELU, packed);
        }
        else if (activation === 'sigmoid') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.SIGMOID, packed);
        }
        else if (activation === 'leakyrelu') {
            activationOpSnippet = getUnaryOpString(UnaryOpType.LEAKYRELU, packed);
        }
        else {
            throw new Error(`Activation ${activation} has not been implemented for the WebGPU backend.`);
        }
        const elementSize = packed ? 4 : 1;
        const dataType = typeSnippet(elementSize);
        let activationFnSnippet = '';
        if (hasPreluActivationWeights) {
            activationFnSnippet = `
      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${dataType} {
        let b = getPreluActivationWeightsByOutputCoords(coords);
        ${activationOpSnippet}
      }`;
        }
        else {
            activationFnSnippet = `
      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${dataType} {
        ${activationOpSnippet}
      }`;
        }
        return activationFnSnippet;
    }
    function biasActivationSnippet(hasBias, activation) {
        return `
      ${hasBias ? 'value = value + getBiasByOutputCoords(coords);' : ''}
      ${activation ? 'value = activation(value, coords);' : ''}
      `;
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function matMulReadFnSource(batchAEqualOne, batchBEqualOne, transposeA, transposeB, fitAOuter = false, fitBOuter = false, fitInner = false, component = 1) {
        tf.util.assert(transposeA && component === 1 || !transposeA, () => `transposeA ${transposeA} is not compatible with component size ${component}`);
        const sampleA = `
      let batch = ${batchAEqualOne ? '0' : 'batchIn'};
      ${transposeA ? `value = getA(batch, col, row);` :
        `value = getA(batch, row, col);`}

    `;
        const sampleB = transposeB ? `value = getB(batch, col, row);` :
            `value = getB(batch, row, col);`;
        return `
  fn mm_readA(batchIn: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {
    var value = ${typeSnippet(component)}(0.0);
    let col = colIn * ${component};
    ${fitAOuter && fitInner ?
        sampleA :
        `
    ${transposeA ?
            `if(row < uniforms.dimAOuter && col < uniforms.dimInner)` :
            `if(row < uniforms.aShape[1] && col < uniforms.aShape[2])`}
    {
      ${sampleA}
    }
    `}
    return value;
  }

  fn mm_readB(batchIn: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {
    let col = colIn * ${component};
    let batch = ${batchBEqualOne ? '0' : 'batchIn'};
    var value = ${typeSnippet(component)}(0.0);
    ${sampleB}
    return value;
  }
  `;
    }
    function matMulReadWriteFnSource(hasBias, activation, batchAEqualOne, batchBEqualOne, transposeA, transposeB, fitAOuter = false, fitBOuter = false, fitInner = false, component = 1) {
        return `
  ${matMulReadFnSource(batchAEqualOne, batchBEqualOne, transposeA, transposeB, fitAOuter, fitBOuter, fitInner, component)}
  fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${typeSnippet(component)}) {
    let col = colIn * ${component};
    ${fitAOuter && fitBOuter ?
        '' :
        'if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)'}
    {
      var value = valueIn;
      let coords = vec3<i32>(batch, row, col);
      ${biasActivationSnippet(hasBias, activation)}
      setOutputAtCoords(coords[0], coords[1], coords[2], value);
    }
  }
  `;
    }
    const writeDataToSubAVec4Snippet = (transpose) => {
        if (transpose) {
            return `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          kStart + inputRow,
          globalRowStart / InnerElementSize + inputCol);
        `;
        }
        else {
            return `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          globalRow + innerRow,
          kStart / InnerElementSize + inputCol);
        `;
        }
    };
    const calculateResultSnippet = (transposeA, innerElementSize) => {
        if (transposeA) {
            return `
        let ACached0 = mm_Asub[k * InnerElementSize][localRow];
        let ACached1 = mm_Asub[k * InnerElementSize + 1][localRow];
        let ACached2 = mm_Asub[k * InnerElementSize + 2][localRow];
        ${innerElementSize === 3 ?
            '' :
            'let ACached3 = mm_Asub[k * InnerElementSize + 3][localRow];'}
        for (var i = 0; i < RowPerThread; i = i + 1) {
          acc[i] = BCached0 * ACached0[i] + acc[i];
          acc[i] = BCached1 * ACached1[i] + acc[i];
          acc[i] = BCached2 * ACached2[i] + acc[i];
          ${innerElementSize === 3 ?
            '' :
            'acc[i] = BCached3 * ACached3[i] + acc[i];'}
        }`;
        }
        else {
            return `
        for (var i = 0; i < RowPerThread; i = i + 1) {
          let ACached = mm_Asub[tileRow + i][k];
          acc[i] = BCached0 * ACached.x + acc[i];
          acc[i] = BCached1 * ACached.y + acc[i];
          acc[i] = BCached2 * ACached.z + acc[i];
          ${innerElementSize === 3 ? '' :
            'acc[i] = BCached3 * ACached.w + acc[i];'}
        }`;
        }
    };
    function makeMatMulPackedVec4Source(workPerThread, workGroupSize, transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32, isVectorA = false) {
        const tileAOuter = workGroupSize[1] * workPerThread[1];
        const tileBOuter = workGroupSize[0] * workPerThread[0];
        const tileAWidth = transposeA ? tileAOuter : tileInner;
        const tileAHight = transposeA ? tileInner : tileAOuter;
        const innerElementSize = tileAWidth / workGroupSize[0];
        const rowPerThreadB = tileInner / workGroupSize[1];
        tf.util.assert(((transposeA && innerElementSize === 4 && workPerThread[1] === 4) ||
            (!transposeA && (innerElementSize === 3 || innerElementSize === 4))) &&
            tileAWidth % workGroupSize[0] === 0 &&
            tileInner % workGroupSize[1] === 0 && workPerThread[0] === 4, () => `If transposeA ${transposeA} is true, innerElementSize ${innerElementSize} and workPerThread[1] ${workPerThread[1]} must be 4.
          Otherwise, innerElementSize ${innerElementSize} must be 3 or 4.
      tileAWidth ${tileAWidth} must be divisible by workGroupSize[0]${workGroupSize[0]}. tileInner ${tileInner} must be divisible by workGroupSize[1] ${workGroupSize[1]}. ColPerThread ${workPerThread[0]} must be 4.`);
        return `
  var<workgroup> mm_Asub : array<array<vec${innerElementSize}<f32>, ${tileAWidth / innerElementSize}>, ${tileAHight}>;
  var<workgroup> mm_Bsub : array<array<vec4<f32>, ${tileBOuter / workPerThread[0]}>, ${tileInner}>;

  const RowPerThread = ${workPerThread[1]};
  const ColPerThread = ${workPerThread[0]};
  const InnerElementSize = ${innerElementSize};
  const TileInner = ${tileInner};

  @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)
  fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
            @builtin(global_invocation_id) GlobalId : vec3<u32>,
            @builtin(num_workgroups) NumWorkgroups: vec3<u32>,
            @builtin(workgroup_id) workgroupId: vec3<u32>) {
    localId = LocalId;
    globalId = GlobalId;
    numWorkgroups = NumWorkgroups;

    let localRow = i32(localId.y);
    let tileRow = ${isVectorA ? '0' : 'localRow * RowPerThread'};
    let tileCol = i32(localId.x);

    let globalRow = ${isVectorA ? '0' : 'i32(globalId.y) * RowPerThread'};
    let globalCol = i32(globalId.x);
    let batch = ${splitK ? '0' : 'i32(globalId.z)'};
    let globalRowStart = i32(workgroupId.y) * ${tileAOuter};

    let numTiles = ${splitK ? `${Math.ceil(splitedDimInner / tileInner)}` :
        '(uniforms.dimInner - 1) / TileInner + 1'};
    var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : '0'};

    var acc: array<vec4<f32>, RowPerThread>;

    // Loop over shared dimension.
    let tileRowB = localRow * ${rowPerThreadB};
    for (var t = 0; t < numTiles; t = t + 1) {
        // Load one tile of A into local memory.
        for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
            let inputRow = tileRow + innerRow;
            let inputCol = tileCol;
            ${writeDataToSubAVec4Snippet(transposeA)}
        }

        // Load one tile of B into local memory.
        for (var innerRow = 0; innerRow < ${rowPerThreadB}; innerRow = innerRow + 1) {
            let inputRow = tileRowB + innerRow;
            let inputCol = tileCol;
            mm_Bsub[inputRow][inputCol] = mm_readB(batch, kStart + inputRow, globalCol);
        }
        kStart = kStart + TileInner;
        workgroupBarrier();

        // Compute acc values for a single thread.
        for (var k = 0; k < TileInner / InnerElementSize; k = k + 1) {
            let BCached0 = mm_Bsub[k * InnerElementSize][tileCol];
            let BCached1 = mm_Bsub[k * InnerElementSize + 1][tileCol];
            let BCached2 = mm_Bsub[k * InnerElementSize + 2][tileCol];
            ${innerElementSize === 3 ?
        '' :
        'let BCached3 = mm_Bsub[k * InnerElementSize + 3][tileCol];'}

            ${calculateResultSnippet(transposeA, innerElementSize)}
        }

        workgroupBarrier();
    }

    for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
        mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);
    }
  }`;
    }
    const writeDataToSubASnippet = (transpose) => {
        if (transpose) {
            return `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          kStart + inputRow,
          globalRowStart + inputCol);
        `;
        }
        else {
            return `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          globalRowStart + inputRow,
          kStart + inputCol);
        `;
        }
    };
    const readDataFromSubASnippet = (transposeA) => {
        return transposeA ? 'let ACached = mm_Asub[k][tileRow + innerRow];' :
            'let ACached = mm_Asub[tileRow + innerRow][k];';
    };
    // sequentialAccessByThreads means sequential data in memory is accessed by
    // threads, instead of a single thread (default behavior).
    function makeMatMulPackedSource(workPerThread, workGroupSize, transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32, sequentialAccessByThreads = false) {
        const tileAOuter = workPerThread[1] * workGroupSize[1];
        const tileBOuter = workPerThread[0] * workGroupSize[0];
        const tileAWidth = transposeA ? tileAOuter : tileInner;
        const tileAHight = transposeA ? tileInner : tileAOuter;
        tf.util.assert(tileAHight % workGroupSize[1] === 0 &&
            tileAWidth % workGroupSize[0] === 0 &&
            tileInner % workGroupSize[1] === 0, () => `tileAHight ${tileAHight} must be divisible by workGroupSize[1]${workGroupSize[1]}, tileAWidth ${tileAWidth} must be divisible by workGroupSize[0]${workGroupSize[0]}, tileInner ${tileInner} must be divisible by workGroupSize[1]${workGroupSize[1]}`);
        const rowPerThreadA = tileAHight / workGroupSize[1];
        const colPerThreadA = tileAWidth / workGroupSize[0];
        const rowPerThreadB = tileInner / workGroupSize[1];
        const matmulSnippet = sequentialAccessByThreads ?
            `
      let localRow = i32(localId.y);
      let localCol = i32(localId.x);
      let globalRowStart = i32(workgroupId.y) * ${tileAOuter};
      let globalColStart = i32(workgroupId.x) * ${tileBOuter};

      // Loop over shared dimension.
      for (var t = 0; t < numTiles; t = t + 1) {
        // Load one tile of A into local memory.
        for (var inputRow = localRow; inputRow < ${tileAHight}; inputRow = inputRow + ${workGroupSize[1]}) {
          for (var inputCol = localCol; inputCol < ${tileAWidth}; inputCol = inputCol + ${workGroupSize[0]}) {
            ${writeDataToSubASnippet(transposeA)}
          }
        }
        // Load one tile of B into local memory.
        for (var inputRow = localRow; inputRow < ${tileInner}; inputRow = inputRow + ${workGroupSize[1]}) {
              for (var inputCol = localCol; inputCol < ${tileBOuter}; inputCol = inputCol + ${workGroupSize[0]}) {
            mm_Bsub[inputRow][inputCol] = mm_readB(batch,
              kStart + inputRow,
              globalColStart + inputCol);
          }
        }
        kStart = kStart + TileInner;
        workgroupBarrier();

        // Compute acc values for a single thread.
        var BCached : array<f32, ColPerThread>;
        for (var k = 0; k < TileInner; k = k + 1) {
          for (var inner = 0; inner < ColPerThread; inner = inner + 1) {
            BCached[inner] = mm_Bsub[k][localCol + inner * ${workGroupSize[0]}];
          }
          for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
            let ACached = ${transposeA ?
            `mm_Asub[k][localRow + innerRow * ${workGroupSize[1]}];` :
            `mm_Asub[localRow + innerRow * ${workGroupSize[1]}][k];`}
            for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
              acc[innerRow][innerCol] = acc[innerRow][innerCol] +
                  ACached * BCached[innerCol];
            }
          }
        }
        workgroupBarrier();
      }
      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
        let gRow = globalRowStart + localRow + innerRow * ${workGroupSize[1]};
        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
          let gCol = globalColStart + localCol + innerCol * ${workGroupSize[0]};
          mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);
        }
      }
      ` :
            `
  let tileRow = i32(localId.y) * RowPerThread;
  let tileCol = i32(localId.x) * ColPerThread;

  let globalRow = i32(globalId.y) * RowPerThread;
  let globalCol = i32(globalId.x) * ColPerThread;
  let globalRowStart = i32(workgroupId.y) * ${tileAOuter};

  let tileRowA = i32(localId.y) * ${rowPerThreadA};
  let tileColA = i32(localId.x) * ${colPerThreadA};
  let tileRowB = i32(localId.y) * ${rowPerThreadB};
  // Loop over shared dimension.
  for (var t = 0; t < numTiles; t = t + 1) {
    // Load one tile of A into local memory.
    for (var innerRow = 0; innerRow < ${rowPerThreadA}; innerRow = innerRow + 1) {
      for (var innerCol = 0; innerCol < ${colPerThreadA}; innerCol = innerCol + 1) {
        let inputRow = tileRowA + innerRow;
        let inputCol = tileColA + innerCol;
        ${writeDataToSubASnippet(transposeA)}
      }
    }

    // Load one tile of B into local memory.
    for (var innerRow = 0; innerRow < ${rowPerThreadB}; innerRow = innerRow + 1) {
      for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
        let inputRow = tileRowB + innerRow;
        let inputCol = tileCol + innerCol;
        mm_Bsub[inputRow][inputCol] = mm_readB(batch,
          kStart + inputRow,
          globalCol + innerCol);
      }
    }
    kStart = kStart + TileInner;
    workgroupBarrier();

    // Compute acc values for a single thread.
    var BCached : array<f32, ColPerThread>;
    for (var k = 0; k < TileInner; k = k + 1) {
      for (var inner = 0; inner < ColPerThread; inner = inner + 1) {
        BCached[inner] = mm_Bsub[k][tileCol + inner];
      }

      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
        ${readDataFromSubASnippet(transposeA)}
        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
          acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];
        }
      }
    }

    workgroupBarrier();
  }

  for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
      mm_write(batch, globalRow + innerRow, globalCol + innerCol,
          acc[innerRow][innerCol]);
    }
  }
  `;
        return `
    var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHight}>;
    var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;
    const RowPerThread = ${workPerThread[1]};
    const ColPerThread = ${workPerThread[0]};
    const TileInner = ${tileInner};

    @compute @workgroup_size(workGroupSizeX, workGroupSizeY, workGroupSizeZ)
    fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
              @builtin(global_invocation_id) GlobalId : vec3<u32>,
              @builtin(num_workgroups) NumWorkgroups: vec3<u32>,
              @builtin(workgroup_id) workgroupId: vec3<u32>) {
      localId = LocalId;
      globalId = GlobalId;
      numWorkgroups = NumWorkgroups;

      let batch = ${splitK ? '0' : 'i32(globalId.z)'};
      let numTiles = ${splitK ? `${Math.ceil(splitedDimInner / tileInner)}` :
        '(uniforms.dimInner - 1) / TileInner + 1'};
      var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : '0'};

      var acc : array<array<f32, ColPerThread>, RowPerThread>;

      // Without this initialization strange values show up in acc.
      for (var innerRow = 0; innerRow < RowPerThread; innerRow = innerRow + 1) {
        for (var innerCol = 0; innerCol < ColPerThread; innerCol = innerCol + 1) {
          acc[innerRow][innerCol] = 0.0;
        }
      }
      ${matmulSnippet}
    }
  `;
    }
    const readVectorASnippet = (transpose) => {
        return transpose ? `
      mm_readA(batch, colA, globalRow),
      mm_readA(batch, colA + 1, globalRow),
      mm_readA(batch, colA + 2, globalRow),
      mm_readA(batch, colA + 3, globalRow)
  ` :
            `
      mm_readA(batch, globalRow, colA),
      mm_readA(batch, globalRow, colA + 1),
      mm_readA(batch, globalRow, colA + 2),
      mm_readA(batch, globalRow, colA + 3)
  `;
    };
    function makeVectorMatrixProductSource(workGroupSize, transposeA = false) {
        tf.util.assert(workGroupSize[1] === 1 && workGroupSize[2] === 1, () => `A linear work group size is required. But got ${workGroupSize}.`);
        return `
    const TileSize = ${workGroupSize[0] * 4};
    var<workgroup> mm_Asub : array<vec4<f32>, ${workGroupSize[0]}>;

    ${getMainHeaderString()} {
      let tileCol = i32(localId.x);
      let globalCol = i32(globalId.x);
      let globalRow = i32(globalId.y);

      let numTiles = (uniforms.dimInner - 1) / TileSize + 1;
      let batch = i32(globalId.z);
      // Without this initialization strange values show up in acc.
      var acc = 0.0;

      // Loop over shared dimension.
      for (var t = 0; t < numTiles; t = t + 1) {
        // Load one tile of A into local memory.
        let colA = t * TileSize + tileCol * 4;
        mm_Asub[tileCol] = vec4<f32>(${readVectorASnippet(transposeA)});
        workgroupBarrier();

        // Compute acc values for a single thread.
        for (var k = 0; k < TileSize / 4; k = k + 1) {
          let rowB = t * TileSize + k * 4;
          let BCached = vec4<f32>(mm_readB(batch, rowB, globalCol),
                              mm_readB(batch, rowB + 1, globalCol),
                              mm_readB(batch, rowB + 2, globalCol),
                              mm_readB(batch, rowB + 3, globalCol));

          let ACached = mm_Asub[k];
          acc = acc + dot(ACached, BCached);
        }

        workgroupBarrier();
      }

      mm_write(batch, globalRow, globalCol, acc);
    }
  `;
    }
    class MatMulPackedProgram {
        constructor(aShape, outputShape, batchAEqualOne, batchBEqualOne, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null, sequentialAccessByThreads = false) {
            this.variableNames = ['A', 'B'];
            this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [2], y: [1], z: [0] };
            const dimInner = transposeA ? aShape[1] : aShape[2];
            this.isVec4 = ((dimInner % 4 === 0 && !transposeA) ||
                (outputShape[1] % 4 === 0 && transposeA)) &&
                outputShape[2] % 4 === 0 && !transposeB;
            this.isVectorA = outputShape[1] === 1 && !transposeA;
            if (!this.isVec4 && this.isVectorA) {
                // For makeVectorMatrixProductSource
                this.elementsPerThread = [1, 1, 1];
                this.workGroupSize = [32, 1, 1];
            }
            else {
                const workGroupInfo = computeWorkGroupInfoForMatMul(outputShape[1], dimInner, outputShape[2], transposeA);
                this.workGroupSize = workGroupInfo.workGroupSize;
                this.elementsPerThread = workGroupInfo.elementsPerThread;
            }
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, this.elementsPerThread);
            const addBias = bias != null;
            const hasPreluActivationWeights = preluActivationWeights != null;
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivationWeights) {
                this.variableNames.push('preluActivationWeights');
            }
            this.sequentialAccessByThreads = sequentialAccessByThreads;
            this.transposeA = transposeA;
            this.transposeB = transposeB;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivationWeights = hasPreluActivationWeights;
            this.batchAEqualOne = batchAEqualOne;
            this.batchBEqualOne = batchBEqualOne;
            [this.fitAOuter, this.fitBOuter, this.fitInner] =
                this.getShapeFit(outputShape[1], outputShape[2], dimInner);
            this.shaderKey = `matMulPacked_${this.elementsPerThread}_${transposeA}_${transposeB}_${this.activation}_${this.fitAOuter}_${this.fitBOuter}_${this.fitInner}_${this.isVec4}_${this.isVectorA}_${this.batchAEqualOne}_${this.batchBEqualOne}_${this.sequentialAccessByThreads}`;
        }
        getShapeFit(dimAOuter, dimBOuter, dimInner) {
            const tileAOuter = this.workGroupSize[1] * this.elementsPerThread[1];
            const tileBOuter = this.workGroupSize[0] * this.elementsPerThread[0];
            if (!this.isVec4 && this.isVectorA) {
                // For makeVectorMatrixProductSource
                this.tileInner = this.workGroupSize[0] * 4;
            }
            else {
                this.tileInner = tileBOuter;
            }
            const fitAOuter = dimAOuter % tileAOuter === 0;
            const fitBOuter = dimBOuter % tileBOuter === 0;
            const fitInner = dimInner % this.tileInner === 0;
            return [fitAOuter, fitBOuter, fitInner];
        }
        getUserCode() {
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights, this.isVec4)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, this.batchAEqualOne, this.batchBEqualOne, false /* transposeA is implemented in makeMatMulPackedSource */, this.transposeB, this.fitAOuter, this.fitBOuter, this.fitInner, this.isVec4 ? 4 : 1)}
      ${this.isVec4 ?
            makeMatMulPackedVec4Source(this.elementsPerThread, this.workGroupSize, this.transposeA, this.tileInner, false, null, this.isVectorA) :
            (this.isVectorA ? makeVectorMatrixProductSource(this.workGroupSize, this.transposeA) :
                makeMatMulPackedSource(this.elementsPerThread, this.workGroupSize, this.transposeA, this.tileInner, false, null, this.sequentialAccessByThreads))}
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function makeMatMulReduceSource() {
        return `
    var<workgroup> sumValues : array<f32, workGroupSizeX>;
    ${getMainHeaderString()} {
      let coords = getOutputCoords();
      let batch = coords[0];
      let row = coords[1];
      let col = coords[2];
      var sum = 0.0;
      let Length = uniforms.dimInner;
      for (var k = i32(localId.x); k < Length; k = k + i32(workGroupSizeX)) {
        let dataA = mm_readA(batch, row, k);
        let dataB = mm_readB(batch, k, col);
        sum = sum + dataA * dataB;
      }
      sumValues[localId.x] = sum;
      workgroupBarrier();

      for(var currentSize = workGroupSizeX / 2u; currentSize > 1u;
          currentSize = currentSize / 2u) {
        if (localId.x < currentSize)
        {
          sumValues[localId.x] = sumValues[localId.x] + sumValues[localId.x + currentSize];
        }
        workgroupBarrier();
      }

      if (localId.x == 0u) {
        sum = sumValues[0] + sumValues[1];
        mm_write(batch, row, col, sum);
      }
    }
  `;
    }
    class MatMulReduceProgram {
        constructor(outputShape, batchAEqualOne, batchBEqualOne, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null) {
            this.variableNames = ['A', 'B'];
            this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
            this.workGroupSize = [256, 1, 1];
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [], y: [1, 2], z: [0] };
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            const addBias = bias != null;
            const hasPreluActivationWeights = preluActivationWeights != null;
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivationWeights) {
                this.variableNames.push('preluActivationWeights');
            }
            this.transposeA = transposeA;
            this.transposeB = transposeB;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivationWeights = hasPreluActivationWeights;
            this.batchAEqualOne = batchAEqualOne;
            this.batchBEqualOne = batchBEqualOne;
            this.shaderKey = `matMulReduce_${this.activation}_${transposeA}_${transposeB}_${this.batchAEqualOne}_${this.batchBEqualOne}`;
        }
        getUserCode() {
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, this.batchAEqualOne, this.batchBEqualOne, this.transposeA, this.transposeB)}
      ${makeMatMulReduceSource()}
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function makeMatMulSmallOutputSizeSource(workGroupSize) {
        const tileAOuter = workGroupSize[1];
        const tileBOuter = workGroupSize[0];
        const tileInner = tileAOuter > tileBOuter ? tileAOuter : tileBOuter;
        return `
  var<workgroup> mm_Asub : array<array<f32, ${tileInner}>, ${tileAOuter}>;
  var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;

  // If the output size is small for matrix multiplication, avoid to use vec4
  // and handle some elements per thread to optimally utilize the ALU.
  // Read data from global memory to registers firstly, then store them into
  // shared memory, so it is instruction-Level parallelism for arithmetic
  // operations and others handle IO operations between barrier api, makes ALU
  // and load/store units work simultaneously, could improves the performance.
  ${getMainHeaderString()} {
    let tileRow = i32(localId.y);
    let tileCol = i32(localId.x);
    let globalRow = i32(globalId.y);
    let globalCol = i32(globalId.x);
    let batch = i32(globalId.z);

    // uniforms.dimInner should be greater than 0.
    let numTiles = (uniforms.dimInner - 1) / ${tileInner} + 1;
    var acc = 0.0;

    var globalColA = tileCol;
    var globalRowB = 0;
    var regA = mm_readA(batch, globalRow, globalColA);
    var regB0 = mm_readB(batch, globalRowB + 2 * tileRow, globalCol);
    var regB1 = mm_readB(batch, globalRowB + 2 * tileRow + 1, globalCol);
    globalColA = globalColA + ${tileInner};
    globalRowB = globalRowB + ${tileInner};

    for (var t = 0; t < numTiles; t = t + 1) {
      mm_Asub[tileRow][tileCol] = regA;
      mm_Bsub[2 * tileRow][tileCol] = regB0;
      mm_Bsub[2 * tileRow + 1][tileCol] = regB1;

      workgroupBarrier();

      regA = mm_readA(batch, globalRow, globalColA);
      regB0 = mm_readB(batch, globalRowB + 2 * tileRow, globalCol);
      regB1 = mm_readB(batch, globalRowB + 2 * tileRow + 1, globalCol);
      globalColA = globalColA + ${tileInner};
      globalRowB = globalRowB + ${tileInner};

      for (var k = 0; k < ${tileInner}; k = k + 1) {
        acc = acc + mm_Asub[tileRow][k] * mm_Bsub[k][tileCol];
      }
      workgroupBarrier();
    }

    mm_write(batch, globalRow, globalCol, acc);
  }
  `;
    }
    class MatMulSmallOutputSizeProgram {
        constructor(aShape, bShape, outputShape, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null) {
            this.variableNames = ['A', 'B'];
            this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
            this.workGroupSize = [16, 8, 1];
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [2], y: [1], z: [0] };
            this.dispatch = [
                Math.ceil(outputShape[2] / this.workGroupSize[0]),
                Math.ceil(outputShape[1] / this.workGroupSize[1]), outputShape[0]
            ];
            const addBias = bias != null;
            if (addBias) {
                this.variableNames.push('bias');
            }
            const hasPreluActivationWeights = preluActivationWeights != null;
            if (hasPreluActivationWeights) {
                this.variableNames.push('preluActivationWeights');
            }
            this.transposeA = transposeA;
            this.transposeB = transposeB;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivationWeights = hasPreluActivationWeights;
            this.batchAEqualOne = aShape[0] === 1;
            this.batchBEqualOne = bShape[0] === 1;
            this.shaderKey = `matMulSmallOutputSize_${this.activation}_${transposeA}_${transposeB}_${this.batchAEqualOne}_${this.batchBEqualOne}`;
        }
        getUserCode() {
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, this.batchAEqualOne, this.batchBEqualOne, this.transposeA, this.transposeB)}
      ${makeMatMulSmallOutputSizeSource(this.workGroupSize)}
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class MatMulSplitKProgram {
        constructor(outputShape, dimInner, batchAEqualOne, batchBEqualOne, transposeA = false, transposeB = false) {
            this.variableNames = ['A', 'B'];
            this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
            this.workGroupSize = [8, 8, 1];
            this.atomic = true;
            this.isVec4 = false;
            this.splitedDimInner = 128;
            tf.util.assert(outputShape[0] === 1, () => 'MatMulSplitKProgram only supports batch = 1.');
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [2], y: [1], z: [0, 3] };
            this.isVec4 = (transposeA && this.outputShape[1] % 4 === 0 ||
                !transposeA && dimInner % 4 === 0) &&
                this.outputShape[2] % 4 === 0;
            this.elementsPerThread = [4, 4, this.splitedDimInner];
            if (!this.isVec4) {
                if (this.outputShape[1] < 16) {
                    this.elementsPerThread[1] = 1;
                }
                if (this.outputShape[2] < 16) {
                    this.elementsPerThread[0] = 1;
                }
            }
            this.dispatch = computeDispatch(this.dispatchLayout, [
                this.outputShape[0], this.outputShape[1], this.outputShape[2],
                dimInner
            ], this.workGroupSize, this.elementsPerThread);
            this.transposeA = transposeA;
            this.transposeB = transposeB;
            this.batchAEqualOne = batchAEqualOne;
            this.batchBEqualOne = batchBEqualOne;
            this.shaderKey =
                `matMulSplitK_${transposeA}_${transposeB}_${batchAEqualOne}_${batchBEqualOne}_${this.elementsPerThread}_${this.isVec4}`;
        }
        getUserCode() {
            // atomicAdd only supports uint/int type. For float, we use
            // atomicCompareExchangeWeak to simulate.
            const atomicAddSnippet = (component) => {
                return `
      for (var i = 0; i < ${component}; i = i + 1)
      {
        var oldValue = atomicLoad(&(result[flatIndex + i]));
        var exchanged = false;
        for (; !exchanged;) {
          let newValueF32 = bitcast<f32>(oldValue) + ${component > 1 ? 'value[i]' : 'value'};
          let newValue = bitcast<i32>(newValueF32);
          let res = atomicCompareExchangeWeak(&(result[flatIndex + i]), oldValue, newValue);
          oldValue = res.old_value;
          exchanged = res.exchanged;
        }
      }
      `;
            };
            const component = this.isVec4 ? 4 : 1;
            const userCode = `
      ${matMulReadFnSource(this.batchAEqualOne, this.batchBEqualOne, false, this.transposeB, false, false, false, component)}
      fn mm_write(batch: i32, row : i32, colIn : i32, value : ${typeSnippet(component)}) {
        let col = colIn * ${component};
        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter) {
          let coords = vec3<i32>(batch, row, col);
          let flatIndex = getOutputIndexFromCoords(coords);
          // The problem is that we should initialize output to zero before using.
          // Otherwise, the original value will be added to the result.
          ${atomicAddSnippet(component)}
        }
      }
      ${this.isVec4 ? makeMatMulPackedVec4Source(this.elementsPerThread, this.workGroupSize, this.transposeA, 32, true, this.splitedDimInner) :
            makeMatMulPackedSource(this.elementsPerThread, this.workGroupSize, this.transposeA, 32, true, this.splitedDimInner)}
    `;
            return userCode;
        }
    }
    class BiasActivationProgram {
        constructor(outputShape, bias = null, activation = null, preluActivationWeights = null) {
            this.uniforms = '';
            this.variableNames = ['x'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.addBias = bias != null;
            this.hasPreluActivationWeights = preluActivationWeights != null;
            this.activation = activation;
            if (this.addBias) {
                this.variableNames.push('bias');
            }
            if (this.hasPreluActivationWeights) {
                this.variableNames.push('preluActivationWeights');
            }
            this.shaderKey = `biasActivation_${activation}`;
        }
        getUserCode() {
            return `
    ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
    ${getMainHeaderString('index')} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        var value = getXByOutputIndex(index);
        ${biasActivationSnippet(this.addBias, this.activation)}
        setOutputAtIndex(index, value);
      }
    }
    `;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class FillProgram {
        constructor(shape) {
            this.variableNames = [];
            this.outputShape = [];
            this.uniforms = 'value : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = 'fill';
        }
        getUserCode() {
            const userCode = `
    ${getMainHeaderString('index')} {
      if (index < uniforms.size) {
        setOutputAtIndex(index, uniforms.value);
      }
    }
  `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function fill(args) {
        const { backend, attrs } = args;
        const { shape, value } = attrs;
        let { dtype } = attrs;
        dtype = dtype || tf.util.inferDtype(value);
        if (dtype === 'string') {
            // String type should be handled in CPU memory.
            const values = tf.util.getArrayFromDType(dtype, tf.util.sizeFromShape(shape));
            values.fill(value);
            return backend.makeTensorInfo(shape, dtype, values);
        }
        else {
            const program = new FillProgram(shape);
            const uniformData = [{ type: 'float32', data: [value] }];
            return backend.runWebGPUProgram(program, [], dtype, uniformData);
        }
    }
    const fillConfig = {
        kernelName: tf.Fill,
        backendName: 'webgpu',
        kernelFunc: fill
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function reshape(args) {
        const { inputs, attrs } = args;
        const { x } = inputs;
        const { shape } = attrs;
        const xSize = tf.util.sizeFromShape(x.shape);
        const $shape = tf.util.inferFromImplicitShape(shape, xSize);
        const $xSize = tf.util.sizeFromShape($shape);
        tf.util.assert(xSize === $xSize, () => `The new shape (${$shape}) has ${$xSize} elements and the old ` +
            `shape (${x.shape}) has ${xSize} elements. The new shape and old ` +
            `shape must have the same number of elements.`);
        // Backend needs to track refCount for the dataId for reshape op
        args.backend.incRef(x.dataId);
        return { dataId: x.dataId, shape: $shape, dtype: x.dtype };
    }
    const reshapeConfig = {
        kernelName: tf.Reshape,
        backendName: 'webgpu',
        kernelFunc: reshape
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function batchMatMulImpl({ a, b, transposeA, transposeB, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
        const aRank = a.shape.length;
        const bRank = b.shape.length;
        const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];
        const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];
        const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];
        const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];
        const outerDimsA = a.shape.slice(0, -2);
        const outerDimsB = b.shape.slice(0, -2);
        const batchDimA = tf.util.sizeFromShape(outerDimsA);
        const batchDimB = tf.util.sizeFromShape(outerDimsB);
        const outShapeOuterDims = tf.broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));
        const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);
        tf.util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` +
            `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +
            `${b.shape} and transposeA=${transposeA}` +
            ` and transposeB=${transposeB} must match.`);
        const a3dShape = transposeA ?
            [batchDimA, innerShapeA, outerShapeA] :
            [batchDimA, outerShapeA, innerShapeA];
        const b3dShape = transposeB ?
            [batchDimB, outerShapeB, innerShapeB] :
            [batchDimB, innerShapeB, outerShapeB];
        // The rest of the implementation is designed to operate on rank-3 tensors
        const a3d = reshape({ inputs: { x: a }, backend, attrs: { shape: a3dShape } });
        const b3d = reshape({ inputs: { x: b }, backend, attrs: { shape: b3dShape } });
        const intermediates = [a3d, b3d];
        const batchDim = Math.max(batchDimA, batchDimB);
        const batchAEqualOne = batchDimA === 1;
        const batchBEqualOne = batchDimB === 1;
        const inputs = [a3d, b3d];
        const dimensions = [
            { type: 'int32', data: [outerShapeA] }, { type: 'int32', data: [outerShapeB] },
            { type: 'int32', data: [innerShapeA] }
        ];
        let program;
        let out;
        const outputShape = [batchDim, outerShapeA, outerShapeB];
        let matmulProgramType = tf.env().get('WEBGPU_MATMUL_PROGRAM_TYPE');
        if (matmulProgramType < 0) {
            if (outerShapeA * outerShapeB <= 128) {
                matmulProgramType = MatMulProgramType.MatMulReduceProgram;
            }
            else if (
            // These boundaries are based on bodypix-ResNet50-image-0.5.
            // TODO: Relax or tight these boundaries when we have a complete matmul
            // test coverage.
            batchDim === 1 && outerShapeA <= 128 && outerShapeB <= 48 &&
                innerShapeB >= 2000) {
                matmulProgramType = MatMulProgramType.MatMulSplitKProgram;
            }
            else if (
            // When the output size is absolutely small or relatively small, we may
            // use MatMulSmallOutputSizeProgram to get better performance.
            // Absolutely small size means that the output size is smaller than [16,
            // 512]. Relatively small size means that one demension size of the
            // output is smaller than 16, and the output size is also more than or
            // equal two times smaller than each of the two input sizes. For
            // example, if input sizes are [12, 2048] and [2048, 1024], the output
            // size is [12, 1024], which is relatively small compared to input
            // sizes.
            (outerShapeA <= 16 &&
                (outerShapeB <= 512 || innerShapeB >= 2 * outerShapeB)) ||
                (outerShapeB <= 16 &&
                    (outerShapeA <= 512 || innerShapeA >= 2 * outerShapeA))) {
                matmulProgramType = MatMulProgramType.MatMulSmallOutputSizeProgram;
            }
            else {
                matmulProgramType = MatMulProgramType.MatMulPackedProgram;
            }
        }
        switch (matmulProgramType) {
            case MatMulProgramType.MatMulReduceProgram:
                program = new MatMulReduceProgram(outputShape, batchAEqualOne, batchBEqualOne, transposeA, transposeB, bias, activation, preluActivationWeights);
                break;
            case MatMulProgramType.MatMulSplitKProgram: {
                // The output buffer must be initailzed to zero before using since we
                // use atomicAdd in MatMulSplitKProgram.
                out = fill({ backend, attrs: { shape: outputShape, value: 0, dtype: a.dtype } });
                program = new MatMulSplitKProgram(outputShape, innerShapeB, batchAEqualOne, batchBEqualOne, transposeA, transposeB);
                if (bias || activation) {
                    out =
                        backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);
                    const biasActivationProgram = new BiasActivationProgram(out.shape, bias, activation, preluActivationWeights);
                    let uniformData = null;
                    const activationInputs = [out];
                    if (bias) {
                        activationInputs.push(bias);
                    }
                    if (preluActivationWeights) {
                        activationInputs.push(preluActivationWeights);
                    }
                    if (activation === 'leakyrelu') {
                        uniformData = [{ type: 'float32', data: [leakyreluAlpha] }];
                        biasActivationProgram.uniforms += ' alpha : f32,';
                    }
                    const outActivated = backend.runWebGPUProgram(biasActivationProgram, activationInputs, out.dtype, uniformData);
                    intermediates.push(out);
                    const outReshaped = reshape({ inputs: { x: outActivated }, backend, attrs: { shape: outShape } });
                    intermediates.push(outActivated);
                    for (const i of intermediates) {
                        backend.disposeData(i.dataId);
                    }
                    return outReshaped;
                }
                break;
            }
            case MatMulProgramType.MatMulSmallOutputSizeProgram:
                program = new MatMulSmallOutputSizeProgram(a3dShape, b3dShape, outputShape, transposeA, transposeB, bias, activation, preluActivationWeights);
                break;
            case MatMulProgramType.MatMulPackedProgram:
                // Experiments show that sequential access is more friendly for Intel
                // GPUs.
                const sequentialAccessByThreads = backend.adapterInfo.isIntel();
                program = new MatMulPackedProgram(a3dShape, outputShape, batchAEqualOne, batchBEqualOne, transposeA, transposeB, bias, activation, preluActivationWeights, sequentialAccessByThreads);
                break;
            default:
                throw new Error(`Unsupported MatMulProgramType ${matmulProgramType}.`);
        }
        if (bias) {
            inputs.push(bias);
        }
        if (preluActivationWeights) {
            inputs.push(preluActivationWeights);
        }
        if (activation === 'leakyrelu') {
            dimensions.push({ type: 'float32', data: [leakyreluAlpha] });
            program.uniforms += ' alpha : f32,';
        }
        out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);
        const outReshaped = reshape({ inputs: { x: out }, backend, attrs: { shape: outShape } });
        intermediates.push(out);
        for (const i of intermediates) {
            backend.disposeData(i.dataId);
        }
        return outReshaped;
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function _fusedMatMul(args) {
        const { inputs, backend, attrs } = args;
        const { a, b, bias, preluActivationWeights } = inputs;
        const { transposeA, transposeB, activation, leakyreluAlpha } = attrs;
        return batchMatMulImpl({
            a,
            b,
            transposeA,
            transposeB,
            backend,
            bias,
            preluActivationWeights,
            leakyreluAlpha,
            activation
        });
    }
    const _fusedMatMulConfig = {
        kernelName: tf._FusedMatMul,
        backendName: 'webgpu',
        kernelFunc: _fusedMatMul,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class BinaryOpComplexProgram {
        constructor(op, aShape, bShape) {
            this.variableNames = ['AReal', 'AImag', 'BReal', 'BImag'];
            this.workGroupSize = [128, 1, 1];
            this.size = true;
            this.outputShape = tf.backend_util.assertAndGetBroadcastShape(aShape, bShape);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `binaryOpComplex_${op}`;
            this.op = op;
        }
        getUserCode() {
            const opStr = getBinaryOpString(this.op, false);
            const userCode = `
      fn binaryOpComplex(
          areal : f32, aimag : f32, breal : f32, bimag : f32) -> f32 {
        ${opStr}
      }

      ${getMainHeaderString('index')} {
        if(index < uniforms.size) {
          let areal = getARealByOutputIndex(index);
          let aimag = getAImagByOutputIndex(index);
          let breal = getBRealByOutputIndex(index);
          let bimag = getBImagByOutputIndex(index);
          setOutputAtIndex(index, binaryOpComplex(areal, aimag, breal, bimag));
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class BinaryOpProgram {
        constructor(op, aShape, bShape) {
            this.size = true;
            this.variableNames = ['A', 'B'];
            this.outputShape = tf.backend_util.assertAndGetBroadcastShape(aShape, bShape);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.op = op;
            this.useSharedMemoryWithA =
                aShape.length <= 1 && bShape.length > 1 && aShape[0] < 128;
            this.useSharedMemoryWithB =
                bShape.length <= 1 && aShape.length > 1 && bShape[0] < 128;
            if (this.useSharedMemoryWithA || this.useSharedMemoryWithB) {
                this.isVec4 = false;
                // lastDimensionSize is used as sharedBuf array size, so can not be
                // used as uniform.
                this.lastDimensionSize =
                    this.useSharedMemoryWithB ? bShape[0] : aShape[0];
                this.shaderKey = `binary_${this.type}_${op}_${this.lastDimensionSize}_${this.useSharedMemoryWithB}`;
                this.type = 'shared';
                // This is an experimental value when using shared memory.
                // Note that the maximum of workgroup X dimension is 256.
                this.workGroupSize = [256, 1, 1];
                this.workPerThread = 1;
            }
            else {
                if (tf.util.arraysEqual(aShape, bShape) &&
                    tf.util.sizeFromShape(aShape) % 4 === 0) {
                    this.isVec4 = true;
                    this.type = 'vec4';
                    this.workPerThread = 4;
                }
                else {
                    this.isVec4 = false;
                    this.type = 'plain';
                    this.workPerThread = 1;
                }
                this.shaderKey = `binary_${this.type}_${op}`;
                // TODO(jiajia.qin@intel.com): Heuristically select a good work group
                // size.
                this.workGroupSize = [128, 1, 1];
            }
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
        }
        getUserCode() {
            let userCode;
            const dType = this.isVec4 ? 'vec4<f32>' : 'f32';
            const opFnStr = `
    fn binaryOperation(a : ${dType}, b : ${dType}) -> ${dType} {
      ${getBinaryOpString(this.op, this.isVec4)}
    };
    `;
            if (this.type === 'shared') {
                const sharedIndexSnippet = this.lastDimensionSize > 1 ?
                    `coords[${this.outputShape.length - 1}]` :
                    '0';
                const accessDataSnippet = this.useSharedMemoryWithB ?
                    `let a = getAByOutputIndex(index);
          let b = sharedBuf[${sharedIndexSnippet}];` :
                    `let a = sharedBuf[${sharedIndexSnippet}];
          let b = getBByOutputIndex(index);`;
                userCode = `
        ${opFnStr}
        var<workgroup> sharedBuf : array<f32, ${this.lastDimensionSize}>;
        ${getMainHeaderString('index')} {
          // Fill in the shared memory buffer.
          let localIndex = i32(localId.x);
          if(localIndex < ${this.lastDimensionSize}) {
            sharedBuf[localIndex] = f32(${this.useSharedMemoryWithB ? 'B' : 'A'}[localIndex]);
          }
          workgroupBarrier();

          if(index < uniforms.size) {
            let coords = getCoordsFromIndex(index);
            ${accessDataSnippet}
            setOutputAtIndex(index, binaryOperation(a, b));
          }
        }
        `;
            }
            else {
                userCode = `
       ${opFnStr}
       ${getMainHeaderString('index')} {
         if (index < uniforms.size) {
           let a = getAByOutputIndex(index);
           let b = getBByOutputIndex(index);
           setOutputAtIndex(index, binaryOperation(a, b));
         }
       }
       `;
            }
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function identity(args) {
        const { inputs } = args;
        const { x } = inputs;
        args.backend.incRef(x.dataId);
        return { dataId: x.dataId, shape: x.shape, dtype: x.dtype };
    }
    const identityConfig = {
        kernelName: tf.Identity,
        backendName: 'webgpu',
        kernelFunc: identity
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Complex tensors share data with their real and imaginary components. Complex
     * tensors' reference to the components is tracked by refCount on the individual
     * component. The refCounts are increased by the identity call.
     *
     * When a complex tensor is disposed, it will reduce the refCount on the
     * components by calling disposeData on each.
     */
    function complex(args) {
        const { inputs, backend } = args;
        const { real, imag } = inputs;
        const complexInfo = backend.makeTensorInfo(real.shape, 'complex64');
        const complex = backend.tensorMap.get(complexInfo.dataId);
        const realTensorInfo = identity({ inputs: { x: real }, backend });
        const imagTensorInfo = identity({ inputs: { x: imag }, backend });
        complex.complexTensorInfos = { real: realTensorInfo, imag: imagTensorInfo };
        return complexInfo;
    }
    const complexConfig = {
        kernelName: tf.Complex,
        backendName: 'webgpu',
        kernelFunc: complex
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class UnaryOpProgram {
        constructor(outputShape, op) {
            this.variableNames = ['A'];
            this.size = true;
            // TODO(jiajia.qin@intel.com): Heuristically select a good work group size.
            const workGroupSizeX = 128;
            this.workGroupSize = [workGroupSizeX, 1, 1];
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.op = op;
            this.shaderKey = `unary_${op}`;
        }
        getUserCode() {
            return `
      fn unaryOperation(a : f32) -> f32 {
        ${getUnaryOpString(this.op, false)}
      }
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let a = getAByOutputIndex(index);
          setOutputAtIndex(index, unaryOperation(a));
        }
      }
      `;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Template that creates a `KernelFunc` for unary ops.
     * @param opType Op type to create `UnaryOpProgram`.
     * @param cpuKernelImpl Optional. Shared functionality from tfjs-backend-cpu, it
     *     will be involved when necessary.
     * @param dtype Optional. If set, the result has this dtype. Otherwise, the
     *     result has the same dtype as the first input. This is mainly used in
     *     comparison kernels, such as Equal, Less, Greater, etc.
     */
    function unaryKernelFunc({ opType, cpuKernelImpl, dtype }) {
        return ({ inputs, backend }) => {
            const { x } = inputs;
            const webgpuBackend = backend;
            const $dtype = dtype || x.dtype;
            if (webgpuBackend.shouldExecuteOnCPU([x]) && cpuKernelImpl != null) {
                const xData = webgpuBackend.tensorMap.get(x.dataId);
                const outValues = cpuKernelImpl(xData.values, $dtype);
                return webgpuBackend.makeTensorInfo(x.shape, $dtype, outValues);
            }
            const program = new UnaryOpProgram(x.shape, opType);
            return webgpuBackend.runWebGPUProgram(program, [x], $dtype);
        };
    }
    /**
     * Template that creates a `KernelFunc` for binary ops.
     * @param opType Op type to create `BinaryOpProgram`.
     * @param cpuKernelImpl Optional. Shared functionality from tfjs-backend-cpu, it
     *     will be involved when necessary.
     * @param dtype Optional. If set, the result has this dtype. Otherwise, the
     *     result has the same dtype as the first input. This is mainly used in
     *     comparison kernels, such as Equal, Less, Greater, etc.
     */
    function binaryKernelFunc({ opType, cpuKernelImpl, supportsComplex = false, dtype }) {
        return ({ inputs, backend }) => {
            const { a, b } = inputs;
            const webgpuBackend = backend;
            if (supportsComplex && a.dtype === 'complex64') {
                const aData = webgpuBackend.tensorMap.get(a.dataId);
                const bData = webgpuBackend.tensorMap.get(b.dataId);
                let real, imag;
                if (opType !== BinaryOpType.MUL) {
                    [real, imag] = [
                        [aData.complexTensorInfos.real, bData.complexTensorInfos.real],
                        [aData.complexTensorInfos.imag, bData.complexTensorInfos.imag]
                    ].map(complexParts => {
                        const [aPart, bPart] = complexParts;
                        const aHandle = {
                            dataId: aPart.dataId,
                            dtype: aPart.dtype,
                            shape: a.shape
                        };
                        const bHandle = {
                            dataId: bPart.dataId,
                            dtype: bPart.dtype,
                            shape: b.shape
                        };
                        const program = new BinaryOpProgram(opType, a.shape, b.shape);
                        return webgpuBackend.runWebGPUProgram(program, [aHandle, bHandle], tf.upcastType(aPart.dtype, bPart.dtype));
                    });
                }
                else {
                    const realProgram = new BinaryOpComplexProgram(BinaryOpType.COMPLEX_MULTIPLY_REAL, a.shape, b.shape);
                    const imagProgram = new BinaryOpComplexProgram(BinaryOpType.COMPLEX_MULTIPLY_IMAG, a.shape, b.shape);
                    const inputs = [
                        {
                            dataId: aData.complexTensorInfos.real.dataId,
                            dtype: aData.complexTensorInfos.real.dtype,
                            shape: a.shape
                        },
                        {
                            dataId: aData.complexTensorInfos.imag.dataId,
                            dtype: aData.complexTensorInfos.imag.dtype,
                            shape: a.shape
                        },
                        {
                            dataId: bData.complexTensorInfos.real.dataId,
                            dtype: bData.complexTensorInfos.real.dtype,
                            shape: b.shape
                        },
                        {
                            dataId: bData.complexTensorInfos.imag.dataId,
                            dtype: bData.complexTensorInfos.imag.dtype,
                            shape: b.shape
                        }
                    ];
                    real = webgpuBackend.runWebGPUProgram(realProgram, inputs, 'float32');
                    imag = webgpuBackend.runWebGPUProgram(imagProgram, inputs, 'float32');
                }
                const complexOutput = complex({ inputs: { real, imag }, backend: webgpuBackend });
                webgpuBackend.disposeData(real.dataId);
                webgpuBackend.disposeData(imag.dataId);
                // TODO: Implement CPU forwarding for complex inputs.
                return complexOutput;
            }
            const $dtype = dtype || tf.upcastType(a.dtype, b.dtype);
            if ((a.dtype === 'string' || b.dtype === 'string' ||
                webgpuBackend.shouldExecuteOnCPU([a, b])) &&
                cpuKernelImpl != null) {
                const aData = webgpuBackend.tensorMap.get(a.dataId).values;
                const bData = webgpuBackend.tensorMap.get(b.dataId).values;
                const decodedAVals = a.dtype === 'string' ?
                    // tslint:disable-next-line: no-any
                    tf.backend_util.fromUint8ToStringArray(aData) :
                    aData;
                const decodedBVals = a.dtype === 'string' ?
                    // tslint:disable-next-line: no-any
                    tf.backend_util.fromUint8ToStringArray(bData) :
                    bData;
                const [outValues, outShape] = cpuKernelImpl(a.shape, b.shape, decodedAVals, decodedBVals, $dtype);
                return webgpuBackend.makeTensorInfo(outShape, $dtype, outValues);
            }
            const program = new BinaryOpProgram(opType, a.shape, b.shape);
            return webgpuBackend.runWebGPUProgram(program, [a, b], $dtype);
        };
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function simpleAbsImpl(vals) {
        const resultValues = new Float32Array(vals.length);
        for (let i = 0; i < vals.length; ++i) {
            resultValues[i] = Math.abs(vals[i]);
        }
        return resultValues;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Template that creates implementation for binary ops. Supports broadcast.
     */
    function createSimpleBinaryKernelImpl(op) {
        return (aShape, bShape, aVals, bVals, dtype) => {
            const newShape = tf.backend_util.assertAndGetBroadcastShape(aShape, bShape);
            const resultRank = newShape.length;
            const resultStrides = tf.util.computeStrides(newShape);
            const resultSize = tf.util.sizeFromShape(newShape);
            const result = tf.util.getTypedArrayFromDType(dtype, resultSize);
            const aRank = aShape.length;
            const bRank = bShape.length;
            const aStrides = tf.util.computeStrides(aShape);
            const bStrides = tf.util.computeStrides(bShape);
            const aBroadcastDims = tf.backend_util.getBroadcastDims(aShape, newShape);
            const bBroadcastDims = tf.backend_util.getBroadcastDims(bShape, newShape);
            if (aBroadcastDims.length + bBroadcastDims.length === 0) {
                for (let i = 0; i < result.length; ++i) {
                    result[i] = op(aVals[i % aVals.length], bVals[i % bVals.length]);
                }
            }
            else {
                for (let i = 0; i < result.length; ++i) {
                    const loc = tf.util.indexToLoc(i, resultRank, resultStrides);
                    const aLoc = loc.slice(-aRank);
                    aBroadcastDims.forEach(d => aLoc[d] = 0);
                    const aIndex = tf.util.locToIndex(aLoc, aRank, aStrides);
                    const bLoc = loc.slice(-bRank);
                    bBroadcastDims.forEach(d => bLoc[d] = 0);
                    const bIndex = tf.util.locToIndex(bLoc, bRank, bStrides);
                    result[i] = op(aVals[aIndex], bVals[bIndex]);
                }
            }
            return [result, newShape];
        };
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function castImpl(values, shape, inputType, dtype) {
        if (dtype === 'int32') {
            const resultValues = Int32Array.from(values);
            return [shape, 'int32', resultValues];
        }
        if (dtype === 'bool') {
            // This is essentially the result of notEqual(x, 0). We avoid using
            // kernel notEqual to avoid circular dependency, i.e. binary_utils ->
            // cast -> notEqual -> binary_utils.
            const zero = tf.util.toTypedArray([0], inputType);
            const [resultData, resultShape] = createSimpleBinaryKernelImpl((a, b) => (a !== b) ? 1 : 0)(shape, [], values, zero, 'bool');
            return [resultShape, 'bool', resultData];
        }
        throw new Error(`Error in Cast: failed to cast ${inputType} to ${dtype}`);
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const addImpl = createSimpleBinaryKernelImpl(((a, b) => a + b));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function bincountImpl(xVals, weightsVals, weightsDtype, weightsShape, size) {
        const weightsSize = tf.util.sizeFromShape(weightsShape);
        const outVals = tf.util.makeZerosTypedArray(size, weightsDtype);
        for (let i = 0; i < xVals.length; i++) {
            const value = xVals[i];
            if (value < 0) {
                throw new Error('Input x must be non-negative!');
            }
            if (value >= size) {
                continue;
            }
            if (weightsSize > 0) {
                outVals[value] += weightsVals[i];
            }
            else {
                outVals[value] += 1;
            }
        }
        return outVals;
    }
    function bincountReduceImpl(xBuf, weightsBuf, size, binaryOutput = false) {
        const numRows = xBuf.shape[0];
        const numCols = xBuf.shape[1];
        const outBuf = tf.buffer([numRows, size], weightsBuf.dtype);
        for (let i = 0; i < numRows; i++) {
            for (let j = 0; j < numCols; j++) {
                const value = xBuf.get(i, j);
                if (value < 0) {
                    throw new Error('Input x must be non-negative!');
                }
                if (value >= size) {
                    continue;
                }
                if (binaryOutput) {
                    outBuf.set(1, i, value);
                }
                else {
                    if (weightsBuf.size > 0) {
                        outBuf.set(outBuf.get(i, value) + weightsBuf.get(i, j), i, value);
                    }
                    else {
                        outBuf.set(outBuf.get(i, value) + 1, i, value);
                    }
                }
            }
        }
        return outBuf;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * Template that creates implementation for unary op.
     */
    function createSimpleUnaryImpl(op) {
        return (values, dtype, attrs) => {
            const newValues = tf.util.getTypedArrayFromDType(dtype, values.length);
            for (let i = 0; i < values.length; ++i) {
                newValues[i] = op(values[i], attrs);
            }
            return newValues;
        };
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const ceilImpl = createSimpleUnaryImpl((xi) => Math.ceil(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function concatImpl$1(inputs, outShape, dtype, simplyConcat) {
        const outVals = tf.util.getArrayFromDType(dtype, tf.util.sizeFromShape(outShape));
        if (simplyConcat && dtype !== 'string') {
            // Use built-in TypedArray.set() method for speed.
            let offset = 0;
            inputs.forEach(input => {
                const size = tf.util.sizeFromShape(input.shape);
                outVals.set(input.vals, offset);
                offset += size;
            });
        }
        else {
            let colOffset = 0;
            inputs.forEach(input => {
                const decodedData = dtype === 'string' ?
                    tf.backend_util.fromUint8ToStringArray(input.vals) :
                    input.vals;
                let tIdx = 0;
                for (let row = 0; row < input.shape[0]; ++row) {
                    const resIdx = row * outShape[1] + colOffset;
                    for (let col = 0; col < input.shape[1]; ++col) {
                        outVals[resIdx + col] = decodedData[tIdx++];
                    }
                }
                colOffset += input.shape[1];
            });
        }
        return outVals;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const equalImpl = createSimpleBinaryKernelImpl((a, b) => (a === b) ? 1 : 0);

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const expImpl = createSimpleUnaryImpl((xi) => Math.exp(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const expm1Impl = createSimpleUnaryImpl((xi) => Math.expm1(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const floorImpl = createSimpleUnaryImpl((xi) => Math.floor(xi));

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function gatherNdImpl(indicesData, paramsBuf, dtype, numSlices, sliceRank, sliceSize, strides, paramsShape, paramsSize) {
        const outBuf = tf.buffer([numSlices, sliceSize], dtype);
        for (let i = 0; i < numSlices; i++) {
            const index = [];
            let flattenIndex = 0;
            for (let j = 0; j < sliceRank; j++) {
                const dim = indicesData[i * sliceRank + j];
                flattenIndex += dim * strides[j];
                index.push(dim);
            }
            if (flattenIndex < 0 || flattenIndex >= paramsSize / sliceSize) {
                throw new Error(`Invalid indices: ${index} does not index into ${paramsShape}`);
            }
            for (let k = 0; k < sliceSize; k++) {
                outBuf.values[i * sliceSize + k] =
                    paramsBuf.get(...paramsBuf.indexToLoc(flattenIndex * sliceSize + k));
            }
        }
        return outBuf;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function gatherV2Impl(xBuf, indicesBuf, flattenOutputShape) {
        const outBuf = tf.buffer(flattenOutputShape, xBuf.dtype);
        for (let i = 0; i < outBuf.size; ++i) {
            const newLoc = outBuf.indexToLoc(i);
            const originalLoc = newLoc.slice();
            const batchIdx = originalLoc[0];
            const indicesIdx = originalLoc[2];
            const indicesIndex = indicesBuf.locToIndex([batchIdx, indicesIdx]);
            originalLoc[2] = indicesBuf.values[indicesIndex];
            const originalIndex = xBuf.locToIndex(originalLoc);
            if (0 <= originalIndex && originalIndex < xBuf.values.length) {
                outBuf.values[i] = xBuf.values[originalIndex];
            } // Else, index is out of bounds, so leave the default zero val in outBuf.
        }
        return outBuf;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const greaterImpl = createSimpleBinaryKernelImpl((a, b) => (a > b) ? 1 : 0);

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const greaterEqualImpl = createSimpleBinaryKernelImpl((a, b) => (a >= b) ? 1 : 0);

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const lessImpl = createSimpleBinaryKernelImpl((a, b) => (a < b) ? 1 : 0);

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const lessEqualImpl = createSimpleBinaryKernelImpl((a, b) => (a <= b) ? 1 : 0);

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function linSpaceImpl(start, stop, num) {
        const step = (stop - start) / (num - 1);
        const values = tf.util.makeZerosTypedArray(num, 'float32');
        values[0] = start;
        for (let i = 1; i < values.length; i++) {
            values[i] = values[i - 1] + step;
        }
        return values;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const logImpl = createSimpleUnaryImpl((xi) => Math.log(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function maxImpl(aVals, reduceSize, outShape, dtype) {
        const vals = tf.util.getTypedArrayFromDType(dtype, tf.util.sizeFromShape(outShape));
        for (let i = 0; i < vals.length; ++i) {
            const offset = i * reduceSize;
            let max = aVals[offset];
            for (let j = 0; j < reduceSize; ++j) {
                const value = aVals[offset + j];
                if (Number.isNaN(value) ||
                    value > max) { // comparison with NaN always return false
                    max = value;
                }
            }
            vals[i] = max;
        }
        return vals;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const maximumImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => Math.max(aValue, bValue)));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const minimumImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => Math.min(aValue, bValue)));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const multiplyImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => aValue * bValue));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function negImpl(xVals, xShape, xDtype) {
        const minusOne = tf.util.createScalarValue(-1, xDtype);
        return multiplyImpl([], xShape, minusOne, xVals, xDtype);
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const notEqualImpl = createSimpleBinaryKernelImpl(((a, b) => (a !== b) ? 1 : 0));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function transposeImpl(xVals, xShape, dtype, perm, newShape) {
        const xRank = xShape.length;
        const xSize = tf.util.sizeFromShape(xShape);
        const xStrides = tf.util.computeStrides(xShape);
        const newStrides = tf.util.computeStrides(newShape);
        const result = tf.util.getTypedArrayFromDType(dtype, tf.util.sizeFromShape(newShape));
        for (let i = 0; i < xSize; ++i) {
            const loc = tf.util.indexToLoc(i, xRank, xStrides);
            // Permute location.
            const newLoc = new Array(loc.length);
            for (let i = 0; i < newLoc.length; i++) {
                newLoc[i] = loc[perm[i]];
            }
            const newIndex = tf.util.locToIndex(newLoc, xRank, newStrides);
            result[newIndex] = xVals[i];
        }
        return result;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function prodImpl(xShape, xDtype, xVals, reductionAxes) {
        const [outShape, reduceShape] = tf.backend_util.computeOutAndReduceShapes(xShape, reductionAxes);
        const outDtype = tf.upcastType(xDtype, 'int32');
        const outVals = tf.util.makeZerosTypedArray(tf.util.sizeFromShape(outShape), outDtype);
        const reduceSize = tf.util.sizeFromShape(reduceShape);
        for (let i = 0; i < outVals.length; ++i) {
            const offset = i * reduceSize;
            let prod = 1;
            for (let j = 0; j < reduceSize; ++j) {
                prod *= xVals[offset + j];
            }
            outVals[i] = prod;
        }
        return { outVals, outShape, outDtype };
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function validateIndices(indices, indicesShape, numParams) {
        indices.forEach((index, i) => {
            if (index < 0 || index >= numParams) {
                const locString = tf.util.indexToLoc(i, indicesShape.length, tf.util.computeStrides(indicesShape))
                    .join(',');
                throw new Error(`indices[${locString}] = ${index} is not in [0, ${numParams})`);
            }
        });
    }
    function validateSplits(paramsNestedSplits, numParamsDenseValues) {
        // Validate
        for (let dim = 0; dim < paramsNestedSplits.length; ++dim) {
            const splits = paramsNestedSplits[dim];
            const lastSplit = (dim === paramsNestedSplits.length - 1) ?
                numParamsDenseValues :
                paramsNestedSplits[dim + 1].length;
            if (splits.length === 0) {
                throw new Error('Ragged splits may not be empty');
            }
            if (splits[0] < 0) {
                throw new Error('Ragged splits must be non-negative');
            }
            if (splits[splits.length - 1] > lastSplit) {
                throw new Error('Ragged splits must not point past values');
            }
            for (let i = 1; i < splits.length; ++i) {
                if (splits[i - 1] > splits[i]) {
                    throw new Error('Ragged splits must be sorted in ascending order');
                }
            }
        }
    }
    // Construct the `splits` output tensors, encoded using a nested vector.
    // Also find the slices of values that need to be copied, and store them
    // in `valueSlices`.  The total number of values that will be copied (which
    // we need for allocating the output values tensor) is stored in `numValues`.
    function makeSplits(indices, indicesShape, paramsNestedSplits, numParamsDenseValues) {
        const valueSlices = [];
        let numValues = 0;
        const numSplits = indicesShape.length - 1 + paramsNestedSplits.length;
        const outSplits = new Array(numSplits).fill(null).map(() => [0]);
        validateSplits(paramsNestedSplits, numParamsDenseValues);
        // Add `splits` that come from all but the last dimension of the dense
        // Tensor `indices`.  In particular, for each dimension D, we add a
        // splits tensor whose values are:
        //   range(reduceProd(splits.shape[:D]) + 1) * splits.shape[D+1]
        // E.g., if indices.shape=[2, 3, 4] then we will add splits tensors:
        //   [0, 3, 6]                    # length=2+1, stride=3
        //   [0, 4, 8, 12, 16, 20, 24]    # length=2*3+1, stride=4
        let nrows = 1;
        for (let dim = 0; dim < indicesShape.length - 1; ++dim) {
            nrows *= indicesShape[dim];
            const rowLength = indicesShape[dim + 1];
            for (let i = 1; i < nrows + 1; ++i) {
                outSplits[dim].push(i * rowLength);
            }
        }
        // Add `splits` that come from `paramsNestedSplits`.  Starting with the
        // outermost ragged dimension (i.e., the first `splits` tensor), we work
        // our way in, finding the range of values that should be copied.  As we
        // go, we update the output `splits` for each dimension with the appropriate
        // values.  In particular, the *lengths* of the slices from `param_splits`
        // should be copied to generate corresponding slice lengths in the output
        // splits.  E.g., if we are copying a ragged row with length 4, then we
        // should add a new split point to outSplits that is 4 greater than the
        // previous split point in outSplits.
        for (let i = 0; i < indices.length; ++i) {
            let start = indices[i];
            let limit = indices[i] + 1;
            // Copy splits.
            for (let dim = 0; dim < paramsNestedSplits.length; ++dim) {
                const splits = paramsNestedSplits[dim];
                const outDim = dim + indicesShape.length - 1;
                if (outDim >= 0) {
                    const outSplitsOutDim = outSplits[outDim];
                    const delta = outSplitsOutDim[outSplitsOutDim.length - 1] - splits[start];
                    for (let j = start; j < limit; ++j) {
                        outSplits[outDim].push(splits[j + 1] + delta);
                    }
                }
                start = splits[start];
                limit = splits[limit];
            }
            if (limit !== start) {
                valueSlices.push([start, limit]);
                numValues += limit - start;
            }
        }
        return { outSplits, valueSlices, numValues };
    }
    function getSplits(outSplits) {
        const splitsOut = [];
        for (let i = 0; i < outSplits.length; ++i) {
            const numSplits = outSplits[i].length;
            const splits = tf.util.getArrayFromDType('int32', numSplits);
            splitsOut.push(splits);
            outSplits[i].forEach((value, j) => splits[j] = value);
        }
        return splitsOut;
    }
    function computeFlatOuterDims(orig, numOutDims) {
        const outDims = orig.slice(0, numOutDims);
        while (outDims.length < numOutDims) {
            outDims.push(1);
        }
        for (let inDim = numOutDims; inDim < orig.length; inDim++) {
            outDims[numOutDims - 1] *= orig[inDim];
        }
        return outDims;
    }
    // For each slice in `(start, limit)` in `valueSlices`, append
    // `paramsDenseValues[start,...,limit] to `values`.  `valueSize` indicates
    // the number of scalars contained in each value paramsDenseValues[i].
    function writeValueSlices(paramsDenseValues, paramsDenseValuesShape, valueSlices, valueSize, values, valuesShape) {
        const denseM = computeFlatOuterDims(paramsDenseValuesShape, 2)[1];
        const valuesM = computeFlatOuterDims(valuesShape, 2)[1];
        let outPos = 0;
        for (const slice of valueSlices) {
            for (let i = slice[0]; i < slice[1]; ++i) {
                for (let j = 0; j < valueSize; ++j) {
                    values[outPos * valuesM + j] = paramsDenseValues[i * denseM + j];
                }
                ++outPos;
            }
        }
    }
    function getValues(paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, valueSlices, numValues) {
        const valuesShape = paramsDenseValuesShape.slice();
        valuesShape[0] = numValues;
        const valuesOut = tf.util.getArrayFromDType(paramsDenseValuesDType, tf.util.sizeFromShape(valuesShape));
        const numElements = paramsDenseValues.length;
        const valueSize = numElements === 0 ? 0 : (numElements / paramsDenseValuesShape[0]);
        writeValueSlices(paramsDenseValues, paramsDenseValuesShape, valueSlices, valueSize, valuesOut, valuesShape);
        return [valuesOut, valuesShape];
    }
    function raggedGatherImpl(paramsNestedSplits, paramsNestedSplitsShapes, paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, indices, indicesShape, outputRaggedRank) {
        if (paramsNestedSplits.length === 0) {
            throw new Error('paramsNestedSplits must be non empty');
        }
        if (paramsNestedSplitsShapes[0].length === 0) {
            throw new Error('Split tensors must not be scalars');
        }
        const numParams = paramsNestedSplitsShapes[0][0] - 1;
        validateIndices(indices, indicesShape, numParams);
        if (paramsDenseValuesShape.length === 0) {
            throw new Error('params.rank must be nonzero');
        }
        const numParamsDenseValues = paramsDenseValuesShape[0];
        // Calculate the `splits`, and store the value slices that we need to
        // copy in `valueSlices`.
        const { outSplits, valueSlices, numValues } = makeSplits(indices, indicesShape, paramsNestedSplits, numParamsDenseValues);
        // Write the output tensors.
        const outputNestedSplits = getSplits(outSplits);
        const outputDenseValues = getValues(paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, valueSlices, numValues);
        return [outputNestedSplits, outputDenseValues[0], outputDenseValues[1]];
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    var RowPartitionType = tf.backend_util.RowPartitionType;
    // Based on
    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc
    class RaggedTensorToTensorOp {
        constructor(shape, shapeShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypeStrings) {
            this.shape = shape;
            this.shapeShape = shapeShape;
            this.values = values;
            this.valuesShape = valuesShape;
            this.valuesDType = valuesDType;
            this.defaultValue = defaultValue;
            this.defaultValueShape = defaultValueShape;
            this.rowPartitionValues = rowPartitionValues;
            this.rowPartitionValuesShapes = rowPartitionValuesShapes;
            this.rowPartitionTypes =
                tf.backend_util.getRowPartitionTypesHelper(rowPartitionTypeStrings);
            this.raggedRank = tf.backend_util.getRaggedRank(this.rowPartitionTypes);
        }
        getRowPartitionTypeByDimension(dimension) {
            if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {
                return this.rowPartitionTypes[dimension + 1];
            }
            else {
                return this.rowPartitionTypes[dimension];
            }
        }
        // Returns the relationship between dimension and dimension + 1.
        getRowPartitionTensor(dimension) {
            if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {
                return this.rowPartitionValues[dimension + 1];
            }
            else {
                return this.rowPartitionValues[dimension];
            }
        }
        getMaxWidth(dimension) {
            const rowPartitionTensor = this.getRowPartitionTensor(dimension - 1);
            switch (this.getRowPartitionTypeByDimension(dimension - 1)) {
                case RowPartitionType.VALUE_ROWIDS:
                    return RaggedTensorToTensorOp.getMaxWidthValueRowID(rowPartitionTensor);
                case RowPartitionType.ROW_SPLITS:
                    return RaggedTensorToTensorOp.getMaxWidthRowSplit(rowPartitionTensor);
                default:
                    throw new Error(`Cannot handle partition type ${RowPartitionType[this.getRowPartitionTypeByDimension(dimension - 1)]}`);
            }
        }
        static getMaxWidthRowSplit(rowSplit) {
            const tensorLength = rowSplit.length;
            if (tensorLength === 0 || tensorLength === 1) {
                return 0;
            }
            let maxWidth = 0;
            for (let i = 0; i < tensorLength - 1; ++i) {
                const currentWidth = rowSplit[i + 1] - rowSplit[i];
                if (currentWidth > maxWidth) {
                    maxWidth = currentWidth;
                }
            }
            return maxWidth;
        }
        static getMaxWidthValueRowID(valueRowIds) {
            const indexLength = valueRowIds.length;
            if (indexLength === 0) {
                return 0;
            }
            let firstEqualIndex = 0;
            let firstEqualIndexValue = valueRowIds[0];
            let maxWidth = 0;
            for (let i = 1; i < indexLength; ++i) {
                const value = valueRowIds[i];
                if (value !== firstEqualIndexValue) {
                    firstEqualIndexValue = value;
                    maxWidth = Math.max(i - firstEqualIndex, maxWidth);
                    firstEqualIndex = i;
                }
            }
            return Math.max(indexLength - firstEqualIndex, maxWidth);
        }
        tensorShapeFromTensor(t, tShape, isPartial = true) {
            if (tShape.length === 0) {
                if (t[0] === -1) {
                    return [];
                }
                throw new Error(`The only valid scalar shape tensor is the fully unknown shape specified as -1.`);
            }
            // MakePartialShape/MakeShapeHelper.
            return makeShape(t, isPartial);
        }
        calculateOutputSize(firstDim) {
            const valueShape = this.valuesShape;
            const defaultValueShape = this.defaultValueShape;
            tf.backend_util.validateDefaultValueShape(defaultValueShape, valueShape);
            const shape = this.tensorShapeFromTensor(this.shape, this.shapeShape);
            const outputShape = tf.backend_util.combineRaggedTensorToTensorShapes(this.raggedRank, shape, valueShape);
            const result = outputShape;
            if (result[0] < 0) {
                result[0] = firstDim;
            }
            for (let i = 1; i <= this.raggedRank; ++i) {
                if (result[i] < 0) {
                    result[i] = this.getMaxWidth(i);
                }
            }
            return result;
        }
        /**
         * The outputIndex represents the index in the output tensor
         * where the first element of a particular dimension would be written.
         * If it is -1, it indicates that the index is out of scope.
         * Example, given firstDimension = 10, firstDimensionOutput = 6,
         * and outputIndexMultiplier = 100:
         * result = [0 100 200 300 400 500 -1 -1 -1 -1]
         * If firstDimensionOutput = 11 instead, then:
         * result = [0 100 200 300 400 500 600 700 800 900]
         */
        calculateFirstParentOutputIndex(firstDimension, outputIndexMultiplier, firstDimensionOutput) {
            const minDimension = Math.min(firstDimension, firstDimensionOutput);
            const result = [];
            let currentOutputIndex = 0;
            for (let i = 0; i < minDimension; ++i, currentOutputIndex += outputIndexMultiplier) {
                result.push(currentOutputIndex);
            }
            for (let i = minDimension; i < firstDimension; ++i) {
                result.push(-1);
            }
            tf.util.assert(result.length === firstDimension, () => 'Final length of result must be equal to firstDimension.');
            return result;
        }
        calculateOutputIndexRowSplit(rowSplit, parentOutputIndex, outputIndexMultiplier, outputSize) {
            const rowSplitSize = rowSplit.length;
            const result = [];
            for (let i = 0; i < rowSplitSize - 1; ++i) {
                const rowLength = rowSplit[i + 1] - rowSplit[i];
                let realLength = Math.min(outputSize, rowLength);
                let parentOutputIndexCurrent = parentOutputIndex[i];
                if (parentOutputIndexCurrent === -1) {
                    realLength = 0;
                }
                for (let j = 0; j < realLength; ++j) {
                    result.push(parentOutputIndexCurrent);
                    parentOutputIndexCurrent += outputIndexMultiplier;
                }
                for (let j = 0; j < rowLength - realLength; ++j) {
                    result.push(-1);
                }
            }
            if (rowSplitSize > 0 && result.length !== rowSplit[rowSplitSize - 1]) {
                throw new Error('Invalid row split size.');
            }
            return result;
        }
        // Calculate the output index of the first element of a list.
        // The parentOutputIndex is the same computation for the previous list.
        // -1 indicates an element or list that is out of range.
        // The outputIndexMultiplier is the number of output indices one moves
        // forward for each column.
        // E.g., given:
        // valueRowIds:[0 1 2 2 2 3 5 5 6]
        // parentOutputIndex:[1000 1100 2000 2100 -1 3000 4000]
        // outputIndexMultiplier: 10
        // outputSize: 2
        // You get:
        // result = [1000 1100 2000 2010 -1 2100 -1 -1 3000]
        // result[0] = parentOutputIndex[valueRowIds[0]]
        // result[1] = parentOutputIndex[valueRowIds[1]]
        // result[2] = parentOutputIndex[valueRowIds[2]]
        // result[3] = parentOutputIndex[valueRowIds[2] + 10]
        // result[4] = -1 because it is the third element the size is 2.
        // result[5] = parentOutputIndex[valueRowIds[3]]
        // result[6] = -1 because parentOutputIndex[valueRowIds[6]] == -1
        // result[7] = -1 because parentOutputIndex[valueRowIds[6]] == -1
        // result[8] = parentOutputIndex[valueRowIds[7]]
        calculateOutputIndexValueRowID(valueRowIds, parentOutputIndex, outputIndexMultiplier, outputSize) {
            const indexSize = valueRowIds.length;
            const result = [];
            if (indexSize === 0) {
                return [];
            }
            let currentOutputColumn = 0;
            let currentValueRowId = valueRowIds[0];
            if (currentValueRowId >= parentOutputIndex.length) {
                throw new Error(`Got currentValueRowId=${currentValueRowId}, which is not less than ${parentOutputIndex.length}`);
            }
            let currentOutputIndex = parentOutputIndex[currentValueRowId];
            result.push(currentOutputIndex);
            for (let i = 1; i < indexSize; ++i) {
                const nextValueRowId = valueRowIds[i];
                if (nextValueRowId === currentValueRowId) {
                    if (currentOutputIndex >= 0) {
                        ++currentOutputColumn;
                        if (currentOutputColumn < outputSize) {
                            currentOutputIndex += outputIndexMultiplier;
                        }
                        else {
                            currentOutputIndex = -1;
                        }
                    }
                }
                else {
                    currentOutputColumn = 0;
                    currentValueRowId = nextValueRowId;
                    if (nextValueRowId >= parentOutputIndex.length) {
                        throw new Error(`Got nextValueRowId=${nextValueRowId} which is not less than ${parentOutputIndex.length}`);
                    }
                    currentOutputIndex = parentOutputIndex[nextValueRowId];
                }
                result.push(currentOutputIndex);
            }
            if (result.length !== valueRowIds.length) {
                throw new Error('Invalid row ids.');
            }
            return result;
        }
        calculateOutputIndex(dimension, parentOutputIndex, outputIndexMultiplier, outputSize) {
            const rowPartitionTensor = this.getRowPartitionTensor(dimension);
            const partitionType = this.getRowPartitionTypeByDimension(dimension);
            switch (partitionType) {
                case RowPartitionType.VALUE_ROWIDS:
                    return this.calculateOutputIndexValueRowID(rowPartitionTensor, parentOutputIndex, outputIndexMultiplier, outputSize);
                case RowPartitionType.ROW_SPLITS:
                    if (rowPartitionTensor.length - 1 > parentOutputIndex.length) {
                        throw new Error(`Row partition size is greater than output size: ${rowPartitionTensor.length - 1} > ${parentOutputIndex.length}`);
                    }
                    return this.calculateOutputIndexRowSplit(rowPartitionTensor, parentOutputIndex, outputIndexMultiplier, outputSize);
                default:
                    throw new Error(`Unsupported partition type: ${RowPartitionType[partitionType]}`);
            }
        }
        getFirstDimensionSize() {
            const firstPartitionTensor = this.rowPartitionValues[0];
            if (this.rowPartitionTypes.length === 0) {
                throw new Error('No row_partition_types given.');
            }
            const firstPartitionType = this.rowPartitionTypes[0];
            switch (firstPartitionType) {
                case RowPartitionType.FIRST_DIM_SIZE:
                    return firstPartitionTensor[0];
                case RowPartitionType.VALUE_ROWIDS:
                    throw new Error('Cannot handle VALUE_ROWIDS in first dimension.');
                case RowPartitionType.ROW_SPLITS:
                    return this.rowPartitionValuesShapes[0][0] - 1;
                default:
                    throw new Error(`Cannot handle type ${RowPartitionType[firstPartitionType]}`);
            }
        }
        compute() {
            const firstPartitionTensor = this.rowPartitionValues[0];
            if (firstPartitionTensor.length <= 0) {
                throw new Error('Invalid first partition input. ' +
                    'Tensor requires at least one element.');
            }
            const firstDimension = this.getFirstDimensionSize();
            const outputSize = this.calculateOutputSize(firstDimension);
            const multiplier = new Array(this.raggedRank + 1);
            multiplier[multiplier.length - 1] = 1;
            for (let i = multiplier.length - 2; i >= 0; --i) {
                multiplier[i] = multiplier[i + 1] * outputSize[i + 1];
            }
            // Full size of the tensor.
            const outputShape = makeShape(outputSize, false);
            const outputTensor = tf.util.getArrayFromDType(this.valuesDType, tf.util.sizeFromShape(outputShape));
            const fullSize = multiplier[0] * outputSize[0];
            if (fullSize > 0) {
                let outputIndex = this.calculateFirstParentOutputIndex(firstDimension, multiplier[0], outputSize[0]);
                for (let i = 1; i <= this.raggedRank; ++i) {
                    const newOutputIndex = this.calculateOutputIndex(i - 1, outputIndex, multiplier[i], outputSize[i]);
                    outputIndex = newOutputIndex;
                }
                this.setOutput(this.raggedRank, outputIndex, outputTensor, outputShape);
            }
            return [outputShape, outputTensor];
        }
        setOutput(raggedRank, outputIndex, outputTensor, outputShape) {
            if (outputTensor.length === 0) {
                return;
            }
            const valuesBase = this.values;
            const outputBase = outputTensor;
            let elementShape = outputShape.slice();
            elementShape = elementShape.slice(raggedRank + 1);
            const valueElementSize = tf.util.sizeFromShape(elementShape);
            const outputIndexSize = outputIndex.length;
            // Broadcast the default value to value_element_size.  (We can skip this
            // if defaultValueTensor.size == 1, since we use fill when that's true.)
            let defaultValue = this.defaultValue;
            if (defaultValue.length !== valueElementSize && defaultValue.length !== 1) {
                const srcShape = this.defaultValueShape;
                tf.tidy(() => {
                    const defaultValueTensor = tf.reshape(defaultValue, srcShape);
                    const bCastDefault = tf.broadcastTo(defaultValueTensor, elementShape);
                    defaultValue = bCastDefault.dataSync();
                });
            }
            // Loop through the outputIndex array, finding contiguous regions that
            // should be copied.  Once we find the end of a contiguous region, copy it
            // and add any necessary padding (with defaultValue).
            let srcStart = 0; // Start of contiguous region (in values)
            let dstStart = 0; // Destination for contiguous region (in output)
            let dstEnd = 0; // Destination for contiguous region (in output)
            for (let srcI = 0; srcI <= outputIndexSize; ++srcI) {
                // dstI is the destination where the value at srcI should be copied.
                let dstI = srcI < outputIndexSize ? outputIndex[srcI] : -1;
                // If we're still in a contiguous region, then update dstEnd go to the
                // next srcI.
                if (dstI === dstEnd) {
                    ++dstEnd;
                    continue;
                }
                // We found the end of contiguous region.  This can be because we found
                // a gap (dstI > dstEnd), or a source value that shouldn't be copied
                // because it's out-of-bounds (dstI == -1), or the end of the tensor
                // (dstI === -1).
                if (dstStart < dstEnd) {
                    // Copy the contiguous region.
                    const src = valuesBase.subarray(srcStart * valueElementSize);
                    const dst = outputBase.subarray(dstStart * valueElementSize);
                    const nVals = (dstEnd - dstStart) * valueElementSize;
                    copyArray(dst, src, nVals);
                }
                // Add any necessary padding (w/ defaultValue).
                if (srcI >= outputIndexSize) {
                    // We reached the end of values: pad to the end of output.
                    const outputSize = outputTensor.length;
                    dstI = Math.floor(outputSize / valueElementSize);
                }
                if (dstI > dstEnd) {
                    if (this.defaultValue.length === 1) {
                        outputBase
                            .subarray(dstEnd * valueElementSize, dstI * valueElementSize)
                            .fill(this.defaultValue[0]);
                        dstEnd = dstI;
                    }
                    else {
                        while (dstI > dstEnd) {
                            const dst = outputBase.slice(dstEnd * valueElementSize);
                            copyArray(dst, defaultValue, valueElementSize);
                            ++dstEnd;
                        }
                    }
                }
                // Update indices.
                if (dstI < 0) {
                    // srcI should be skipped -- leave it out of the contiguous region.
                    srcStart = srcI + 1;
                    dstStart = dstEnd;
                }
                else {
                    // srcI should be copied -- include it in the contiguous region.
                    srcStart = srcI;
                    dstStart = dstEnd;
                    dstEnd = dstStart + 1;
                }
            }
        }
    }
    function copyArray(dst, src, size) {
        for (let i = 0; i < size; i++) {
            dst[i] = src[i];
        }
    }
    function makeShape(shape, isPartial) {
        const out = [];
        for (let dim of shape) {
            if (dim < 0) {
                if (!isPartial) {
                    throw new Error(`Dimension ${dim} must be >= 0`);
                }
                if (dim < -1) {
                    throw new Error(`Dimension ${dim} must be >= -1`);
                }
                dim = -1;
            }
            out.push(dim);
        }
        return out;
    }
    function raggedTensorToTensorImpl(shape, shapesShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypes) {
        return new RaggedTensorToTensorOp(shape, shapesShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypes)
            .compute();
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function rangeImpl(start, stop, step, dtype) {
        const sameStartStop = start === stop;
        const increasingRangeNegativeStep = start < stop && step < 0;
        const decreasingRangePositiveStep = stop < start && step > 1;
        if (sameStartStop || increasingRangeNegativeStep ||
            decreasingRangePositiveStep) {
            return tf.util.makeZerosTypedArray(0, dtype);
        }
        const numElements = Math.abs(Math.ceil((stop - start) / step));
        const values = tf.util.makeZerosTypedArray(numElements, dtype);
        if (stop < start && step === 1) {
            // Auto adjust the step's sign if it hasn't been set
            // (or was set to 1)
            step = -1;
        }
        values[0] = start;
        for (let i = 1; i < values.length; i++) {
            values[i] = values[i - 1] + step;
        }
        return values;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const rsqrtImpl = createSimpleUnaryImpl((xi) => 1 / Math.sqrt(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function scatterImpl(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices) {
        const flattenShape = [outputSize / sliceSize, sliceSize];
        const indicesData = indices.values;
        const updatesData = updates.values;
        if (outputSize === 0) {
            return tf.buffer(shape, updates.dtype);
        }
        const outBuf = tf.buffer(flattenShape, updates.dtype);
        if (typeof defaultValue === 'string') {
            outBuf.values.fill(defaultValue);
        }
        else if (typeof defaultValue === 'number') {
            outBuf.values.fill(defaultValue);
        }
        else if (typeof defaultValue === 'boolean') {
            outBuf.values.fill(+defaultValue);
        }
        for (let i = 0; i < numUpdates; i++) {
            const index = [];
            let flattenIndex = 0;
            for (let j = 0; j < sliceRank; j++) {
                const dim = indicesData[i * sliceRank + j];
                index.push(dim);
                flattenIndex += dim * strides[j];
            }
            if (flattenIndex < 0 || flattenIndex >= outputSize / sliceSize) {
                throw new Error(`Invalid indices: ${index} does not index into ${shape}`);
            }
            for (let k = 0; k < sliceSize; k++) {
                if (sumDupeIndices) {
                    outBuf.values[flattenIndex * sliceSize + k] +=
                        updatesData[i * sliceSize + k];
                }
                else {
                    outBuf.values[flattenIndex * sliceSize + k] = updates.rank === 0 ?
                        updatesData[0] :
                        updatesData[i * sliceSize + k];
                }
            }
        }
        return outBuf;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sigmoidImpl = createSimpleUnaryImpl((xi) => 1 / (1 + Math.exp(-xi)));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sliceImpl(vals, begin, size, shape, dtype) {
        const isContinous = tf.slice_util.isSliceContinous(shape, begin, size);
        const length = tf.util.sizeFromShape(size);
        const xStrides = tf.util.computeStrides(shape);
        if (isContinous) {
            const flatOffset = tf.slice_util.computeFlatOffset(begin, xStrides);
            if (dtype === 'string') {
                return vals.slice(flatOffset, flatOffset + length);
            }
            return vals.subarray(flatOffset, flatOffset + length);
        }
        const decodedData = dtype === 'string' ?
            tf.backend_util.fromUint8ToStringArray(vals) :
            vals;
        const inBuf = tf.buffer(shape, dtype, decodedData);
        const outBuf = tf.buffer(size, dtype);
        for (let i = 0; i < outBuf.size; ++i) {
            const outLoc = outBuf.indexToLoc(i);
            const inLoc = outLoc.map((idx, j) => idx + begin[j]);
            outBuf.set(inBuf.get(...inLoc), ...outLoc);
        }
        if (dtype === 'string') {
            return tf.backend_util.fromStringArrayToUint8(outBuf.values);
        }
        return outBuf.values;
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sparseFillEmptyRowsImpl(indices, indicesShape, indicesDType, values, valuesDType, denseShape, defaultValue) {
        const indicesCount = indicesShape[0];
        const denseRows = denseShape[0];
        const emptyRowIndicator = new Array(denseRows);
        const reverseIndexMap = new Array(indicesCount);
        const rank = indicesShape[1];
        if (denseRows === 0) {
            if (indicesCount !== 0) {
                throw new Error(tf.backend_util.getSparseFillEmptyRowsIndicesDenseShapeMismatch(indicesCount));
            }
            const outputIndices = tf.util.getArrayFromDType(indicesDType, 0);
            const outputValues = tf.util.getArrayFromDType(valuesDType, 0);
            return [
                outputIndices, [0, rank], outputValues, emptyRowIndicator, reverseIndexMap
            ];
        }
        let rowsAreOrdered = true;
        let lastIndicesRow = 0;
        const csrOffset = new Array(denseRows).fill(0);
        for (let i = 0; i < indicesCount; ++i) {
            // indices is a 2d tensor with shape of [N, rank]
            const row = indices[i * rank];
            if (row < 0) {
                throw new Error(tf.backend_util.getSparseFillEmptyRowsNegativeIndexErrorMessage(i, row));
            }
            if (row >= denseRows) {
                throw new Error(tf.backend_util.getSparseFillEmptyRowsOutOfRangeIndexErrorMessage(i, row, denseRows));
            }
            ++csrOffset[row];
            rowsAreOrdered = rowsAreOrdered && (row >= lastIndicesRow);
            lastIndicesRow = row;
        }
        let allRowsFull = true;
        for (let row = 0; row < denseRows; ++row) {
            // csrOffset here describes the number of elements in this dense row
            const rowEmpty = (csrOffset[row] === 0);
            emptyRowIndicator[row] = rowEmpty;
            allRowsFull = allRowsFull && !rowEmpty;
            // In filled version, each row has at least one element.
            csrOffset[row] = Math.max(csrOffset[row], 1);
            // Update csrOffset to represent the number of elements up to and
            // including denseRows + 1:
            //  csrOffset[0] == #{elements of row 0}
            //  csrOffset[1] == #{elements of row 1} + #{elements of row 0}
            //  ..
            //  csrOffset[i] == starting index for elements in row i + 1.
            if (row > 0) {
                csrOffset[row] += csrOffset[row - 1];
            }
        }
        if (allRowsFull && rowsAreOrdered) {
            const outputIndices = indices;
            const outputValues = values;
            for (let i = 0; i < indicesCount; ++i) {
                reverseIndexMap[i] = i;
            }
            return [
                outputIndices, [indicesCount, rank], outputValues, emptyRowIndicator,
                reverseIndexMap
            ];
        }
        else {
            const fullIndicesCount = csrOffset[denseRows - 1];
            const outputIndices = tf.util.getArrayFromDType(indicesDType, fullIndicesCount * rank);
            const outputValues = tf.util.getArrayFromDType(valuesDType, fullIndicesCount);
            const filledCount = new Array(denseRows).fill(0);
            // Fill in values for rows that are not missing
            for (let i = 0; i < indicesCount; ++i) {
                // indices is a 2d tensor with shape of [N, rank]
                const row = indices[i * rank];
                const offset = filledCount[row];
                const outputI = ((row === 0) ? 0 : csrOffset[row - 1]) + offset;
                filledCount[row]++; // Increment the filled count for this row.
                for (let j = 0; j < rank; ++j) {
                    // indices and outputIndices are 2d tensors with shape of [N, rank]
                    outputIndices[outputI * rank + j] = indices[i * rank + j];
                }
                outputValues[outputI] = values[i];
                // We'll need this reverse index map to backprop correctly.
                reverseIndexMap[i] = outputI;
            }
            // Fill in values for rows that are missing
            for (let row = 0; row < denseRows; ++row) {
                const rowCount = filledCount[row];
                if (rowCount === 0) { // We haven't filled this row
                    const startingIndex = (row === 0) ? 0 : csrOffset[row - 1];
                    // Remaining index values were set to zero already.
                    // Just need to set the row index in the right location.
                    // outputIndices is a 2d tensor with shape of [N, rank]
                    outputIndices[startingIndex * rank + 0] = row;
                    for (let col = 1; col < rank; ++col) {
                        outputIndices[startingIndex * rank + col] = 0;
                    }
                    outputValues[startingIndex] = defaultValue;
                }
            }
            return [
                outputIndices, [fullIndicesCount, rank], outputValues, emptyRowIndicator,
                reverseIndexMap
            ];
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sparseReshapeImpl(inputIndices, inputIndicesShape, inputDType, inputShape, targetShape) {
        const denseSize = tf.util.sizeFromShape(inputShape);
        const nnz = inputIndicesShape[0];
        const outputRank = targetShape.length;
        // Compute the output shape. Determine product of specified dimensions, and
        // find the index of the unspecified one.
        const outputShape = [];
        let product = 1;
        let unknownIndex = -1;
        for (let d = 0; d < outputRank; ++d) {
            const size = targetShape[d];
            if (size === -1) {
                if (unknownIndex !== -1) {
                    throw new Error(tf.backend_util
                        .getSparseReshapeMultipleNegativeOneOutputDimErrorMessage(unknownIndex, d));
                }
                unknownIndex = d;
                outputShape.push(1);
            }
            else {
                if (size < 0) {
                    throw new Error(tf.backend_util.getSparseReshapeNegativeOutputDimErrorMessage(d, size));
                }
                product *= size;
                outputShape.push(size);
            }
        }
        if (unknownIndex !== -1) {
            if (product <= 0) {
                throw new Error(tf.backend_util.getSparseReshapeEmptyTensorZeroOutputDimErrorMessage());
            }
            const missing = Math.trunc(denseSize / product);
            if (product * missing !== denseSize) {
                throw new Error(tf.backend_util.getSparseReshapeInputOutputMultipleErrorMessage(inputShape, outputShape));
            }
            outputShape[unknownIndex] = missing;
        }
        const outputSize = tf.util.sizeFromShape(outputShape);
        if (outputSize !== denseSize) {
            throw new Error(tf.backend_util.getSparseReshapeInputOutputMismatchErrorMessage(inputShape, outputShape));
        }
        const inputRank = inputShape.length;
        const inputStrides = [];
        if (inputRank > 0) {
            inputStrides[inputRank - 1] = 1;
            for (let d = inputRank - 2; d >= 0; --d) {
                inputStrides[d] = inputStrides[d + 1] * inputShape[d + 1];
            }
        }
        const outputStrides = [];
        if (outputRank > 0) {
            outputStrides[outputRank - 1] = 1;
            for (let d = outputRank - 2; d >= 0; --d) {
                outputStrides[d] = outputStrides[d + 1] * outputShape[d + 1];
            }
        }
        const newIndices = tf.util.getArrayFromDType(inputDType, nnz * outputRank);
        for (let i = 0; i < nnz; ++i) {
            let id = 0;
            for (let j = 0; j < inputRank; ++j) {
                // inputIndices is a 2d tensor with shape of [nnz, inputRank]
                id += inputIndices[i * inputRank + j] * inputStrides[j];
            }
            for (let j = 0; j < outputRank; ++j) {
                // newIndices is a 2d tensor with shape of [nnz, outputRank]
                newIndices[i * outputRank + j] = Math.trunc(id / outputStrides[j]);
                id %= outputStrides[j];
            }
        }
        return [newIndices, [nnz, outputRank], outputShape];
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sparseSegmentReductionImpl(input, inputShape, inputDType, indices, segmentIds, isMean = false, defaultValue = 0) {
        const numIndices = indices.length;
        // Flatten the array to two dimensions
        const inputFlat = [inputShape[0], input.length / inputShape[0]];
        const numCol = inputFlat[1];
        // Note that the current implementation assumes that segmentIds values are
        // sorted.
        const lastSegmentIdPlusOne = numIndices > 0 ? segmentIds[numIndices - 1] + 1 : 0;
        const outputRows = lastSegmentIdPlusOne;
        if (outputRows < 0) {
            throw new Error(tf.backend_util.getSparseSegmentReductionNegativeSegmentIdsErrorMessage());
        }
        const outputShape = inputShape.slice();
        outputShape[0] = outputRows;
        const outputLength = outputShape.reduce((product, value) => product * value, 1);
        // Output array is initialized with the value 0 by default.
        const output = tf.util.getArrayFromDType(inputDType, outputLength);
        // Note that we do not initialize the output buffer with a default value, so
        // we need to explicitly set missing indices to the default value.
        if (numIndices === 0) {
            if (outputRows > 0) {
                output.fill(defaultValue);
            }
            return [output, outputShape];
        }
        if (outputRows <= 0) {
            throw new Error(tf.backend_util.getSparseSegmentReductionNegativeSegmentIdsErrorMessage());
        }
        let start = 0, end = 1;
        // Index from which the output is not initialized.
        let uninitializedIndex = 0;
        let outIndex = segmentIds[start];
        while (true) {
            // We initialize nextIndex to 0 to avoid may be uninitialized warning
            let nextIndex = 0;
            if (end < numIndices) {
                nextIndex = segmentIds[end];
                if (outIndex === nextIndex) {
                    ++end;
                    continue;
                }
                // We have a new segment here.  Verify that the segment ids are growing.
                if (outIndex >= nextIndex) {
                    throw new Error(tf.backend_util
                        .getSparseSegmentReductionNonIncreasingSegmentIdsErrorMessage());
                }
            }
            if (outIndex < 0 || outIndex >= outputRows) {
                throw new Error(tf.backend_util.getSparseSegmentReductionSegmentIdOutOfRangeErrorMessage(outIndex, outputRows));
            }
            // If there is a gap between two indices, we need to set that gap to the
            // default value.
            if (outIndex > uninitializedIndex) {
                output.fill(defaultValue, uninitializedIndex * numCol, outIndex * numCol);
            }
            for (let i = start; i < end; ++i) {
                const index = indices[i];
                if (index < 0 || index >= inputFlat[0]) {
                    throw new Error(tf.backend_util.getSparseSegmentReductionIndicesOutOfRangeErrorMessage(i, indices[i], inputFlat[0]));
                }
                for (let j = 0; j < numCol; j++) {
                    output[outIndex * numCol + j] += input[index * numCol + j];
                }
            }
            if (isMean) {
                for (let j = 0; j < numCol; j++) {
                    output[outIndex * numCol + j] /= end - start;
                }
            }
            start = end;
            ++end;
            uninitializedIndex = outIndex + 1;
            outIndex = nextIndex;
            if (end > numIndices) {
                break;
            }
        }
        // Fill the gap at the end with the default value.
        if (uninitializedIndex < outputRows) {
            output.fill(defaultValue, uninitializedIndex * numCol, outputRows * numCol);
        }
        return [output, outputShape];
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sqrtImpl = createSimpleUnaryImpl((xi) => Math.sqrt(xi));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const squaredDifferenceImpl = createSimpleBinaryKernelImpl(((a, b) => {
        const diff = a - b;
        return diff * diff;
    }));

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function stridedSliceImpl(outShape, xBuf, strides, begin) {
        const outBuf = tf.buffer(outShape, xBuf.dtype);
        for (let i = 0; i < outBuf.size; i++) {
            const loc = outBuf.indexToLoc(i);
            const newLoc = new Array(loc.length);
            for (let j = 0; j < newLoc.length; j++) {
                newLoc[j] = loc[j] * strides[j] + begin[j];
            }
            outBuf.set(xBuf.get(...newLoc), ...loc);
        }
        return outBuf;
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * The StringNGramsOp class creates ngrams from ragged string data.
     * The constructor contains all attributes related to the operation such as
     * padding widths and strings, and the compute function can be used to
     * compute the ngrams for different ragged tensor inputs.
     */
    class StringNGramsOp {
        constructor(separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences) {
            this.separator = tf.util.encodeString(separator);
            this.nGramWidths = nGramWidths;
            this.leftPad = tf.util.encodeString(leftPad);
            this.rightPad = tf.util.encodeString(rightPad);
            this.padWidth = padWidth;
            this.preserveShort = preserveShortSequences;
        }
        getPadWidth(nGramWidth) {
            // Ngrams can be padded with either a fixed pad width or a dynamic pad
            // width depending on the 'padWidth' arg, but in no case should the padding
            // ever be wider than 'nGramWidth' - 1.
            return Math.min(this.padWidth < 0 ? nGramWidth - 1 : this.padWidth, nGramWidth - 1);
        }
        getNumNGrams(length, nGramWidth) {
            const padWidth = this.getPadWidth(nGramWidth);
            return Math.max(0, ((length + 2 * padWidth) - nGramWidth) + 1);
        }
        createNGrams(data, splitIndex, output, outputStartIndex, numNGrams, nGramWidth) {
            for (let nGramIndex = 0; nGramIndex < numNGrams; ++nGramIndex) {
                const padWidth = this.getPadWidth(nGramWidth);
                const leftPadding = Math.max(0, padWidth - nGramIndex);
                const rightPadding = Math.max(0, padWidth - (numNGrams - (nGramIndex + 1)));
                const numTokens = nGramWidth - (leftPadding + rightPadding);
                const dataStartIndex = splitIndex + (leftPadding > 0 ? 0 : nGramIndex - padWidth);
                // Calculate the total expected size of the nGram so we can reserve the
                // correct amount of space in the string.
                let nGramSize = 0;
                // Size of the left padding.
                nGramSize += leftPadding * this.leftPad.length;
                // Size of the tokens.
                for (let n = 0; n < numTokens; ++n) {
                    nGramSize += data[dataStartIndex + n].length;
                }
                // Size of the right padding.
                nGramSize += rightPadding * this.rightPad.length;
                // Size of the separators.
                const numSeparators = leftPadding + rightPadding + numTokens - 1;
                nGramSize += numSeparators * this.separator.length;
                // Build the nGram.
                output[outputStartIndex + nGramIndex] = new Uint8Array(nGramSize);
                const nGram = output[outputStartIndex + nGramIndex];
                let nextNGramIndex = 0;
                const appendToNGram = (str) => str.forEach((value) => nGram[nextNGramIndex++] = value);
                for (let n = 0; n < leftPadding; ++n) {
                    appendToNGram(this.leftPad);
                    appendToNGram(this.separator);
                }
                // Only output first numTokens - 1 pairs of data and separator
                for (let n = 0; n < numTokens - 1; ++n) {
                    appendToNGram(data[dataStartIndex + n]);
                    appendToNGram(this.separator);
                }
                // Handle case when there are no tokens or no right padding as these
                // can result in consecutive separators.
                if (numTokens > 0) {
                    // If we have tokens, then output last and then pair each separator
                    // with the right padding that follows, to ensure nGram ends either with
                    // the token or with the right pad.
                    appendToNGram(data[dataStartIndex + numTokens - 1]);
                    for (let n = 0; n < rightPadding; ++n) {
                        appendToNGram(this.separator);
                        appendToNGram(this.rightPad);
                    }
                }
                else {
                    // If we don't have tokens, then the last item inserted into the nGram
                    // has been the separator from the left padding loop above. Hence,
                    // output right pad and separator and make sure to finish with a
                    // padding, not a separator.
                    for (let n = 0; n < rightPadding - 1; ++n) {
                        appendToNGram(this.rightPad);
                        appendToNGram(this.separator);
                    }
                    appendToNGram(this.rightPad);
                }
            }
        }
        // Data and splits together form the definition of the ragged tensor,
        // where data is 1 dimensional and contains the values of the tensor
        // and splits denotes the indices at which each row starts.
        compute(data, splits) {
            // Validate that the splits are valid indices into data, only if there are
            // splits specified.
            const inputDataSize = data.length;
            const splitsSize = splits.length;
            if (splitsSize > 0) {
                let prevSplit = splits[0];
                if (prevSplit !== 0) {
                    throw new Error(`First split value must be 0, got ${prevSplit}`);
                }
                for (let i = 1; i < splitsSize; ++i) {
                    let validSplits = splits[i] >= prevSplit;
                    validSplits = validSplits && (splits[i] <= inputDataSize);
                    if (!validSplits) {
                        throw new Error(`Invalid split value ${splits[i]}, must be in [${prevSplit}, ${inputDataSize}]`);
                    }
                    prevSplit = splits[i];
                }
                if (prevSplit !== inputDataSize) {
                    throw new Error(`Last split value must be data size. Expected ${inputDataSize}, got ${prevSplit}`);
                }
            }
            const numBatchItems = splitsSize - 1;
            const nGramsSplits = tf.util.getArrayFromDType('int32', splitsSize);
            // If there is no data or size, return an empty ragged tensor.
            if (inputDataSize === 0 || splitsSize === 0) {
                const empty = new Array(inputDataSize);
                for (let i = 0; i <= numBatchItems; ++i) {
                    nGramsSplits[i] = 0;
                }
                return [empty, nGramsSplits];
            }
            nGramsSplits[0] = 0;
            for (let i = 1; i <= numBatchItems; ++i) {
                const length = splits[i] - splits[i - 1];
                let numNGrams = 0;
                this.nGramWidths.forEach((nGramWidth) => {
                    numNGrams += this.getNumNGrams(length, nGramWidth);
                });
                if (this.preserveShort && length > 0 && numNGrams === 0) {
                    numNGrams = 1;
                }
                nGramsSplits[i] = nGramsSplits[i - 1] + numNGrams;
            }
            const nGrams = new Array(nGramsSplits[numBatchItems]);
            for (let i = 0; i < numBatchItems; ++i) {
                const splitIndex = splits[i];
                let outputStartIdx = nGramsSplits[i];
                this.nGramWidths.forEach((nGramWidth) => {
                    const length = splits[i + 1] - splits[i];
                    const numNGrams = this.getNumNGrams(length, nGramWidth);
                    this.createNGrams(data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);
                    outputStartIdx += numNGrams;
                });
                // If we're preserving short sequences, check to see if no sequence was
                // generated by comparing the current output start idx to the original
                // one (nGramSplitsdata). If no ngrams were generated, then they will
                // be equal (since we increment outputStartIdx by numNGrams every
                // time we create a set of ngrams.)
                if (this.preserveShort && outputStartIdx === nGramsSplits[i]) {
                    const dataLength = splits[i + 1] - splits[i];
                    // One legitimate reason to not have any ngrams when this.preserveShort
                    // is true is if the sequence itself is empty. In that case, move on.
                    if (dataLength === 0) {
                        continue;
                    }
                    // We don't have to worry about dynamic padding sizes here: if padding
                    // was dynamic, every sequence would have had sufficient padding to
                    // generate at least one nGram.
                    const nGramWidth = dataLength + 2 * this.padWidth;
                    const numNGrams = 1;
                    this.createNGrams(data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);
                }
            }
            return [nGrams, nGramsSplits];
        }
    }
    function stringNGramsImpl(data, dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences) {
        return new StringNGramsOp(separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences)
            .compute(data, dataSplits);
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function split(str, delimiters, skipEmpty, result) {
        if (!str.length) {
            return;
        }
        // When the delimiter is empty, the input is split into individual characters.
        if (delimiters.length === 0) {
            for (let i = 0; i < str.length; ++i) {
                result.push(str.subarray(i, i + 1));
            }
            return;
        }
        // When there is one delimiter, the input is split only at that delimiter.
        if (delimiters.length === 1) {
            const delimiter = delimiters[0];
            let f = str.indexOf(delimiter);
            while (f !== -1) {
                const token = str.subarray(0, f);
                if (!skipEmpty || token.length !== 0) {
                    result.push(token);
                }
                str = str.subarray(f + 1);
                f = str.indexOf(delimiter);
            }
            if (!skipEmpty || str.length !== 0) {
                result.push(str);
            }
            return;
        }
        // When there are multiple delimiters, the input is split at every instance
        // one of the delimiters appears.
        let tokenStart = 0;
        for (let i = 0; i < str.length + 1; i++) {
            if ((i === str.length) || (delimiters.indexOf(str[i]) !== -1)) {
                const token = str.subarray(tokenStart, i);
                if (!skipEmpty || token.length !== 0) {
                    result.push(token);
                }
                tokenStart = i + 1;
            }
        }
    }
    function stringSplitImpl(input, delimiter, skipEmpty) {
        const batchSize = input.length;
        // Empty delimiter means split the input character by character.
        const tokens = [];
        let outputSize = 0;
        let maxNumEntries = 0;
        const numIndices = new Array(batchSize);
        for (let i = 0; i < batchSize; ++i) {
            const prevTokensLength = tokens.length;
            split(input[i], delimiter, skipEmpty, tokens);
            const nEntries = tokens.length - prevTokensLength;
            numIndices[i] = nEntries;
            outputSize += nEntries;
            maxNumEntries = Math.max(maxNumEntries, nEntries);
        }
        const indices = tf.util.getArrayFromDType('int32', outputSize * 2);
        const values = new Array(outputSize);
        const shape = [batchSize, maxNumEntries];
        let c = 0;
        for (let i = 0; i < batchSize; ++i) {
            for (let j = 0; j < numIndices[i]; ++j) {
                // indices is a 2d tensor with shape of [outputSize, 2]
                indices[c * 2] = i;
                indices[c * 2 + 1] = j;
                values[c] = tokens[c];
                ++c;
            }
        }
        return [indices, values, shape];
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function stringToHashBucketFastImpl(input, numBuckets) {
        const output = tf.util.getArrayFromDType('int32', input.length);
        for (let i = 0; i < input.length; ++i) {
            output[i] =
                tf.util.fingerPrint64(input[i]).modulo(numBuckets).getLowBitsUnsigned();
        }
        return output;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const subImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => aValue - bValue));

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    /**
     * An implementation of the tile kernel shared between webgl and cpu for string
     * tensors only.
     */
    function tileImpl(xBuf, reps) {
        const newShape = new Array(xBuf.rank);
        for (let i = 0; i < newShape.length; i++) {
            newShape[i] = xBuf.shape[i] * reps[i];
        }
        const result = tf.buffer(newShape, xBuf.dtype);
        for (let i = 0; i < result.values.length; ++i) {
            const newLoc = result.indexToLoc(i);
            const originalLoc = new Array(xBuf.rank);
            for (let j = 0; j < originalLoc.length; j++) {
                originalLoc[j] = newLoc[j] % xBuf.shape[j];
            }
            const originalIndex = xBuf.locToIndex(originalLoc);
            result.values[i] = xBuf.values[originalIndex];
        }
        return result;
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const comparePair = (a, b) => {
        const valueDiff = b.value - a.value;
        return valueDiff === 0 ? a.index - b.index : valueDiff;
    };
    /**
     * Partitions array where all elements smaller than the (k+1) smallest element
     * are found to the left of it, and all larger to the right of it.
     * Based on the Floyd-Rivest Algorithm, ref:
     * https://en.wikipedia.org/wiki/Floyd%E2%80%93Rivest_algorithm
     * @param array: Array to partition
     * @param left: Left index for the interval
     * @param right: Right index for the interval
     * @param k: Desired index value, where array[k] is the (k+1)th smallest element
     *           when left = 0
     */
    function select$1(array, k, left = 0, right = array.length - 1) {
        while (right > left) {
            // Use select recursively to sample a smaller set of size s
            // the arbitrary constants 600 and 0.5 are used in the original
            // version to minimize execution time.
            if (right - left > 600) {
                const n = right - left + 1;
                const i = k - left + 1;
                const z = Math.log(n);
                const s = 0.5 * Math.exp(2 * z / 3);
                const sd = 0.5 * Math.sqrt(z * s * (n - s) / n) * Math.sign(i - n / 2);
                const newLeft = Math.max(left, Math.floor(k - i * s / n + sd));
                const newRight = Math.min(right, Math.floor(k + (n - i) * s / n + sd));
                select$1(array, k, newLeft, newRight);
            }
            // partition the elements between left and right around t
            const t = array[k];
            let i = left;
            let j = right;
            tf.util.swap(array, left, k);
            if (comparePair(array[right], t) > 0) {
                tf.util.swap(array, left, right);
            }
            while (i < j) {
                tf.util.swap(array, i, j);
                i++;
                j--;
                while (comparePair(array[i], t) < 0) {
                    i = i + 1;
                }
                while (comparePair(array[j], t) > 0) {
                    j = j - 1;
                }
            }
            if (comparePair(array[left], t) === 0) {
                tf.util.swap(array, left, j);
            }
            else {
                j = j + 1;
                tf.util.swap(array, j, right);
            }
            // Adjust left and right towards the boundaries of the subset
            // containing the (k - left + 1)th smallest element.
            if (j <= k) {
                left = j + 1;
            }
            if (k <= j) {
                right = j - 1;
            }
        }
    }
    function topKImpl(x, xShape, xDtype, k, sorted) {
        // Reshape into a 2d tensor [batch, lastDim] and compute topk along lastDim.
        const lastDim = xShape[xShape.length - 1];
        const [batch, size] = [x.length / lastDim, lastDim];
        const allTopKVals = tf.util.getTypedArrayFromDType(xDtype, batch * k);
        const allTopKIndices = tf.util.getTypedArrayFromDType('int32', batch * k);
        for (let b = 0; b < batch; b++) {
            const offset = b * size;
            const vals = x.subarray(offset, offset + size);
            let valAndInd = new Array(vals.length);
            vals.forEach((value, index) => valAndInd[index] = { value, index });
            if (k < valAndInd.length) {
                select$1(valAndInd, k);
                valAndInd = valAndInd.slice(0, k);
            }
            if (sorted) {
                valAndInd.sort(comparePair);
            }
            const outOffset = b * k;
            const topKVals = allTopKVals.subarray(outOffset, outOffset + k);
            const topKIndices = allTopKIndices.subarray(outOffset, outOffset + k);
            for (let i = 0; i < k; i++) {
                topKVals[i] = valAndInd[i].value;
                topKIndices[i] = valAndInd[i].index;
            }
        }
        // Reshape back to the original input shape, except that the last
        // dimension is k.
        const outputShape = xShape.slice();
        outputShape[outputShape.length - 1] = k;
        return [
            tf.buffer(outputShape, xDtype, allTopKVals),
            tf.buffer(outputShape, 'int32', allTopKIndices)
        ];
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function uniqueImpl(values, axis, shape, dtype) {
        // Normalize and validate axis.
        const $axis = tf.util.parseAxisParam(axis, shape)[0];
        // Calculate the new shape that is suitable for extracting data along the
        // given axis.
        //
        // The rank is 3.
        // The size of the 1st dimension is the size of all the axes < the given axis.
        // The size of the 2nd dimension is the same as the size of the given axis.
        // The size of the 3rd dimension is the size of all the axes > the given axis.
        //
        // For example, for a 4D tensor with shape=[2, 3, 5, 4] and axis=2, the
        // newShape would be: [2*3, 5, 4].
        //
        // Note that this is not the final output shape. This will be the shape for an
        // intermediate TensorBuffer (see inputBuffer below) to allow us to extract
        // values along the given axis. To demonstrate how it works, consider the
        // following example:
        //
        // Input: a 3D tensor, with shape [1, 2, 3]
        // [
        //   [
        //      [1,2,3],
        //      [4,5,6]
        //   ]
        // ]
        // Axis: 2 (the last axis).
        // Along axis 2, we expect to extract 3 tensors: [1,4], [2,5], [3,6].
        //
        // For this example, newShape would be: [2, 3, 1], where 2 is calculated from
        // 1*2. The re-shaped data would look like:
        //
        // [
        //   [
        //     [1], [2], [3]
        //   ],
        //   [
        //     [4], [5], [6]
        //   ]
        // ]
        //
        // Then, we can construct a 3-level nested loop by the following dimension
        // order to extract the values along the axis (dimension1):
        // i: dimension1       // 0,1,2 (newShape[1])
        //   m: dimension0     // 0,1   (newShape[0])
        //     n: dimension2   // 0     (newShape[2])
        //
        //                       m, i, n
        //                      ---------
        // Iteration 0: data at [0, 0, 0] => "1"
        // Iteration 1: data at [1, 0, 0] => "4"
        // We got [1,4].
        // Iteration 2: data at [0, 1, 0] => "2"
        // Iteration 3: data at [1, 1, 0] => "5"
        // We got [2,5].
        // Iteration 4: data at [0, 2, 0] => "3"
        // Iteration 5: data at [1, 2, 0] => "6"
        // We got [3,6].
        const newShape = [1, shape[0], 1];
        for (let i = 0; i < $axis; i++) {
            newShape[0] *= shape[i];
        }
        newShape[1] = shape[$axis];
        for (let i = $axis + 1; i < shape.length; i++) {
            newShape[2] *= shape[i];
        }
        // A map from unique elements (their string representations) to their values
        // in "indices" (below).
        const uniqueElements = {};
        // The indices of each unique element in the original tensor along the given
        // axis. It is 1D and has the same size as the given axis.
        const indices = new Int32Array(shape[$axis]);
        // Create a buffer so we can easily extract value at a given location.
        const inputBuffer = new tf.TensorBuffer(newShape, dtype, values);
        // The indices along the given axis that have unique elements. This is a
        // de-duped version of "indices" above.
        const uniqueIndices = [];
        const is1DTensor = newShape[0] === 1 && newShape[2] === 1;
        for (let i = 0; i < shape[$axis]; i++) {
            // Extract values along the axis.
            let element;
            if (is1DTensor) {
                // Fast path for 1D tensor input.
                element = values[i].toString();
            }
            else {
                const axisValues = [];
                for (let m = 0; m < newShape[0]; m++) {
                    for (let n = 0; n < newShape[2]; n++) {
                        axisValues.push(inputBuffer.get(m, i, n));
                    }
                }
                element = axisValues.join(',');
            }
            // Dedup and update various indices.
            if (uniqueElements[element] !== undefined) {
                indices[i] = uniqueElements[element];
            }
            else {
                const uniqueIndex = Object.keys(uniqueElements).length;
                uniqueElements[element] = uniqueIndex;
                indices[i] = uniqueIndex;
                uniqueIndices.push(i);
            }
        }
        // Now we know where each of the unique elements are located along the axis
        // (uniqueIndices). Extract them from input buffer and store them in the
        // output buffer.
        const outputTmpShape = newShape.slice();
        outputTmpShape[1] = Object.keys(uniqueElements).length;
        const outputBuffer = new tf.TensorBuffer(outputTmpShape, dtype);
        uniqueIndices.forEach((uniqueElementIndex, i) => {
            for (let m = 0; m < newShape[0]; m++) {
                for (let n = 0; n < newShape[2]; n++) {
                    outputBuffer.set(inputBuffer.get(m, uniqueElementIndex, n), m, i, n);
                }
            }
        });
        // The output shape can be calculated from the input shape with the size of
        // the given axis replaced by the number of unique elements along that axis.
        const outputShape = shape.slice();
        outputShape[$axis] = outputTmpShape[1];
        return {
            outputValues: outputBuffer.values,
            outputShape,
            indices,
        };
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */

    var shared = {
        __proto__: null,
        simpleAbsImpl: simpleAbsImpl,
        addImpl: addImpl,
        bincountImpl: bincountImpl,
        bincountReduceImpl: bincountReduceImpl,
        castImpl: castImpl,
        ceilImpl: ceilImpl,
        concatImpl: concatImpl$1,
        equalImpl: equalImpl,
        expImpl: expImpl,
        expm1Impl: expm1Impl,
        floorImpl: floorImpl,
        gatherNdImpl: gatherNdImpl,
        gatherV2Impl: gatherV2Impl,
        greaterImpl: greaterImpl,
        greaterEqualImpl: greaterEqualImpl,
        lessImpl: lessImpl,
        lessEqualImpl: lessEqualImpl,
        linSpaceImpl: linSpaceImpl,
        logImpl: logImpl,
        maxImpl: maxImpl,
        maximumImpl: maximumImpl,
        minimumImpl: minimumImpl,
        multiplyImpl: multiplyImpl,
        negImpl: negImpl,
        notEqualImpl: notEqualImpl,
        prodImpl: prodImpl,
        raggedGatherImpl: raggedGatherImpl,
        raggedTensorToTensorImpl: raggedTensorToTensorImpl,
        rangeImpl: rangeImpl,
        rsqrtImpl: rsqrtImpl,
        scatterImpl: scatterImpl,
        sigmoidImpl: sigmoidImpl,
        sliceImpl: sliceImpl,
        sparseFillEmptyRowsImpl: sparseFillEmptyRowsImpl,
        sparseReshapeImpl: sparseReshapeImpl,
        sparseSegmentReductionImpl: sparseSegmentReductionImpl,
        sqrtImpl: sqrtImpl,
        squaredDifferenceImpl: squaredDifferenceImpl,
        stridedSliceImpl: stridedSliceImpl,
        stringNGramsImpl: stringNGramsImpl,
        stringSplitImpl: stringSplitImpl,
        stringToHashBucketFastImpl: stringToHashBucketFastImpl,
        subImpl: subImpl,
        tileImpl: tileImpl,
        topKImpl: topKImpl,
        transposeImpl: transposeImpl,
        uniqueImpl: uniqueImpl
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const { addImpl: addImplCPU, castImpl: castImplCPU, ceilImpl: ceilImplCPU, concatImpl: concatImplCPU, equalImpl: equalImplCPU, expImpl: expImplCPU, expm1Impl: expm1ImplCPU, floorImpl: floorImplCPU, gatherNdImpl: gatherNdImplCPU, gatherV2Impl: gatherV2ImplCPU, greaterEqualImpl: greaterEqualImplCPU, greaterImpl: greaterImplCPU, lessEqualImpl: lessEqualImplCPU, lessImpl: lessImplCPU, logImpl: logImplCPU, maxImpl: maxImplCPU, maximumImpl: maximumImplCPU, minimumImpl: minimumImplCPU, multiplyImpl: multiplyImplCPU, negImpl: negImplCPU, notEqualImpl: notEqualImplCPU, prodImpl: prodImplCPU, rangeImpl: rangeImplCPU, rsqrtImpl: rsqrtImplCPU, scatterImpl: scatterImplCPU, simpleAbsImpl: simpleAbsImplCPU, sliceImpl: sliceImplCPU, stridedSliceImpl: stridedSliceImplCPU, stringNGramsImpl: stringNGramsImplCPU, subImpl: subImplCPU, tileImpl: tileImplCPU, topKImpl: topKImplCPU, transposeImpl: transposeImplCPU, uniqueImpl: uniqueImplCPU, } = shared;

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const abs = unaryKernelFunc({ opType: UnaryOpType.ABS, cpuKernelImpl: simpleAbsImplCPU });
    const absConfig = {
        kernelName: tf.Abs,
        backendName: 'webgpu',
        kernelFunc: abs
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const addKernelFunc = binaryKernelFunc({ opType: BinaryOpType.ADD, cpuKernelImpl: addImplCPU, supportsComplex: true });
    const addConfig = {
        kernelName: tf.Add,
        backendName: 'webgpu',
        kernelFunc: addKernelFunc
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class AddNPackedProgram {
        constructor(shapes) {
            this.workPerThread = 1;
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = shapes[0];
            this.variableNames = shapes.map((_, i) => `T${i}`);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            this.shaderKey = 'addN';
        }
        getUserCode() {
            const snippets = [];
            // Get target elements from every input tensor.
            this.variableNames.forEach(variable => {
                snippets.push(`let v${variable} = get${variable}ByOutputCoords(coords);`);
            });
            // Calculate the sum of all elements.
            const operation = this.variableNames
                .map(variable => {
                return `v${variable}`;
            })
                .join(' + ');
            const userCode = `
      ${getMainHeaderString('index')} {
        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if (flatIndex < uniforms.size) {
            let coords = getCoordsFromIndex(flatIndex);
            ${snippets.join('\n        ')}
            setOutputAtIndex(flatIndex, ${operation});
          }
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function addN(args) {
        const { inputs, backend } = args;
        const tensors = inputs;
        if (tensors.length === 1) {
            return identity({ inputs: { x: tensors[0] }, backend });
        }
        const dtype = tensors.map(t => t.dtype).reduce((d1, d2) => tf.upcastType(d1, d2));
        const shapes = tensors.map(t => t.shape);
        const program = new AddNPackedProgram(shapes);
        return backend.runWebGPUProgram(program, tensors, dtype);
    }
    const addNConfig = {
        kernelName: tf.AddN,
        backendName: 'webgpu',
        kernelFunc: addN
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ArgMinMaxProgram {
        constructor(inputShape, axis, reduceType) {
            this.workGroupSize = [64, 1, 1];
            this.variableNames = ['x'];
            this.uniforms = 'infinityValue : f32,';
            this.size = true;
            const axes = [axis];
            this.op = reduceType === 'min' ? '<' : '>';
            // |outShape| is the shape with the removed axis
            const [outputShape, reduceShape] = tf.backend_util.computeOutAndReduceShapes(inputShape, axes);
            this.outputShape = outputShape.length === 0 ? [1] : outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            // The shared algorithm is mainly used for large reduce size. It fully
            // utilizes the threads in one workgroup to do the reduction. However,
            // when the reduce size is very small or the output shape is too large. It's
            // better to use the plain algorithm to reduce the number of workgroups to
            // speedup. The threthold can be further tuned.
            if (tf.util.sizeFromShape(reduceShape) < 32 ||
                tf.util.sizeFromShape(outputShape) > 1000) {
                this.type = 'plain';
                this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            }
            else {
                this.type = 'shared';
                // A work group only outputs a data, so we transfer [1, 1, 1] to compute
                // dispatch size.
                this.dispatch =
                    computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);
            }
            this.inputShape = inputShape;
            this.shaderKey = `argMinMax_${this.op}_${this.type}`;
        }
        getUserCode() {
            const getInputShapeLastDim = () => {
                if (this.inputShape.length === 1) {
                    return 'uniforms.xShape';
                }
                else {
                    return `uniforms.xShape.${getCoordsXYZ(this.inputShape.length - 1)}`;
                }
            };
            const splitOutputCoords = () => {
                let snippet = '';
                if (this.outputShape.length === 1) {
                    if (this.inputShape.length !== 1) {
                        snippet += 'outputCoords,';
                    }
                }
                else {
                    for (let i = 0; i < this.outputShape.length; i++) {
                        snippet += `outputCoords.${getCoordsXYZ(i)},`;
                    }
                }
                return snippet;
            };
            if (this.type === 'shared') {
                const sharedMemorySnippet = `
      var<workgroup> xBestIndices : array<i32, ${this.workGroupSize[0]}>;
      var<workgroup> xBestValues : array<f32, ${this.workGroupSize[0]}>;
    `;
                const userCode = `
      fn DIV_CEIL(a : u32, b : u32) -> u32 {
        return ((a - 1u) / b + 1u);
      }

      ${sharedMemorySnippet}

      ${getMainHeaderString('index')} {
        let outputIndex = index / i32(workGroupSizeX);
        let reduceLength = ${getInputShapeLastDim()};

        var bestIndex = i32(localId.x);
        var bestValue = uniforms.infinityValue;
        let outputCoords = getCoordsFromIndex(outputIndex);
        for (var k = i32(localId.x); k < reduceLength && outputIndex < uniforms.size;
            k = k + i32(workGroupSizeX)) {
          let candidate = getX(${splitOutputCoords()} k);
          if (!isnan(candidate) && candidate ${this.op} bestValue) {
            bestValue = candidate;
            bestIndex = k;
          }
        }
        xBestValues[localId.x] = bestValue;
        xBestIndices[localId.x] = bestIndex;
        workgroupBarrier();

        var reduceSize = min(u32(reduceLength), workGroupSizeX);
        for (var currentSize = reduceSize / 2u; reduceSize > 1u;
            currentSize = reduceSize / 2u) {
          let interval = DIV_CEIL(reduceSize, 2u);
          if (localId.x < currentSize) {
            let candidate = xBestValues[localId.x + interval];
            if (candidate ${this.op} bestValue) {
              bestValue = candidate;
              xBestValues[localId.x] = bestValue;
              xBestIndices[localId.x] = xBestIndices[localId.x + interval];
            }
          }
          reduceSize = interval;
          workgroupBarrier();
        }

        if (localId.x == 0u && outputIndex < uniforms.size) {
          setOutputAtIndexI32(outputIndex, xBestIndices[localId.x]);
        }
      }
    `;
                return userCode;
            }
            else {
                const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let outputCoords = getCoordsFromIndex(index);
          var bestIndex = 0;
          var bestValue = getX(${splitOutputCoords()} 0);
          let reduceLength = ${getInputShapeLastDim()};
          for (var i = 1; i < reduceLength; i++) {
            let candidate = getX(${splitOutputCoords()} i);
            if (candidate ${this.op} bestValue) {
              bestValue = candidate;
              bestIndex = i;
            }
          }
          setOutputAtIndexI32(index, bestIndex);
        }
      }
      `;
                return userCode;
            }
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class TransposeSharedProgram {
        constructor(aShape, newDim) {
            this.variableNames = ['A'];
            // Note that the maximum number of workgroup invocations by webgpu is 256.
            this.workGroupSize = [16, 16, 1];
            const outputShape = new Array(aShape.length);
            for (let i = 0; i < outputShape.length; i++) {
                outputShape[i] = aShape[newDim[i]];
            }
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [0], y: [1] };
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [1, 1, 1]);
            this.shaderKey = 'transposeShared';
        }
        getUserCode() {
            const userCode = `
      const TILE_DIM = ${this.workGroupSize[0]};
      var<workgroup> tile : array<array<f32, ${this.workGroupSize[0] + 1}>, ${this.workGroupSize[0]}>;
      ${getWorkGroupSizeString()}
      fn _start(@builtin(local_invocation_id) localId : vec3<u32>,
                @builtin(workgroup_id) workgroupId : vec3<u32>) {
        var x = i32(workgroupId.x) * TILE_DIM + i32(localId.x);
        var y = i32(workgroupId.y) * TILE_DIM + i32(localId.y);
        let width = uniforms.outShape[0];
        let height = uniforms.outShape[1];
        if (x < width && y < height) {
          tile[localId.y][localId.x] = A[y * width + x];
        }
        workgroupBarrier();

        x = i32(workgroupId.y) * TILE_DIM + i32(localId.x);
        y = i32(workgroupId.x) * TILE_DIM + i32(localId.y);
        if (x < height && y < width) {
          setOutputAtIndex((y * height + x), tile[localId.x]
            [localId.y]);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class TransposeProgram {
        constructor(aShape, newDim) {
            this.variableNames = ['A'];
            this.workPerThread = 1;
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            const outputShape = new Array(aShape.length);
            for (let i = 0; i < outputShape.length; i++) {
                outputShape[i] = aShape[newDim[i]];
            }
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            this.newDim = newDim;
            this.shaderKey = `transpose_${newDim}`;
        }
        getUserCode() {
            const dtype = getCoordsDataType(this.outputShape.length);
            const switched = getSwitchedCoords(this.newDim);
            const userCode = `
      ${getMainHeaderString('index')} {
        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if(flatIndex < uniforms.size) {
            let resRC = getCoordsFromIndex(flatIndex);
            setOutputAtIndex(flatIndex, A[getIndexFromCoords${this.outputShape.length}D(
              ${dtype}(${switched}), uniforms.aShape)]);
          }
        }
      }
    `;
            return userCode;
        }
    }
    function getSwitchedCoords(newDim) {
        const rank = newDim.length;
        if (rank > 6) {
            throw Error(`Transpose for rank ${rank} is not yet supported`);
        }
        const switchedCoords = new Array(rank);
        for (let i = 0; i < newDim.length; i++) {
            switchedCoords[newDim[i]] = `resRC.${getCoordsXYZ(i)}`;
        }
        return switchedCoords.join();
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function transpose(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { perm } = attrs;
        const webgpuBackend = backend;
        const xRank = x.shape.length;
        const newShape = new Array(xRank);
        for (let i = 0; i < newShape.length; i++) {
            newShape[i] = x.shape[perm[i]];
        }
        if (backend.shouldExecuteOnCPU([x])) {
            const xData = webgpuBackend.tensorMap.get(x.dataId);
            const values = xData.values;
            const outValues = transposeImplCPU(values, x.shape, x.dtype, perm, newShape);
            return backend.makeTensorInfo(newShape, x.dtype, outValues);
        }
        if (x.shape.length === 2 && tf.util.arraysEqual(perm, [1, 0])) {
            const program = new TransposeSharedProgram(x.shape, perm);
            return webgpuBackend.runWebGPUProgram(program, [x], x.dtype);
        }
        const program = new TransposeProgram(x.shape, perm);
        return webgpuBackend.runWebGPUProgram(program, [x], x.dtype);
    }
    const transposeConfig = {
        kernelName: tf.Transpose,
        backendName: 'webgpu',
        kernelFunc: transpose
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function argMax(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis } = attrs;
        let axes = tf.util.parseAxisParam(axis, x.shape);
        const permutedAxes = tf.backend_util.getAxesPermutation(axes, x.shape.length);
        let $x = x;
        const intermediateTensorInfos = [];
        if (permutedAxes != null) {
            $x = transpose({ inputs: { x }, backend, attrs: { perm: permutedAxes } });
            intermediateTensorInfos.push($x);
            axes = tf.backend_util.getInnerMostAxes(axes.length, $x.shape.length);
        }
        tf.backend_util.assertAxesAreInnerMostDims('argMax', [axes[0]], $x.shape.length);
        const program = new ArgMinMaxProgram($x.shape, axes[0], 'max');
        const uniformData = [{ type: 'float32', data: [Number.NEGATIVE_INFINITY] }];
        const out = backend.runWebGPUProgram(program, [$x], 'int32', uniformData);
        intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));
        return out;
    }
    const argMaxConfig = {
        kernelName: tf.ArgMax,
        backendName: 'webgpu',
        kernelFunc: argMax
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function argMin(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis } = attrs;
        let axes = tf.util.parseAxisParam(axis, x.shape);
        const permutedAxes = tf.backend_util.getAxesPermutation(axes, x.shape.length);
        let $x = x;
        const intermediateTensorInfos = [];
        if (permutedAxes != null) {
            $x = transpose({ inputs: { x }, backend, attrs: { perm: permutedAxes } });
            intermediateTensorInfos.push($x);
            axes = tf.backend_util.getInnerMostAxes(axes.length, $x.shape.length);
        }
        tf.backend_util.assertAxesAreInnerMostDims('argMin', [axes[0]], $x.shape.length);
        const program = new ArgMinMaxProgram($x.shape, axes[0], 'min');
        const uniformData = [{ type: 'float32', data: [Number.POSITIVE_INFINITY] }];
        const out = backend.runWebGPUProgram(program, [$x], 'int32', uniformData);
        intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));
        return out;
    }
    const argMinConfig = {
        kernelName: tf.ArgMin,
        backendName: 'webgpu',
        kernelFunc: argMin
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const atan2 = binaryKernelFunc({ opType: BinaryOpType.ATAN2 });
    const atan2Config = {
        kernelName: tf.Atan2,
        backendName: 'webgpu',
        kernelFunc: atan2
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class Pool2DProgram {
        constructor(convInfo, poolType) {
            this.variableNames = ['x'];
            this.uniforms = `stride : vec2<i32>, pad : vec2<i32>, dilation : vec2<i32>, convDims : vec2<i32>, filterDims : vec2<i32>,`;
            // TODO(jiajia.qin@intel.com): Dynamically choose different workGroupSize for
            // different output shapes.
            this.workGroupSize = [128, 1, 1];
            this.size = true;
            this.outputShape = convInfo.outShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `pool2D_${poolType}`;
            this.poolType = poolType;
        }
        getUserCode() {
            let updateSnippet = `resultValue = max(value, resultValue);`;
            if (this.poolType === 'avg') {
                updateSnippet = `resultValue = resultValue + value; count = count + 1.0;`;
            }
            let returnValue = `resultValue`;
            if (this.poolType === 'avg') {
                returnValue = `resultValue / count`;
            }
            const userCode = `
      ${getMainHeaderString('index')} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
          let batch = coords[0];
          let xRCCorner = vec2<i32>(coords.yz) * uniforms.stride - uniforms.pad;
          let xRCorner = xRCCorner.x;
          let xCCorner = xRCCorner.y;

          var resultValue = ${this.poolType === 'avg' ? '0.0' : '-1.0 / pow(10.0, -20.0)'};
          var count = 0.0;

          for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + uniforms.dilation.x) {
            let xR = xRCorner + wR;

            if (xR < 0 || xR >= uniforms.convDims.x) {
              continue;
            }

            for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + uniforms.dilation.y) {
              let xC = xCCorner + wC;
              if (xC < 0 || xC >= uniforms.convDims.y) {
                continue;
              }

              let value = getX(batch, xR, xC, coords[3]);
              ${updateSnippet}
            }
          }

          setOutputAtIndex(index, ${returnValue});
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class PoolWithFilterSizeEqualsOneProgram {
        constructor(convInfo) {
            this.variableNames = ['x'];
            this.uniforms = `stride : vec2<i32>,`;
            this.workGroupSize = [256, 1, 1];
            this.size = true;
            this.outputShape = convInfo.outShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = 'poolWithFilterSizeEqualsOne';
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let batch = coords[0];
          let d = coords[3];

          let xRCCorner = coords.yz * uniforms.stride;
          let xRCorner = xRCCorner.x;
          let xCCorner = xRCCorner.y;

          let value = getX(batch, xRCorner, xCCorner, d);
          setOutputAtIndex(index, value);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ReduceProgram {
        constructor(reduceInfo, reduceType) {
            this.workGroupSize = [64, 1, 1];
            this.variableNames = ['x'];
            this.uniforms = 'reduceSize : i32,';
            this.size = true;
            this.inputShape = [reduceInfo.batchSize, reduceInfo.inSize];
            const [outputShape,] = tf.backend_util.computeOutAndReduceShapes(this.inputShape, [1]);
            this.outputShape = outputShape.length === 0 ? [1] : outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            // A work group only outputs a data, so we transfer [1, 1, 1] to compute
            // dispatch size.
            this.dispatch =
                computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);
            this.reduceType = reduceType;
            this.shaderKey = `reduce_${reduceType}`;
        }
        getUserCode() {
            let reduceOp = ``;
            let initValue = '0.0';
            if (this.reduceType === 'min' || this.reduceType === 'max') {
                reduceOp = `
         if (isnan(candidate)) {
          bestValue = uniforms.NAN;
         } else if (!isnan(bestValue) && candidate ${this.reduceType === 'min' ? '<' : '>'} bestValue)
           {  bestValue = candidate; }`;
                initValue = 'f32(x[offset])';
            }
            else if (this.reduceType === 'sum' || this.reduceType === 'mean') {
                reduceOp = ' bestValue = bestValue + candidate; ';
            }
            else if (this.reduceType === 'prod') {
                reduceOp = ' bestValue = bestValue * candidate; ';
                initValue = '1.0';
            }
            const outputSnippet = this.reduceType === 'mean' ?
                // tslint:disable-next-line:max-line-length
                `setOutputAtIndex(outputIndex, bestValue / f32(uniforms.reduceSize));` :
                `setOutputAtIndex(outputIndex, bestValue);`;
            const sharedMemorySnippet = `
         var<workgroup> xBestValues : array<f32, ${this.workGroupSize[0]}>;
       `;
            const userCode = `
       fn DIV_CEIL(a : u32, b : u32) -> u32 {
        return ((a - 1u) / b + 1u);
       }

       ${sharedMemorySnippet}
       fn getOffset(outputIndex : i32) -> i32 {
         let outputCoords = getCoordsFromIndex(outputIndex);
         let offset = ${this.outputShape.length === 1 ?
            'outputCoords' :
            'outputCoords[0]'} * uniforms.reduceSize;
          return offset;
       }
       ${getMainHeaderString('index')} {
         let outputIndex = index / i32(workGroupSizeX);
         let offset = getOffset(outputIndex);
         var bestValue = ${initValue};
         let Length = uniforms.reduceSize;
         let WorkPerThread = DIV_CEIL(u32(Length), workGroupSizeX);
         for (var k = i32(localId.x); k < Length && outputIndex < uniforms.size;
             k = k + i32(workGroupSizeX)) {
           let candidate = f32(x[offset + k]);
           ${reduceOp}
         }
         xBestValues[localId.x] = bestValue;
         workgroupBarrier();

         var reduceSize = min(u32(Length), workGroupSizeX);
         for (var currentSize = reduceSize / 2u; reduceSize > 1u;
             currentSize = reduceSize / 2u) {
           let interval = DIV_CEIL(reduceSize, 2u);
           if (localId.x < currentSize) {
            let candidate = xBestValues[localId.x + interval];
            ${reduceOp}
            xBestValues[localId.x] = bestValue;
           }
           reduceSize = interval;
           workgroupBarrier();
         }

         if (localId.x == 0u && outputIndex < uniforms.size) {
          ${outputSnippet}
        }
       }
     `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function reduce(x, axis, keepDims, reduceType, backend) {
        const xRank = x.shape.length;
        const toDispose = [];
        const origAxes = tf.util.parseAxisParam(axis, x.shape);
        let axes = origAxes;
        const permutedAxes = tf.backend_util.getAxesPermutation(axes, xRank);
        let input = x;
        if (permutedAxes != null) {
            input = transpose({ inputs: { x }, attrs: { perm: permutedAxes }, backend });
            axes = tf.backend_util.getInnerMostAxes(axes.length, xRank);
            toDispose.push(input);
        }
        tf.backend_util.assertAxesAreInnerMostDims(reduceType, axes, xRank);
        const [reduceOutShape, reduceShape] = tf.backend_util.computeOutAndReduceShapes(input.shape, axes);
        let resOutShape = reduceOutShape;
        if (keepDims) {
            // rather than reshape at the end, set the target shape here.
            resOutShape = tf.backend_util.expandShapeToKeepDim(reduceOutShape, origAxes);
        }
        let res;
        if ((reduceType === 'max' || reduceType === 'prod') &&
            backend.shouldExecuteOnCPU([input])) {
            const xVals = backend.tensorMap.get(input.dataId).values;
            switch (reduceType) {
                case 'max':
                    const outValues = maxImplCPU(xVals, tf.util.sizeFromShape(reduceShape), resOutShape, x.dtype);
                    res = backend.makeTensorInfo(resOutShape, x.dtype, outValues);
                    break;
                case 'prod':
                    const { outVals, outShape, outDtype } = prodImplCPU(input.shape, input.dtype, xVals, axes);
                    res = backend.makeTensorInfo(outShape, outDtype, outVals);
                    break;
                default:
                    throw new Error(`${reduceType} CPU implementation is not yet supported.`);
            }
        }
        else {
            const inSize = tf.util.sizeFromShape(reduceShape);
            const xSize = tf.util.sizeFromShape(input.shape);
            const batchSize = xSize / inSize;
            const reduceInfo = { windowSize: inSize, inSize, batchSize, outSize: 1 };
            const dtype = reduceType === 'mean' ? 'float32' : tf.sumOutType(x.dtype);
            const uniformData = [
                { type: 'int32', data: [inSize] },
            ];
            const program = new ReduceProgram(reduceInfo, reduceType);
            const reduced = backend.runWebGPUProgram(program, [input], dtype, uniformData);
            toDispose.push(reduced);
            res = reshape({ inputs: { x: reduced }, attrs: { shape: resOutShape }, backend });
        }
        toDispose.forEach(t => backend.disposeData(t.dataId));
        return res;
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function max(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { reductionIndices, keepDims } = attrs;
        return reduce(x, reductionIndices, keepDims, 'max', backend);
    }
    const maxConfig = {
        kernelName: tf.Max,
        backendName: 'webgpu',
        kernelFunc: max
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function mean(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { keepDims, axis } = attrs;
        return reduce(x, axis, keepDims, 'mean', backend);
    }
    const meanConfig = {
        kernelName: tf.Mean,
        backendName: 'webgpu',
        kernelFunc: mean
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function poolImpl(x, convInfo, poolType, backend) {
        if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 &&
            tf.util.arraysEqual(convInfo.inShape, convInfo.outShape)) {
            return identity({ inputs: { x }, backend });
        }
        if (convInfo.filterWidth === convInfo.inWidth &&
            convInfo.filterHeight === convInfo.inHeight && convInfo.batchSize === 1 &&
            convInfo.padInfo.type === 'VALID') {
            const length = x.shape.length;
            const reshapeX = reshape({
                inputs: { x },
                backend,
                attrs: {
                    shape: [
                        x.shape[length - 3] * x.shape[length - 2] /* height * width */,
                        x.shape[length - 1] /* channel */
                    ]
                }
            });
            let reduceX;
            if (poolType === 'avg') {
                reduceX = mean({ inputs: { x: reshapeX }, backend, attrs: { axis: 0, keepDims: false } });
            }
            else {
                tf.util.assert(poolType === 'max', () => `Invalid pool type ${poolType}`);
                reduceX = max({
                    inputs: { x: reshapeX },
                    backend,
                    attrs: { reductionIndices: 0, keepDims: false }
                });
            }
            const result = reshape({ inputs: { x: reduceX }, backend, attrs: { shape: convInfo.outShape } });
            backend.disposeData(reshapeX.dataId);
            backend.disposeData(reduceX.dataId);
            return result;
        }
        let program;
        const dimensions = [{ type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth] }];
        if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1) {
            program = new PoolWithFilterSizeEqualsOneProgram(convInfo);
        }
        else {
            if (poolType === 'avg') {
                program = new Pool2DProgram(convInfo, 'avg');
            }
            else {
                tf.util.assert(poolType === 'max', () => `Invalid pool type ${poolType}`);
                program = new Pool2DProgram(convInfo, 'max');
            }
            dimensions.push({ type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left] }, {
                type: 'int32',
                data: [convInfo.dilationHeight, convInfo.dilationWidth]
            }, { type: 'int32', data: [convInfo.inHeight, convInfo.inWidth] }, {
                type: 'int32',
                data: [convInfo.effectiveFilterHeight, convInfo.effectiveFilterWidth]
            });
        }
        return backend.runWebGPUProgram(program, [x], x.dtype, dimensions);
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function avgPool(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { filterSize, strides, pad, dimRoundingMode } = attrs;
        const dilations = 1;
        const convInfo = tf.backend_util.computePool2DInfo(x.shape, filterSize, strides, dilations, pad, dimRoundingMode);
        return poolImpl(x, convInfo, 'avg', backend);
    }
    const avgPoolConfig = {
        kernelName: tf.AvgPool,
        backendName: 'webgpu',
        kernelFunc: avgPool
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function batchMatMul(args) {
        const { inputs, backend, attrs } = args;
        const { a, b } = inputs;
        const { transposeA, transposeB } = attrs;
        return batchMatMulImpl({ a, b, transposeA, transposeB, backend });
    }
    const batchMatMulConfig = {
        kernelName: tf.BatchMatMul,
        backendName: 'webgpu',
        kernelFunc: batchMatMul,
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class SliceProgram {
        constructor(start, destSize) {
            this.variableNames = ['source'];
            this.workPerThread = 1;
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = destSize;
            this.rank = destSize.length;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            this.start = start;
            this.uniforms = `start : ${getCoordsDataType(start.length)}, `;
            this.shaderKey = 'slice';
        }
        getUserCode() {
            const dtype = getCoordsDataType(this.rank);
            const sourceCoords = getCoords$1(this.rank);
            let coordSum;
            if (this.start.length === 1) {
                coordSum = this.outputShape.map((_, i) => {
                    return `sourceLoc = uniforms.start + coords;`;
                });
            }
            else {
                coordSum = this.outputShape.map((_, i) => {
                    return `sourceLoc.${coords[i]} = uniforms.start.${getCoordsXYZ(i)} + coords.${coords[i]};`;
                });
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          var sourceLoc : ${dtype};
          let coords = getCoordsFromIndex(index);
          ${coordSum.join('\n')}
          setOutputAtIndex(index, getSource(${sourceCoords}));
        }
      }
    `;
            return userCode;
        }
    }
    const coords = ['x', 'y', 'z', 'w', 'u', 'v'];
    function getCoords$1(rank) {
        if (rank === 1) {
            return 'sourceLoc';
        }
        else if (rank <= 6) {
            return coords.slice(0, rank).map(coord => `sourceLoc.${coord}`).join(',');
        }
        else {
            throw Error(`Slicing for rank ${rank} is not yet supported`);
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function slice(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { begin, size } = attrs;
        const [$begin, $size] = tf.slice_util.parseSliceParams(x, begin, size);
        tf.slice_util.assertParamsValid(x, $begin, $size);
        if (backend.shouldExecuteOnCPU([x]) || x.dtype === 'string') {
            const xBufferInfo = backend.tensorMap.get(x.dataId);
            const outValues = sliceImplCPU(xBufferInfo.values, $begin, $size, x.shape, x.dtype);
            return backend.makeTensorInfo($size, x.dtype, outValues);
        }
        if (tf.util.sizeFromShape($size) === 0) {
            return backend.makeTensorInfo($size, x.dtype, []);
        }
        // TODO(xing.xu): Add shadow slice support.
        const program = new SliceProgram($begin, $size);
        const uniformData = [{ type: 'int32', data: $begin }];
        return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
    }
    const sliceConfig = {
        kernelName: tf.Slice,
        backendName: 'webgpu',
        kernelFunc: slice
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const batchToSpaceND = (args) => {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { blockShape, crops } = attrs;
        tf.util.assert(x.shape.length <= 4, () => 'batchToSpaceND for rank > 4 with a WebGPU backend not ' +
            'implemented yet');
        const prod = blockShape.reduce((a, b) => a * b);
        const reshaped = tf.backend_util.getReshaped(x.shape, blockShape, prod);
        const permuted = tf.backend_util.getPermuted(reshaped.length, blockShape.length);
        const reshapedPermuted = tf.backend_util.getReshapedPermuted(x.shape, blockShape, prod);
        const sliceBeginCoords = tf.backend_util.getSliceBeginCoords(crops, blockShape.length);
        const sliceSize = tf.backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);
        const toDispose = [];
        const reshapedIntermediate = reshape({ inputs: { x }, backend, attrs: { shape: reshaped } });
        const transposedIntermediate = transpose({ inputs: { x: reshapedIntermediate }, backend, attrs: { perm: permuted } });
        const reshapedIntermediate2 = reshape({
            inputs: { x: transposedIntermediate },
            backend,
            attrs: { shape: reshapedPermuted }
        });
        const sliced = slice({
            inputs: { x: reshapedIntermediate2 },
            backend,
            attrs: { begin: sliceBeginCoords, size: sliceSize }
        });
        toDispose.push(reshapedIntermediate);
        toDispose.push(transposedIntermediate);
        toDispose.push(reshapedIntermediate2);
        toDispose.forEach(t => backend.disposeData(t.dataId));
        return sliced;
    };
    const batchToSpaceNDConfig = {
        kernelName: tf.BatchToSpaceND,
        backendName: 'webgpu',
        kernelFunc: batchToSpaceND
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const notEqual = binaryKernelFunc({
        opType: BinaryOpType.NOT_EQUAL,
        dtype: 'bool',
        cpuKernelImpl: notEqualImplCPU
    });
    const notEqualConfig = {
        kernelName: tf.NotEqual,
        backendName: 'webgpu',
        kernelFunc: notEqual
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function real(args) {
        const { inputs, backend } = args;
        const { input } = inputs;
        const inputData = backend.tensorMap.get(input.dataId);
        return identity({ inputs: { x: inputData.complexTensorInfos.real }, backend });
    }
    const realConfig = {
        kernelName: tf.Real,
        backendName: 'webgpu',
        kernelFunc: real
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function int(input, backend) {
        const program = new UnaryOpProgram(input.shape, UnaryOpType.TO_INT);
        const output = backend.runWebGPUProgram(program, [input], 'int32');
        return { dataId: output.dataId, shape: output.shape, dtype: output.dtype };
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function cast(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { dtype } = attrs;
        // Casting to complex64.
        if (dtype === 'complex64') {
            if (x.dtype === 'complex64') {
                return identity({ inputs: { x }, backend });
            }
            // TODO: Import kernel function once zeros is modularized.
            const zerosTensor = tf__namespace.zeros(x.shape);
            const floatX = cast({ inputs: { x }, backend, attrs: { dtype: 'float32' } });
            const result = complex({ inputs: { real: floatX, imag: zerosTensor }, backend });
            zerosTensor.dispose();
            backend.disposeData(floatX.dataId);
            return result;
        }
        // Casting from complex64
        if (x.dtype === 'complex64') {
            const realPart = real({ inputs: { input: x }, backend });
            const result = cast({ inputs: { x: realPart }, backend, attrs: { dtype } });
            backend.disposeData(realPart.dataId);
            return result;
        }
        if (!tf.util.hasEncodingLoss(x.dtype, dtype)) {
            // We don't change the underlying data, since we cast to higher
            // precision.
            const result = identity({ inputs: { x }, backend });
            return { dataId: result.dataId, shape: result.shape, dtype };
        }
        if (backend.shouldExecuteOnCPU([x])) {
            const values = backend.tensorMap.get(x.dataId).values;
            const [resultShape, resultType, resultData] = castImplCPU(values, x.shape, x.dtype, dtype);
            return backend.makeTensorInfo(resultShape, resultType, resultData);
        }
        if (dtype === 'int32') {
            return int(x, backend);
        }
        if (dtype === 'bool') {
            const zerosTensorInfo = backend.makeTensorInfo([], 'bool', tf.util.getTypedArrayFromDType('bool', 1));
            const binaryInputs = { a: x, b: zerosTensorInfo };
            const result = notEqual({ inputs: binaryInputs, backend });
            backend.disposeData(zerosTensorInfo.dataId);
            return result;
        }
        throw new Error(`Error in Cast: failed to cast ${x.dtype} to ${dtype}`);
    }
    const castConfig = {
        kernelName: tf.Cast,
        backendName: 'webgpu',
        kernelFunc: cast
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const ceil = unaryKernelFunc({ opType: UnaryOpType.CEIL, cpuKernelImpl: ceilImplCPU });
    const ceilConfig = {
        kernelName: tf.Ceil,
        backendName: 'webgpu',
        kernelFunc: ceil
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ClipVec4Program {
        constructor(outputShape) {
            this.variableNames = ['A'];
            this.uniforms = 'minVal : f32, maxVal : f32,';
            this.workPerThread = 4;
            this.workGroupSize = [64, 1, 1];
            this.isVec4 = true;
            this.size = true;
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            this.shaderKey = 'clipVec4';
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if(index < uniforms.size) {
          let value = getAByOutputIndex(index);
          var clampedValue : vec4<f32>;
          for (var i = 0; i < 4; i = i + 1) {
            if (isnan(value[i])) {
              clampedValue[i] = value[i];
            } else {
              clampedValue[i] = clamp(value[i], uniforms.minVal, uniforms.maxVal);
            }
          }

          setOutputAtIndex(index, clampedValue);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ClipProgram {
        constructor(outputShape) {
            this.variableNames = ['A'];
            this.uniforms = 'minVal : f32, maxVal : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = 'clip';
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if(index < uniforms.size) {
          let value = getAByOutputIndex(index);
          if (isnan(value)) {
            setOutputAtIndex(index, value);
            return;
          }
          setOutputAtIndex(index, clamp(value, uniforms.minVal, uniforms.maxVal));
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function clipByValue(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { clipValueMin, clipValueMax } = attrs;
        let program;
        const uniformData = [
            { type: 'float32', data: [clipValueMin] },
            { type: 'float32', data: [clipValueMax] }
        ];
        if (tf.util.sizeFromShape(x.shape) % 4 === 0) {
            program = new ClipVec4Program(x.shape);
        }
        else {
            program = new ClipProgram(x.shape);
        }
        return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
    }
    const clipByValueConfig = {
        kernelName: tf.ClipByValue,
        backendName: 'webgpu',
        kernelFunc: clipByValue
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ConcatProgram {
        constructor(shapes) {
            this.uniforms = '';
            this.workPerThread = 1;
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape =
                tf.backend_util.computeOutShape(shapes, 1 /* axis */);
            this.variableNames = shapes.map((_, i) => `T${i}`);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            this.offsetLength = shapes.length - 1;
            for (let i = 0; i < this.offsetLength; i++) {
                this.uniforms += `offset${i} : i32,`;
            }
            this.shaderKey = 'concat';
        }
        getUserCode() {
            const snippets = [];
            if (this.offsetLength > 0) {
                snippets.push(`if (yC < uniforms.offset0){ setOutputAtCoords(coords.x, coords.y, getT0(yR, yC)); }`);
                for (let i = 1; i < this.offsetLength; i++) {
                    snippets.push(`else if (yC < uniforms.offset${[i]}){ ` +
                        `setOutputAtCoords(coords.x, coords.y, getT${i}(yR, yC - uniforms.offset${i - 1})); }`);
                }
                const lastIndex = this.offsetLength;
                const lastShiftIndex = this.offsetLength - 1;
                snippets.push(`else { setOutputAtCoords(coords.x, coords.y, getT${lastIndex}(yR, yC - uniforms.offset${lastShiftIndex})); }`);
            }
            else {
                snippets.push(`setOutputAtCoords(coords.x, coords.y, getT0(yR, yC));`);
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if(flatIndex < uniforms.size) {
            let coords = getCoordsFromIndex(flatIndex);
            let yR = coords.x;
            let yC = coords.y;

            ${snippets.join('\n        ')}
          }
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function imag(args) {
        const { inputs, backend } = args;
        const { input } = inputs;
        const inputData = backend.tensorMap.get(input.dataId);
        return identity({ inputs: { x: inputData.complexTensorInfos.imag }, backend });
    }
    const imagConfig = {
        kernelName: tf.Imag,
        backendName: 'webgpu',
        kernelFunc: imag
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function concatImpl(inputs, axis, backend) {
        const dtype = inputs[0].dtype;
        if (dtype === 'complex64') {
            const reals = inputs.map((t) => real({ inputs: { input: t }, backend }));
            const imags = inputs.map((t) => imag({ inputs: { input: t }, backend }));
            const realConcated = concatImpl(reals, axis, backend);
            const imagConcated = concatImpl(imags, axis, backend);
            const result = complex({ inputs: { real: realConcated, imag: imagConcated }, backend });
            reals.forEach(r => backend.disposeData(r.dataId));
            imags.forEach(i => backend.disposeData(i.dataId));
            backend.disposeData(realConcated.dataId);
            backend.disposeData(imagConcated.dataId);
            return result;
        }
        let runOnCpu = backend.shouldExecuteOnCPU(inputs);
        // Run on cpu if dtype is string. For string, the backend represents it
        // as Uint8Array[], where each Uint8Array is a character. Given that the
        // computation is only on the outer array, uploading the whole data onto
        // gpu is wasteful. Also, currently webgpu doesn't have a design to
        // upload and retrieve Uint8Array[] between cpu and gpu. Therefore, we
        // just run the kernel on cpu if dtype is string.
        if (dtype === 'string') {
            runOnCpu = true;
        }
        if (runOnCpu) {
            // Any concat of n-dimensional tensors across any axis can be reduced to
            // a concatenation of two-dimensional tensors across the axis 1 by first
            // partitioning the axes of the original tensors into those less than the
            // axis to be concatenated and the rest. Then reshape the tensors
            // into a two-dimensional tensor by collapsing these two sets of axes and
            // concatenate the resulting matrices across the axis 1, finally reshaping
            // the result to have the proper shape.
            const tensors2D = inputs.map(t => {
                const innerSize = tf.util.sizeFromShape(t.shape.slice(axis));
                const shape = [-1, innerSize];
                return reshape({ inputs: { x: t }, backend, attrs: { shape } });
            });
            const inputsValShapes = tensors2D.map(t => {
                return { vals: backend.readSync(t.dataId), shape: t.shape };
            });
            // Concats 2d tensors along axis=1.
            const outShape = tf.backend_util.computeOutShape(tensors2D.map(t => t.shape), 1 /* axis */);
            const simplyConcat = tensors2D[0].shape[0] === 1;
            const outVals = concatImplCPU(inputsValShapes, outShape, dtype, simplyConcat);
            const finalOutShape = tf.backend_util.computeOutShape(inputs.map(t => t.shape), axis);
            const outInfo = backend.makeTensorInfo(finalOutShape, dtype, outVals);
            tensors2D.forEach(t => backend.disposeData(t.dataId));
            return outInfo;
        }
        // There is a storage buffer limitation in compute stage, one for output so
        // the maximum for input is limits.maxStorageBuffersPerShaderStage - 1
        const maxInputNum = backend.device.limits.maxStorageBuffersPerShaderStage - 1;
        if (inputs.length > maxInputNum) {
            const reducedInputs = [];
            for (let i = 0; i < inputs.length; i += maxInputNum) {
                const subArray = inputs.slice(i, i + maxInputNum);
                reducedInputs.push(concatImpl(subArray, axis, backend));
            }
            const result = concatImpl(reducedInputs, axis, backend);
            for (const i of reducedInputs) {
                backend.disposeData(i.dataId);
            }
            return result;
        }
        const { tensors2D, outShape } = computeTensors2D(inputs, axis, backend);
        const shapes = (tensors2D).map(t => t.shape);
        const program = new ConcatProgram(shapes);
        const uniformData = [];
        const offsets = new Array(shapes.length - 1);
        if (offsets.length > 0) {
            offsets[0] = shapes[0][1];
            uniformData.push({ type: 'int32', data: [offsets[0]] });
            for (let i = 1; i < offsets.length; i++) {
                offsets[i] = offsets[i - 1] + shapes[i][1];
                uniformData.push({ type: 'int32', data: [offsets[i]] });
            }
        }
        const res = backend.runWebGPUProgram(program, tensors2D, tensors2D[0].dtype, uniformData);
        tensors2D.forEach(r => backend.disposeData(r.dataId));
        const reshapedResult = reshape({ inputs: { x: res }, backend, attrs: { shape: outShape } });
        backend.disposeData(res.dataId);
        return reshapedResult;
    }
    function computeTensors2D(inputs, axis, backend) {
        const outShape = tf.backend_util.computeOutShape(inputs.map(t => t.shape), axis);
        const tensors2D = inputs.map(t => reshape({
            inputs: { x: t },
            backend,
            attrs: {
                shape: [
                    tf.util.sizeFromShape(t.shape.slice(0, axis)),
                    tf.util.sizeFromShape(t.shape.slice(axis))
                ]
            }
        }));
        return { tensors2D, outShape };
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function concat(args) {
        const { inputs, backend, attrs } = args;
        const { axis } = attrs;
        const $axis = tf.util.parseAxisParam(axis, inputs[0].shape)[0];
        const shapes = inputs.map(t => t.shape);
        tf.backend_util.assertParamsConsistent(shapes, $axis);
        const outShape = tf.backend_util.computeOutShape(inputs.map(t => t.shape), $axis);
        if (tf.util.sizeFromShape(outShape) === 0) {
            return backend.makeTensorInfo(outShape, inputs[0].dtype, []);
        }
        // Keep only non-empty tensors (ignore tensors with 0 in their shape).
        const $inputs = inputs.filter(t => tf.util.sizeFromShape(t.shape) > 0);
        if ($inputs.length === 1) {
            return identity({ inputs: { x: $inputs[0] }, backend });
        }
        return concatImpl($inputs, $axis, backend);
    }
    const concatConfig = {
        kernelName: tf.Concat,
        backendName: 'webgpu',
        kernelFunc: concat
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function conv2dCommonSnippet(isChannelsLast, fitAOuter, fitBOuter, fitInner, addBias = false, activation = null, hasPreluActivationWeights = false, innerElementSizeX = 4, innerElementSizeW = 4, innerElementSize = 4) {
        const getXSnippet = (innerElementSize) => {
            switch (innerElementSize) {
                case 1:
                    return 'resData = x[xIndex];';
                case 3:
                    return 'resData = vec3<f32>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);';
                case 4:
                    return 'resData = x[xIndex / 4];';
                default:
                    throw new Error(`innerElementSize ${innerElementSize} is not supported.`);
            }
        };
        const getWSnippet = (innerElementSize) => {
            switch (innerElementSize) {
                case 1:
                    return 'return W[row * uniforms.wShape[3] + colIn];';
                case 4:
                    return 'return W[row * uniforms.wShape[3] / 4 + colIn];';
                default:
                    throw new Error(`innerElementSize ${innerElementSize} is not supported.`);
            }
        };
        const coordASnippet = isChannelsLast ? `
      let coord = vec4<i32>(batch, xRow, xCol, xCh);
      ` :
            `
      let coord = vec4<i32>(batch, xCh, xRow, xCol);
      `;
        const coordResSnippet = isChannelsLast ? `
      let coords = vec4<i32>(
        batch,
        row / outWidth,
        row % outWidth,
        col);
      ` :
            `
      let coords = vec4<i32>(
        batch,
        row,
        col / outWidth,
        col % outWidth);
      `;
        const xHight = isChannelsLast ? 'uniforms.xShape[1]' : 'uniforms.xShape[2]';
        const xWidth = isChannelsLast ? 'uniforms.xShape[2]' : 'uniforms.xShape[3]';
        const row = isChannelsLast ? 'row' : 'col';
        const col = isChannelsLast ? 'col' : 'row';
        const readXSnippet = `
      let inChannels = uniforms.wShape[2];
      let outWidth = ${isChannelsLast ? 'uniforms.outShape[2]' : 'uniforms.outShape[3]'};
      let outRow = ${row} / outWidth;
      let outCol = ${row} % outWidth;

      let WRow = ${col} / (uniforms.filterDims[1] * inChannels);
      let WCol = ${col} / inChannels % uniforms.filterDims[1];
      let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];
      let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];
      let xCh = ${col} % inChannels;
      var resData = ${typeSnippet(innerElementSizeX)}(0.0);
      // The bounds checking is always needed since we use it to pad zero for
      // the 'same' padding type.
      if (xRow >= 0 && xRow < ${xHight} && xCol >= 0 && xCol < ${xWidth}) {
        ${coordASnippet}
        let xIndex = getIndexFromCoords4D(coord, uniforms.xShape);
        ${getXSnippet(innerElementSizeX)}
      }
      return resData;`;
        const sampleX = isChannelsLast ? (fitAOuter && fitInner ? `
      let col = colIn * ${innerElementSizeX};
      ${readXSnippet}` :
            `
      let col = colIn * ${innerElementSizeX};
      if (row < uniforms.dimAOuter && col < uniforms.dimInner) {
        ${readXSnippet}
      }
      return ${typeSnippet(innerElementSizeX)}(0.0);`) :
            (fitInner && fitBOuter ? `
      let col = colIn * ${innerElementSizeX};
      ${readXSnippet}` :
                `
      let col = colIn * ${innerElementSizeX};
      if (row < uniforms.dimInner && col < uniforms.dimBOuter) {
        ${readXSnippet}
      }
      return ${typeSnippet(innerElementSizeX)}(0.0);`);
        const sampleW = `${getWSnippet(innerElementSizeW)}`;
        const resType = typeSnippet(innerElementSize);
        const aType = isChannelsLast ? typeSnippet(innerElementSizeX) :
            typeSnippet(innerElementSizeW);
        const bType = isChannelsLast ? typeSnippet(innerElementSizeW) :
            typeSnippet(innerElementSizeX);
        const userCode = `
      ${activationFnSnippet(activation, hasPreluActivationWeights, innerElementSize === 4, 4)}
      fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${aType} {
        ${isChannelsLast ? sampleX : sampleW}
      }

      fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${bType} {
        ${isChannelsLast ? sampleW : sampleX}
      }

      fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${resType}) {
        let col = colIn * ${innerElementSize};
        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)
        {
        var value = valueIn;
        let outWidth = ${isChannelsLast ? 'uniforms.outShape[2]' : 'uniforms.outShape[3]'};
        ${coordResSnippet}
        ${biasActivationSnippet(addBias, activation)}
        setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }`;
        return userCode;
    }
    class Conv2DMMProgram {
        constructor(convInfo, dimAOuter, dimBOuter, dimInner, addBias = false, activation = null, hasPreluActivationWeights = false, sequentialAccessByThreads = false) {
            this.variableNames = ['x', 'W'];
            this.uniforms = `filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>, dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
            this.outputShape = convInfo.outShape;
            this.isChannelsLast = convInfo.dataFormat === 'channelsLast';
            this.isVec4 =
                (((convInfo.inChannels % 4 === 0 || convInfo.inChannels % 3 === 0) &&
                    this.isChannelsLast) ||
                    (convInfo.outWidth % 4 === 0 && !this.isChannelsLast)) &&
                    convInfo.outChannels % 4 === 0;
            this.dispatchLayout = this.isChannelsLast ? { x: [3], y: [1, 2], z: [0] } :
                { x: [2, 3], y: [1], z: [0] };
            this.workGroupSize = computeWorkGroupSizeForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
            this.elementsPerThread = computeWorkPerThreadForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, this.elementsPerThread);
            if (this.isVec4) {
                if (this.isChannelsLast && convInfo.inChannels % 4 !== 0) {
                    this.innerElementSize = 3;
                    this.variableTypes = ['f32', 'vec4<f32>'];
                }
                else {
                    this.innerElementSize = 4;
                    this.variableTypes = ['vec4<f32>', 'vec4<f32>'];
                }
                if (addBias) {
                    this.variableNames.push('bias');
                    this.variableTypes.push('vec4<f32>');
                }
                if (hasPreluActivationWeights) {
                    this.variableNames.push('preluActivationWeights');
                    this.variableTypes.push('vec4<f32>');
                }
            }
            else {
                this.innerElementSize = this.elementsPerThread[0];
                if (addBias) {
                    this.variableNames.push('bias');
                }
                if (hasPreluActivationWeights) {
                    this.variableNames.push('preluActivationWeights');
                }
            }
            this.sequentialAccessByThreads = sequentialAccessByThreads;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivationWeights = hasPreluActivationWeights;
            this.tileAOuter = this.workGroupSize[1] * this.elementsPerThread[1];
            this.tileBOuter = this.workGroupSize[0] * this.elementsPerThread[0];
            this.tileInner = Math.max(this.workGroupSize[0] * this.innerElementSize, this.workGroupSize[1]);
            this.fitAOuter = dimAOuter % this.tileAOuter === 0;
            this.fitBOuter = dimBOuter % this.tileBOuter === 0;
            this.fitInner = dimInner % this.tileInner === 0;
            this.shaderKey = `conv2DMM_${this.elementsPerThread}_${this.activation}}_${this.fitAOuter}_${this.fitBOuter}_${this.fitInner}_${this.isVec4}_${this.innerElementSize}_${this.isChannelsLast}_${this.sequentialAccessByThreads}`;
        }
        getUserCode() {
            const matMulSource = this.isVec4 ?
                makeMatMulPackedVec4Source(this.elementsPerThread, this.workGroupSize, !this.isChannelsLast, this.tileInner) :
                makeMatMulPackedSource(this.elementsPerThread, this.workGroupSize, !this.isChannelsLast, this.tileInner, false, null, this.sequentialAccessByThreads);
            const elementsSize = this.isVec4 ? [this.innerElementSize, 4, 4] : [1, 1, 1];
            const userCode = `
    ${conv2dCommonSnippet(this.isChannelsLast, this.fitAOuter, this.fitBOuter, this.fitInner, this.addBias, this.activation, this.hasPreluActivationWeights, elementsSize[0], elementsSize[1], elementsSize[2])}
    ${matMulSource}
  `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class Conv2DNaiveProgram {
        constructor(convInfo, addBias = false, activation = null, hasPreluActivationWeights = false) {
            this.variableNames = ['x', 'W'];
            this.uniforms = 'filterDims: vec2<i32>, pad: vec2<i32>, stride: vec2<i32>, dilation: vec2<i32>,';
            this.workGroupSize = [4, 4, 8];
            this.outputShape = convInfo.outShape;
            this.isChannelsLast = convInfo.dataFormat === 'channelsLast';
            this.dispatchLayout = this.isChannelsLast ? { x: [2], y: [1], z: [0, 3] } :
                { x: [3], y: [2], z: [0, 1] };
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivationWeights = hasPreluActivationWeights;
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivationWeights) {
                this.variableNames.push('preluActivationWeights');
            }
            this.shaderKey = `conv2dnaive_${this.activation}_${this.isChannelsLast}`;
        }
        getUserCode() {
            const userCode = `
       ${activationFnSnippet(this.activation, this.hasPreluActivationWeights, false, 4)}
       fn readInp(batch : i32, row : i32, col : i32, chan : i32) -> f32{
         let coords = vec4<i32>(batch, row, col, chan);
         if (coordsInBounds4D(coords, uniforms.xShape)) {
           return  getX(batch, row, col, chan);
         } else {
          return 0.0;
         }
       }
       fn readFilt(row : i32, col : i32, xChannel : i32, outChannel : i32) -> f32{
         let coords = vec4<i32>(row, col, xChannel, outChannel);
         if(coordsInBounds4D(coords, uniforms.wShape)) {
           return getW(row, col, xChannel, outChannel);
          } else {
            return 0.0;
          }
       }
       fn writeResult(batch : i32, row : i32, col : i32, chan : i32, valueIn : f32) {
         let coords = ${this.isChannelsLast ? `vec4<i32>(batch, row, col, chan);` :
            `vec4<i32>(batch, chan, row, col);`}
         if (coordsInBounds4D(coords, uniforms.outShape)) {
           var value = valueIn;
           ${biasActivationSnippet(this.addBias, this.activation)}
           setOutputAtCoords(coords.x, coords.y, coords.z, coords.w, value);
         }
       }
       ${getMainHeaderString('index')} {
         let coords = getOutputCoords();
         let batch = coords[0];
         let outChannel = ${this.isChannelsLast ? `coords[3];` : `coords[1];`}
         let outRow = ${this.isChannelsLast ? `coords[1];` : `coords[2];`}
         let outCol = ${this.isChannelsLast ? `coords[2];` : `coords[3];`}
         var acc : f32 = 0.0;
         for (var row = 0; row < uniforms.filterDims[0]; row = row + 1) {
           for (var col = 0; col < uniforms.filterDims[1]; col = col + 1) {
             let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * row - uniforms.pad[0];
             let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * col - uniforms.pad[1];
             for (var xChannel = 0; xChannel < ${this.isChannelsLast ? `uniforms.xShape[3];` :
            `uniforms.xShape[1];`} xChannel = xChannel + 1) {
               ${this.isChannelsLast ? `let v = readInp(batch, xRow, xCol, xChannel);` :
            `let v = readInp(batch, xChannel, xRow, xCol);`}
               let f = readFilt(row, col, xChannel, outChannel);
               acc = acc + v * f;
             }
           }
         }
         writeResult(batch, outRow, outCol, outChannel, acc);
       }
     `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // conv2dByMatMul fuses height and width into one dimension to compute
    // batchMatMul, so bias and activation weights are also supposed to fuse the two
    // dimensions into one.
    //
    // This function computes the target shape for fusing height and width
    // dimensions. Returning null means the shape is already compatible.
    function getShapeForBatchMatMul(shape, isChannelsLast) {
        const length = shape.length;
        if (length >= 3) {
            return isChannelsLast ?
                [
                    ...shape.slice(0, -3) /* batch */,
                    shape[length - 3] * shape[length - 2] /* height * width */,
                    shape[length - 1] /* channel */
                ] :
                [
                    ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,
                    shape[length - 2] * shape[length - 1] /* height * width */
                ];
        }
        else if (!isChannelsLast && length === 1 && shape[0] > 1) {
            return [shape[0], 1];
        }
        else {
            return null;
        }
    }
    // For 1x1 kernels that iterate through every point in the input, convolution
    // can be expressed as matrix multiplication (without need for memory
    // remapping).
    function conv2dByMatMul({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
        const isChannelsLast = convInfo.dataFormat === 'channelsLast';
        const transposeA = isChannelsLast ? false : true;
        const transposeB = false;
        const sameSize = isChannelsLast &&
            convInfo.filterHeight === convInfo.inHeight &&
            convInfo.filterWidth === convInfo.inWidth &&
            convInfo.padInfo.type === 'VALID';
        const intermediates = [];
        let xReshaped;
        let filterReshaped;
        if (sameSize) {
            const sharedDim = convInfo.inHeight * convInfo.inWidth * convInfo.inChannels;
            xReshaped = reshape({
                inputs: { x },
                backend,
                attrs: { shape: [1, convInfo.batchSize, sharedDim] }
            });
            filterReshaped = reshape({
                inputs: { x: filter },
                backend,
                attrs: { shape: [1, sharedDim, convInfo.outChannels] }
            });
        }
        else {
            xReshaped = reshape({
                inputs: { x },
                backend,
                attrs: {
                    shape: isChannelsLast ?
                        [
                            convInfo.batchSize, convInfo.inHeight * convInfo.inWidth,
                            convInfo.inChannels
                        ] :
                        [
                            convInfo.batchSize, convInfo.inChannels,
                            convInfo.inHeight * convInfo.inWidth
                        ]
                }
            });
            filterReshaped = reshape({
                inputs: { x: filter },
                backend,
                attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }
            });
        }
        intermediates.push(xReshaped);
        intermediates.push(filterReshaped);
        if (preluActivationWeights != null) {
            const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);
            if (targetShape != null) {
                preluActivationWeights = reshape({
                    inputs: { x: preluActivationWeights },
                    backend,
                    attrs: { shape: targetShape }
                });
                intermediates.push(preluActivationWeights);
            }
        }
        if (bias != null) {
            const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);
            if (targetShape != null) {
                bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: targetShape } });
                intermediates.push(bias);
            }
        }
        const result = batchMatMulImpl({
            a: isChannelsLast ? xReshaped : filterReshaped,
            b: isChannelsLast ? filterReshaped : xReshaped,
            transposeA,
            transposeB,
            backend,
            bias,
            activation,
            preluActivationWeights,
            leakyreluAlpha
        });
        const out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });
        intermediates.push(result);
        for (const i of intermediates) {
            backend.disposeData(i.dataId);
        }
        return out;
    }
    function conv2DImpl({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
        const hasBias = bias != null;
        const hasPreluActivationWeights = preluActivationWeights != null;
        const isChannelsLast = convInfo.dataFormat === 'channelsLast';
        const sameSize = isChannelsLast &&
            convInfo.filterHeight === convInfo.inHeight &&
            convInfo.filterWidth === convInfo.inWidth &&
            convInfo.padInfo.type === 'VALID';
        const useNaiveConv2d = tf.env().getBool('WEBGPU_USE_NAIVE_CONV2D_DEBUG');
        if (!useNaiveConv2d &&
            (sameSize ||
                (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 &&
                    convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&
                    convInfo.strideHeight === 1 && convInfo.strideWidth === 1 &&
                    (convInfo.padInfo.type === 'SAME' ||
                        convInfo.padInfo.type === 'VALID')))) {
            return conv2dByMatMul({
                x,
                filter,
                convInfo,
                backend,
                bias,
                activation,
                preluActivationWeights,
                leakyreluAlpha
            });
        }
        let program;
        const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];
        const dimensions = [
            { type: 'int32', data: [convInfo.filterHeight, convInfo.filterWidth] },
            { type: 'int32', data: [...padInfo] },
            { type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth] },
            { type: 'int32', data: [convInfo.dilationHeight, convInfo.dilationWidth] }
        ];
        if (useNaiveConv2d) {
            program = new Conv2DNaiveProgram(convInfo, hasBias, activation, hasPreluActivationWeights);
        }
        else {
            const dimAOuter = isChannelsLast ? convInfo.outHeight * convInfo.outWidth :
                convInfo.outChannels;
            const dimBOuter = isChannelsLast ? convInfo.outChannels :
                convInfo.outHeight * convInfo.outWidth;
            const dimInner = convInfo.filterHeight * convInfo.filterWidth * convInfo.inChannels;
            dimensions.push({ type: 'int32', data: [dimAOuter] }, { type: 'int32', data: [dimBOuter] }, { type: 'int32', data: [dimInner] });
            // Experiments show that sequential access is more friendly for Intel GPUs.
            const sequentialAccessByThreads = backend.adapterInfo.isIntel();
            program = new Conv2DMMProgram(convInfo, dimAOuter, dimBOuter, dimInner, hasBias, activation, hasPreluActivationWeights, sequentialAccessByThreads);
        }
        const intermediates = [];
        const inputVar = [x, filter];
        if (hasBias) {
            if (!isChannelsLast && bias.shape.length === 1) {
                bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: [bias.shape[0], 1, 1] } });
                intermediates.push(bias);
            }
            inputVar.push(bias);
        }
        if (hasPreluActivationWeights) {
            if (!isChannelsLast && preluActivationWeights.shape.length === 1) {
                preluActivationWeights = reshape({
                    inputs: { x: preluActivationWeights },
                    backend,
                    attrs: { shape: [preluActivationWeights.shape[0], 1, 1] }
                });
                intermediates.push(preluActivationWeights);
            }
            inputVar.push(preluActivationWeights);
        }
        if (activation === 'leakyrelu') {
            dimensions.push({ type: 'float32', data: [leakyreluAlpha] });
            program.uniforms += ' alpha : f32,';
        }
        const out = backend.runWebGPUProgram(program, inputVar, x.dtype, dimensions);
        for (const i of intermediates) {
            backend.disposeData(i.dataId);
        }
        return out;
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function conv2d(args) {
        const { inputs, attrs, backend } = args;
        const { x, filter } = inputs;
        const { strides, pad, dataFormat, dilations, dimRoundingMode } = attrs;
        const $dataFormat = tf.backend_util.convertConv2DDataFormat(dataFormat);
        const convInfo = tf.backend_util.computeConv2DInfo(x.shape, filter.shape, strides, dilations, pad, dimRoundingMode, false /* depthwise */, $dataFormat);
        return conv2DImpl({ x, filter, convInfo, backend });
    }
    const conv2DConfig = {
        kernelName: tf.Conv2D,
        backendName: 'webgpu',
        kernelFunc: conv2d
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function conv2dTransposeCommonSnippet(innerElementSize = 4) {
        const getWSnippet = (innerElementSize) => {
            switch (innerElementSize) {
                case 1:
                    return 'return W[getIndexFromCoords4D(coord, uniforms.wShape)];';
                case 4:
                    return `
            let coord1 = vec4<i32>(coordX, coordY, col + 1, rowInner);
            let coord2 = vec4<i32>(coordX, coordY, col + 2, rowInner);
            let coord3 = vec4<i32>(coordX, coordY, col + 3, rowInner);
            let v0 = W[getIndexFromCoords4D(coord, uniforms.wShape)];
            let v1 = W[getIndexFromCoords4D(coord1, uniforms.wShape)];
            let v2 = W[getIndexFromCoords4D(coord2, uniforms.wShape)];
            let v3 = W[getIndexFromCoords4D(coord3, uniforms.wShape)];
            return vec4<f32>(v0, v1, v2, v3);
            `;
                default:
                    throw new Error(`innerElementSize ${innerElementSize} is not supported.`);
            }
        };
        const readASnippet = `
      let outRow = row / uniforms.outShape[2];
      let outCol = row % uniforms.outShape[2];

      let WRow = col / (uniforms.filterDims[1] * uniforms.outBackprop[3]);
      let WCol = col / uniforms.outBackprop[3] % uniforms.filterDims[1];
      let xR = f32(outRow - uniforms.pads[0] + WRow) / f32(uniforms.stride[0]);
      let xC = f32(outCol - uniforms.pads[1] + WCol) / f32(uniforms.stride[1]);
      if (xR < 0.0 || xR >= f32(uniforms.outBackprop[1]) || fract(xR) > 0.0) {
        return ${typeSnippet(innerElementSize)}(0.0);
      }
      if (xC < 0.0 || xC >= f32(uniforms.outBackprop[2]) || fract(xC) > 0.0) {
        return ${typeSnippet(innerElementSize)}(0.0);
      }
      let coord = vec4<i32>(
          batch,
          i32(xR),
          i32(xC),
          col % uniforms.outBackprop[3]);
      return x[getIndexFromCoords4D(coord, uniforms.xShape)/${innerElementSize}];`;
        const sampleA = `if (row < uniforms.dimAOuter && col < uniforms.dimInner) {
        ${readASnippet}
      }
      return ${typeSnippet(innerElementSize)}(0.0);`;
        const userCode = `
  fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${typeSnippet(innerElementSize)} {
    let col = colIn * ${innerElementSize};
    ${sampleA}
  }

  fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${typeSnippet(innerElementSize)} {
    let col = colIn * ${innerElementSize};
    let coordX = uniforms.filterDims.x - 1 -
        row / (uniforms.filterDims[1] * uniforms.outBackprop[3]);
    let coordY = uniforms.filterDims.y - 1 -
        (row / uniforms.outBackprop[3]) % uniforms.filterDims[1];
    if (row < uniforms.dimInner && col < uniforms.dimBOuter &&
        coordX >= 0 && coordY >= 0) {
      let rowInner = row % uniforms.outBackprop[3];
      let coord = vec4<i32>(coordX, coordY, col, rowInner);
      ${getWSnippet(innerElementSize)}
    }
    return ${typeSnippet(innerElementSize)}(0.0);
  }

  fn mm_write(batch: i32, row : i32, colIn : i32, valueInput : ${typeSnippet(innerElementSize)}) {
    let col = colIn * ${innerElementSize};
    if (row < uniforms.dimAOuter && (col + ${innerElementSize - 1}) < uniforms.dimBOuter) {
      var value = valueInput;
      let outCoord = vec4<i32>(
          batch,
          row / uniforms.outShape[2],
          row % uniforms.outShape[2],
          col);
      result[getIndexFromCoords4D(outCoord, uniforms.outShape)/${innerElementSize}] = value;
    }
  }`;
        return userCode;
    }
    class Conv2DDerInputMMProgram {
        constructor(convInfo) {
            this.variableNames = ['x', 'W'];
            this.uniforms = 'filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,';
            this.outputShape = convInfo.inShape;
            tf.util.assert(convInfo.dataFormat === 'channelsLast', () => 'TODO: NCHW is unimplemented');
            this.isVec4 =
                convInfo.inChannels % 4 === 0 && convInfo.outChannels % 4 === 0;
            this.dispatchLayout = { x: [3], y: [1, 2], z: [0] };
            this.workGroupSize = computeWorkGroupSizeForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
            this.elementsPerThread = computeWorkPerThreadForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, this.elementsPerThread);
            if (this.isVec4) {
                this.variableTypes = ['vec4<f32>', 'f32'];
            }
            this.shaderKey =
                `conv2DDerInputMM_${this.isVec4}_${this.elementsPerThread}`;
        }
        getUserCode() {
            const matMulSource = this.isVec4 ?
                makeMatMulPackedVec4Source(this.elementsPerThread, this.workGroupSize) :
                makeMatMulPackedSource(this.elementsPerThread, this.workGroupSize);
            const userCode = `
    ${conv2dTransposeCommonSnippet(this.isVec4 ? 4 : 1)}
    ${matMulSource}
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class Conv2DDerInputProgram {
        constructor(convInfo) {
            this.variableNames = ['dy', 'W'];
            this.uniforms = 'filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = convInfo.inShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.isChannelsLast = convInfo.dataFormat === 'channelsLast';
            this.shaderKey = `conv2DDerInput_${this.isChannelsLast}`;
        }
        getUserCode() {
            const rowDim = this.isChannelsLast ? 1 : 2;
            const colDim = this.isChannelsLast ? 2 : 3;
            const channelDim = this.isChannelsLast ? 3 : 1;
            return `
    ${getMainHeaderString('index')} {
      if(index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let batch = coords[0];
        let d1 = coords[${channelDim}];

        let dyCorner = vec2<i32>(coords[${rowDim}], coords[${colDim}]) - uniforms.pads;
        let dyRCorner = dyCorner.x;
        let dyCCorner = dyCorner.y;

        // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).
        // ? = to be determined. : = across all values in that axis.
        var dotProd = 0.0;
        for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + 1) {
          let dyR = (f32(dyRCorner) + f32(wR)) / f32(uniforms.stride.x);
          let wRPerm = uniforms.filterDims.x - 1 - wR;
          if (dyR < 0.0 || dyR >= f32(uniforms.outBackprop[1]) || fract(dyR) > 0.0 ||
              wRPerm < 0) {
            continue;
          }
          let idyR = i32(dyR);

          for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + 1) {
            let dyC = (f32(dyCCorner) + f32(wC)) / f32(uniforms.stride.y);
            let wCPerm = uniforms.filterDims.y - 1 - wC;
            if (dyC < 0.0 || dyC >= f32(uniforms.outBackprop[2]) ||
                fract(dyC) > 0.0 || wCPerm < 0) {
              continue;
            }
            let idyC = i32(dyC);

            for (var d2 = 0; d2 < uniforms.outBackprop[3]; d2 = d2 + 1) {
              if (${this.isChannelsLast}) {
                let xValue = getDy(batch, idyR, idyC, d2);
                let wValue = getW(wRPerm, wCPerm, d1, d2);
                dotProd = dotProd + xValue * wValue;
              } else {
                let xValue = getDy(batch, d2, idyR, idyC);
                let wValue = getW(wRPerm, wCPerm, d1, d2);
                dotProd = dotProd + xValue * wValue;
              }

            }
          }
        }
        setOutputAtIndex(index, dotProd);
      }
    }
  `;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function conv2DBackpropInput(args) {
        const { inputs, backend, attrs } = args;
        const { dy, filter } = inputs;
        const { inputShape, strides, pad, dataFormat, dimRoundingMode } = attrs;
        const $dataFormat = tf.backend_util.convertConv2DDataFormat(dataFormat);
        const convInfo = tf.backend_util.computeConv2DInfo(inputShape, filter.shape, strides, 1 /* dilations */, pad, dimRoundingMode, false, $dataFormat);
        const dimensions = [
            { type: 'int32', data: [convInfo.filterHeight, convInfo.filterWidth] },
            {
                type: 'int32',
                data: [
                    convInfo.filterHeight - 1 - convInfo.padInfo.top,
                    convInfo.filterWidth - 1 - convInfo.padInfo.left
                ]
            },
            { type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth] },
            {
                type: 'int32',
                data: [
                    convInfo.batchSize, convInfo.outHeight, convInfo.outWidth,
                    convInfo.outChannels
                ]
            },
        ];
        let program;
        // When filter size is small, Conv2DDerInputProgram is much faster than
        // Conv2DDerInputMMProgram.
        if (tf.env().getBool('WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE') ||
            convInfo.filterHeight <= 2 && convInfo.filterWidth <= 2 &&
                convInfo.outChannels <= 16 && convInfo.inChannels === 1) {
            program = new Conv2DDerInputProgram(convInfo);
        }
        else {
            program = new Conv2DDerInputMMProgram(convInfo);
            const dimAOuter = convInfo.inHeight * convInfo.inWidth;
            const dimBOuter = convInfo.inChannels;
            const dimInner = convInfo.filterHeight * convInfo.filterWidth * convInfo.outChannels;
            dimensions.push({ type: 'uint32', data: [dimAOuter] }, { type: 'uint32', data: [dimBOuter] }, { type: 'uint32', data: [dimInner] });
        }
        return backend.runWebGPUProgram(program, [dy, filter], 'float32', dimensions);
    }
    const conv2DBackpropInputConfig = {
        kernelName: tf.Conv2DBackpropInput,
        backendName: 'webgpu',
        kernelFunc: conv2DBackpropInput,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const cos = unaryKernelFunc({ opType: UnaryOpType.COS });
    const cosConfig = {
        kernelName: tf.Cos,
        backendName: 'webgpu',
        kernelFunc: cos
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const cosh = unaryKernelFunc({ opType: UnaryOpType.COSH });
    const coshConfig = {
        kernelName: tf.Cosh,
        backendName: 'webgpu',
        kernelFunc: cosh
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class CropAndResizeProgram {
        constructor(channnel, boxShape, cropSize, method) {
            this.variableNames = ['Image', 'Boxes', 'BoxInd'];
            this.uniforms = 'extrapolationValue : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            const [numBoxes,] = boxShape;
            this.outputShape = [numBoxes, cropSize[0], cropSize[1], channnel];
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.methodId = method === 'bilinear' ? 1 : 0;
            this.cropHeightBiggerThan1 = this.outputShape[1] > 1;
            this.cropWidthBiggerThan1 = this.outputShape[2] > 1;
            this.shaderKey = `cropAndResize_${this.methodId}_${this.cropHeightBiggerThan1}_${this.cropWidthBiggerThan1}`;
        }
        getUserCode() {
            const [inputHeightFloat, inputWidthFloat] = [`f32(uniforms.imageShape[1] - 1)`, `f32(uniforms.imageShape[2] - 1)`];
            const [heightRatio, heightScale, inY] = this.cropHeightBiggerThan1 ?
                [
                    `(${inputHeightFloat} / f32(uniforms.outShape[1] - 1))`,
                    '(y2-y1) * height_ratio',
                    `y1*${inputHeightFloat} + f32(y)*(height_scale)`,
                ] :
                [
                    '0.0',
                    '0.0',
                    `0.5 * (y1+y2) * ${inputHeightFloat}`,
                ];
            const [widthRatio, widthScale, inX] = this.cropWidthBiggerThan1 ?
                [
                    `(${inputWidthFloat} / f32(uniforms.outShape[2] - 1))`,
                    '(x2-x1) * width_ratio',
                    `x1*${inputWidthFloat} + f32(x)*(width_scale)`,
                ] :
                [
                    '0.0',
                    '0.0',
                    `0.5 * (x1+x2) * ${inputWidthFloat}`,
                ];
            // Reference implementation
            // tslint:disable-next-line:max-line-length
            // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc
            const userCode = `
    ${getMainHeaderString('index')} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let height_ratio = f32(${heightRatio});
        let width_ratio = f32(${widthRatio});
        let b = coords[0];
        let y = coords[1];
        let x = coords[2];
        let d = coords[3];
        // get box vals
        let y1 = getBoxes(b, 0);
        let x1 = getBoxes(b, 1);
        let y2 = getBoxes(b, 2);
        let x2 = getBoxes(b, 3);
        // get image in batch index
        let bInd = i32(round(getBoxInd(b)));
        if(bInd < 0 || bInd >= uniforms.outShape[0]) {
          return;
        }
        let height_scale = ${heightScale};
        let width_scale = ${widthScale};
        let in_y = ${inY};
        if( in_y < 0.0 || in_y > ${inputHeightFloat} ) {
          setOutputAtIndex(index, uniforms.extrapolationValue);
          return;
        }
        let in_x = ${inX};
        if( in_x < 0.0 || in_x > ${inputWidthFloat} ) {
          setOutputAtIndex(index, uniforms.extrapolationValue);
          return;
        }
        let sourceFracIndexCR = vec2<f32>(in_x,in_y);
        if(${this.methodId} == 1) {
          // Compute the four integer indices.
          let sourceFloorCR = vec2<i32>(sourceFracIndexCR);
          let sourceCeilCR = vec2<i32>(ceil(sourceFracIndexCR));
          let topLeft = getImage(bInd, sourceFloorCR.y, sourceFloorCR.x, d);
          let bottomLeft = getImage(bInd, sourceCeilCR.y, sourceFloorCR.x, d);
          let topRight = getImage(bInd, sourceFloorCR.y, sourceCeilCR.x, d);
          let bottomRight = getImage(bInd, sourceCeilCR.y, sourceCeilCR.x, d);
          let fracCR = sourceFracIndexCR - vec2<f32>(sourceFloorCR);
          let top = topLeft + (topRight - topLeft) * fracCR.x;
          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracCR.x;
          let newValue = top + (bottom - top) * fracCR.y;
          setOutputAtIndex(index, newValue);
        } else {
          // Compute the coordinators of nearest neighbor point.
          let sourceNearestCR = vec2<i32>(floor(
            sourceFracIndexCR + vec2<f32>(0.5,0.5)));
          let newValue = getImage(
            bInd, sourceNearestCR.y, sourceNearestCR.x, d);
          setOutputAtIndex(index, newValue);
        }
      }
    }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const cropAndResize = (args) => {
        const { inputs, backend, attrs } = args;
        const { image, boxes, boxInd } = inputs;
        const { cropSize, method, extrapolationValue } = attrs;
        const program = new CropAndResizeProgram(image.shape[3], boxes.shape, cropSize, method);
        const uniformData = [{ type: 'float32', data: [extrapolationValue] }];
        return backend.runWebGPUProgram(program, [image, boxes, boxInd], 'float32', uniformData);
    };
    const cropAndResizeConfig = {
        kernelName: tf.CropAndResize,
        backendName: 'webgpu',
        kernelFunc: cropAndResize
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    var CumOpType;
    (function (CumOpType) {
        CumOpType["Prod"] = "*";
        CumOpType["Sum"] = "+";
    })(CumOpType || (CumOpType = {}));
    class CumProgram {
        constructor(op, shape, exclusive, reverse) {
            this.variableNames = ['x'];
            // pow(i32, i32) is not supported, use pow(f32, f32) instead.
            this.uniforms = 'index : f32,';
            this.size = true;
            const workGroupSizeX = 128;
            this.workGroupSize = [workGroupSizeX, 1, 1];
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.exclusive = exclusive;
            this.reverse = reverse;
            this.op = op;
            this.shaderKey = `cum_${this.op}_${this.exclusive}_${this.reverse}`;
        }
        getUserCode() {
            const rank = this.outputShape.length;
            const initVal = this.op === CumOpType.Prod ? '1.0' : '0.0';
            const val = this.exclusive ? initVal :
                `getX(${getCoords(rank, 'coords', this.op)})`;
            const length = this.outputShape[this.outputShape.length - 1];
            let condition = '';
            let idxString = '';
            // When exclusive is set, the cum op becomes roll op that copies the
            // value from the previous index based on the direction specified by the
            // reverse flag.
            if (this.exclusive) {
                condition = this.reverse ? `end != ${length - 1}` : 'end != 0';
                idxString = this.reverse ? 'end + 1' : 'end - 1';
            }
            else {
                condition = this.reverse ? `end + pow2 < ${length}` : 'end >= pow2';
                idxString = (this.reverse ? 'end + pow2' : 'end - pow2');
            }
            return `
      ${getMainHeaderString('index')} {
       if (index < uniforms.size) {
         var coords = getCoordsFromIndex(index);

         let end = ${getFinalCoord(rank, 'coords', this.op)};
         var val = ${val};
         let pow2 = i32(pow(2.0, uniforms.index));
         if (${condition}) {
           let idx = ${idxString};
           ${getFinalCoord(rank, 'coords', this.op)} = idx;
           val ${this.op}= getX(${getCoords(rank, 'coords', this.op)});
         }
         setOutputAtIndex(index, val);
       }
      }
    `;
        }
    }
    function getCoords(rank, name, op) {
        if (rank === 1) {
            return `${name}`;
        }
        else if (rank === 2) {
            return `${name}.x, ${name}.y`;
        }
        else if (rank === 3) {
            return `${name}.x, ${name}.y, ${name}.z`;
        }
        else if (rank === 4) {
            return `${name}.x, ${name}.y, ${name}.z, ${name}.w`;
        }
        else {
            throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);
        }
    }
    function getFinalCoord(rank, name, op) {
        if (rank === 1) {
            return `${name}`;
        }
        else if (rank === 2) {
            return `${name}.y`;
        }
        else if (rank === 3) {
            return `${name}.z`;
        }
        else if (rank === 4) {
            return `${name}.w`;
        }
        else {
            throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);
        }
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function cumImpl(op, x, backend, axis, exclusive, reverse) {
        const xRank = x.shape.length;
        const permutation = tf.backend_util.getAxesPermutation([axis], xRank);
        let permutedX = x;
        if (permutation != null) {
            permutedX = transpose({ inputs: { x }, backend, attrs: { perm: permutation } });
        }
        const permutedAxis = tf.backend_util.getInnerMostAxes(1, xRank)[0];
        if (permutedAxis !== xRank - 1) {
            throw new Error(`WebGPU cumprod shader expects an inner-most axis=${x.shape.length - 1} ` +
                `but got axis=${axis}`);
        }
        const size = permutedX.shape[permutedAxis];
        let result = identity({ inputs: { x: permutedX }, backend });
        // Use cum parallel algorithm, inspired by:
        // https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda
        // Note: although the algorithm is called sum, it works for any associtative
        // operator with an identity.
        for (let i = 0; i <= Math.ceil(Math.log2(size)) - 1; i++) {
            const program = new CumProgram(op, permutedX.shape, false, reverse);
            const prevResult = result;
            const uniformData = [{ type: 'float32', data: [i] }];
            result =
                backend.runWebGPUProgram(program, [result], result.dtype, uniformData);
            backend.disposeData(prevResult.dataId);
        }
        // For exclusive cum, shift the end result in the direction of product or sum
        // and add 1 for product or 0 for sum to the front index.
        if (exclusive) {
            const program = new CumProgram(op, permutedX.shape, exclusive, reverse);
            const prevResult = result;
            const uniformData = [{ type: 'float32', data: [0] }];
            result =
                backend.runWebGPUProgram(program, [result], result.dtype, uniformData);
            backend.disposeData(prevResult.dataId);
        }
        if (permutation != null) {
            const reversePermutation = tf.backend_util.getUndoAxesPermutation(permutation);
            const reverseTransposedResult = transpose({ inputs: { x: result }, backend, attrs: { perm: reversePermutation } });
            backend.disposeData(result.dataId);
            backend.disposeData(permutedX.dataId);
            return reverseTransposedResult;
        }
        return result;
    }

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function cumprod(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis, exclusive, reverse } = attrs;
        return cumImpl(CumOpType.Prod, x, backend, axis, exclusive, reverse);
    }
    const cumprodConfig = {
        kernelName: tf.Cumprod,
        backendName: 'webgpu',
        kernelFunc: cumprod
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function cumsum(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis, exclusive, reverse } = attrs;
        return cumImpl(CumOpType.Sum, x, backend, axis, exclusive, reverse);
    }
    const cumsumConfig = {
        kernelName: tf.Cumsum,
        backendName: 'webgpu',
        kernelFunc: cumsum
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class DepthToSpaceProgram {
        constructor(outputShape, dataFormat) {
            this.variableNames = ['x'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.uniforms = 'blockSize : i32,';
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `depthToSpace_${dataFormat}`;
            this.dataFormat = dataFormat;
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let h = ${this.getHeightCoordString()};
          let w = ${this.getWidthCoordString()};
          let d = ${this.getDepthCoordString()};

          let in_h = h / uniforms.blockSize;
          let offset_h = h % uniforms.blockSize;
          let in_w = w / uniforms.blockSize;
          let offset_w = w % uniforms.blockSize;
          let offset_d = (offset_h * uniforms.blockSize + offset_w) *
            ${this.getOutputDepthSize()};
          let in_d = d + offset_d;

          let rlt = ${this.getInputSamplingString()};
          setOutputAtIndex(index, rlt);
        }
      }`;
            return userCode;
        }
        getHeightCoordString() {
            if (this.dataFormat === 'NHWC') {
                return `coords[1]`;
            }
            else {
                return `coords[2]`;
            }
        }
        getWidthCoordString() {
            if (this.dataFormat === 'NHWC') {
                return `coords[2]`;
            }
            else {
                return `coords[3]`;
            }
        }
        getDepthCoordString() {
            if (this.dataFormat === 'NHWC') {
                return `coords[3]`;
            }
            else {
                return `coords[1]`;
            }
        }
        getOutputDepthSize() {
            if (this.dataFormat === 'NHWC') {
                return `uniforms.outShape[3]`;
            }
            else {
                return `uniforms.outShape[1]`;
            }
        }
        getInputSamplingString() {
            if (this.dataFormat === 'NHWC') {
                return `getX(b, in_h, in_w, in_d)`;
            }
            else {
                return `getX(b, in_d, in_h, in_w)`;
            }
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function depthToSpace(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { blockSize, dataFormat } = attrs;
        const batchSize = x.shape[0];
        const inputHeight = (dataFormat === 'NHWC') ? x.shape[1] : x.shape[2];
        const inputWidth = (dataFormat === 'NHWC') ? x.shape[2] : x.shape[3];
        const inputDepth = (dataFormat === 'NHWC') ? x.shape[3] : x.shape[1];
        const outputHeight = inputHeight * blockSize;
        const outputWidth = inputWidth * blockSize;
        const outputDepth = inputDepth / (blockSize * blockSize);
        const outputShape = (dataFormat === 'NHWC') ?
            [batchSize, outputHeight, outputWidth, outputDepth] :
            [batchSize, outputDepth, outputHeight, outputWidth];
        const uniformData = [
            { type: 'int32', data: [blockSize] },
        ];
        const program = new DepthToSpaceProgram(outputShape, dataFormat);
        return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
    }
    const depthToSpaceConfig = {
        kernelName: tf.DepthToSpace,
        backendName: 'webgpu',
        kernelFunc: depthToSpace
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class DepthwiseConv2DNCHWSharedProgram {
        constructor(outputShape, filterHeight, filterWidth, addBias = false, activation = null, hasPreluActivation = false) {
            this.variableNames = ['x', 'W'];
            this.uniforms = `pad : vec2<i32>, inDims : vec2<i32>,`;
            this.workGroupSize = [16, 16, 1];
            this.outputShape = outputShape;
            this.dispatchLayout = { x: [3], y: [2], z: [0, 1] };
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivation) {
                this.variableNames.push('preluActivationWeights');
            }
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivation = hasPreluActivation;
            this.filterHeight = filterHeight;
            this.filterWidth = filterWidth;
            this.shaderKey = `depthwiseNCHW_${this.activation}_${this.filterHeight}_${this.filterWidth}`;
        }
        getUserCode() {
            const filterSize = this.filterWidth * this.filterHeight;
            const workGroupSize = this.workGroupSize[0] * this.workGroupSize[1] * this.workGroupSize[2];
            const tileAHeight = this.workGroupSize[1] + this.filterHeight - 1;
            const tileAWidth = this.workGroupSize[0] + this.filterWidth - 1;
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}

      var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHeight}>;
      var<workgroup> mm_Bsub : array<array<f32, ${this.filterWidth}>, ${this.filterHeight}>;
      fn readX(batch : i32, channel : i32, row : i32, col : i32) -> f32 {
        var value = 0.0;
        if (row >=0 && row < uniforms.inDims[0] && col >=0 && col < uniforms.inDims[1])
        {
          value = getX(batch, channel, row, col);
        }
        return value;
      }

      ${getWorkGroupSizeString()}
      fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
                @builtin(global_invocation_id) GlobalId : vec3<u32>,
                @builtin(local_invocation_index) LocalIndex: u32,
                @builtin(num_workgroups) NumWorkgroups: vec3<u32>) {
        localId = LocalId;
        globalId = GlobalId;
        let localIndex = i32(LocalIndex);
        numWorkgroups = NumWorkgroups;
        let coords = getOutputCoords();
        let batch = coords[0];
        let xRCCorner = vec2<i32>(coords.zw) - uniforms.pad;
        let channelMul = uniforms.wShape[3];
        let d1 = coords[1] / channelMul;
        let q = coords[1] % channelMul;

        let inputRowStart = xRCCorner.x;
        let inputColStart = xRCCorner.y;

        let localRow = i32(localId.y);
        let localCol = i32(localId.x);

        // Load one tile of X into local memory.
        for (var inputRow = localRow; inputRow < ${tileAHeight}; inputRow = inputRow + ${this.workGroupSize[1]}) {
          for (var inputCol = localCol; inputCol < ${tileAWidth}; inputCol = inputCol + ${this.workGroupSize[0]}) {
            let rowOffset = inputRow - localRow;
            let colOffset = inputCol - localCol;
            mm_Asub[inputRow][inputCol] = readX(batch, d1, inputRowStart + rowOffset, inputColStart + colOffset);
          }
        }

        // Load one tile of W into local memory.
        var wIndex = localIndex;
        ${filterSize < workGroupSize ?
            `if (wIndex < ${filterSize})` :
            `for(; wIndex < ${filterSize}; wIndex = wIndex + ${workGroupSize})`}

        {
          let wRow = wIndex / ${this.filterWidth};
          let wCol = wIndex % ${this.filterWidth};
          mm_Bsub[wRow][wCol] = getW(wRow, wCol, d1, q);
        }

        workgroupBarrier();

        var value = 0.0;
        for (var wR = 0; wR < ${this.filterHeight}; wR = wR + 1) {
          for (var wC = 0; wC < ${this.filterWidth}; wC = wC + 1) {
            let xVal = mm_Asub[localRow + wR][localCol + wC];
            let wVal = mm_Bsub[wR][wC];
            value = fma(xVal, wVal, value);
          }
        }
        ${biasActivationSnippet(this.addBias, this.activation)}
        if (coordsInBounds4D(coords, uniforms.outShape)) {
          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class DepthwiseConv2DVec4Program {
        constructor(convInfo, addBias = false, activation = null, hasPreluActivation = false) {
            this.variableNames = ['x', 'W'];
            this.uniforms = 'pad : vec2<i32>, inDims : vec2<i32>,';
            this.workGroupSize = [4, 4, 4];
            this.workPerThread = 4;
            this.isVec4 = true;
            this.outputShape = convInfo.outShape;
            this.dispatchLayout = { x: [3], y: [2], z: [0, 1] };
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [4, this.workPerThread, 1]);
            tf.util.assert(convInfo.dataFormat === 'channelsLast', () => 'TODO: NCHW is unimplemented');
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivation) {
                this.variableNames.push('preluActivationWeights');
            }
            this.convInfo = convInfo;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivation = hasPreluActivation;
            this.shaderKey =
                `depthwiseVec4_${activation}_${this.convInfo.filterHeight}_${this.convInfo.filterWidth}_${this.convInfo.strideHeight}_${this.convInfo.strideWidth}_${this.workPerThread}`;
        }
        getUserCode() {
            const xNumber = (this.workPerThread - 1) * this.convInfo.strideWidth +
                this.convInfo.filterWidth;
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, true, 4)}
      fn readX(batch : i32, row : i32, col : i32, channel : i32) -> vec4<f32> {
        var value = vec4<f32>(0.0);
        if (col >=0 && col < uniforms.inDims[1]) {
          value = getX(batch, row, col, channel);
        }
        return value;
      }

      const strideHeight = ${this.convInfo.strideHeight};
      const strideWidth = ${this.convInfo.strideWidth};
      ${getWorkGroupSizeString()}
      fn _start(@builtin(global_invocation_id) globalId: vec3<u32>) {
        let batch = i32(globalId.z) / uniforms.outShape[1];
        let r = i32(globalId.z) % uniforms.outShape[1];
        let c = i32(globalId.y) * ${this.workPerThread};
        let d1 = i32(globalId.x) * 4;
        let xRCCorner = vec2<i32>(r, c) * vec2<i32>(strideHeight, strideWidth) - uniforms.pad;

        let xRCorner = xRCCorner.x;
        let xCCorner = xRCCorner.y;
        var xVals : array<vec4<f32>, ${xNumber}>;
        var dotProd : array<vec4<f32>, ${this.workPerThread}>;
        for (var i = 0; i < ${this.workPerThread}; i++) {
          dotProd[i] = vec4<f32>(0.0);
        }

        // Use constant instead of uniform can give better performance.
        for (var wR = 0; wR < ${this.convInfo.filterHeight}; wR = wR + 1) {
          let xR = xRCorner + wR;
          if (xR >=0 && xR < uniforms.inDims[0]) {
            for (var i = 0; i < ${xNumber}; i++) {
              xVals[i] = readX(batch, xR, xCCorner + i, d1);
            }
            for (var wC = 0; wC < ${this.convInfo.filterWidth}; wC = wC + 1) {
              let wValue = getW(wR, wC, d1, 0);
              for (var i = 0; i < ${this.workPerThread}; i++) {
                dotProd[i] = fma(xVals[i * strideWidth + wC], wValue, dotProd[i]);
              }
            }
          }
        }

        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let coords = vec4<i32>(batch, r, c + i, d1);
          if (coordsInBounds4D(coords, uniforms.outShape)) {
            var value = dotProd[i];
            ${biasActivationSnippet(this.addBias, this.activation)}
            setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
          }
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class DepthwiseConv2DProgram {
        constructor(convInfo, addBias = false, activation = null, hasPreluActivation = false) {
            this.variableNames = ['x', 'W'];
            this.uniforms = `pad : vec2<i32>, inDims : vec2<i32>, filterHeight : i32,
      filterWidth : i32, stride : vec2<i32>, dilation : vec2<i32>,`;
            // This is an experimental value.
            this.workGroupSize = [256, 1, 1];
            this.outputShape = convInfo.outShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.isChannelsLast = convInfo.dataFormat === 'channelsLast';
            if (addBias) {
                this.variableNames.push('bias');
            }
            if (hasPreluActivation) {
                this.variableNames.push('preluActivationWeights');
            }
            this.convInfo = convInfo;
            this.addBias = addBias;
            this.activation = activation;
            this.hasPreluActivation = hasPreluActivation;
            this.shaderKey = `depthwise_${this.activation}_${this.isChannelsLast}`;
        }
        getUserCode() {
            const getXSnippet = this.isChannelsLast ? 'getX(batch, xR, xC, d1);' :
                'getX(batch, d1, xR, xC);';
            const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}

      ${getMainHeaderString()} {
        let coords = getOutputCoords();
        let batch = coords[0];
        let xRCCorner = vec2<i32>(coords.${this.isChannelsLast ? 'yz' : 'zw'}) * uniforms.stride - uniforms.pad;
        let d2 = coords[${this.isChannelsLast ? 3 : 1}];
        let channelMul = uniforms.wShape[3];
        let d1 = d2 / channelMul;
        let q = d2 % channelMul;

        let inputRowStart = xRCCorner.x;
        let inputColStart = xRCCorner.y;
        let inputRowEnd = inputRowStart + uniforms.filterHeight *
            uniforms.dilation[0];
        let inputColEnd = inputColStart + uniforms.filterWidth *
            uniforms.dilation[1];

        // Convolve x(?, ?, d1)|x(d1, ?, ?) with w(:, :, d1, q) to get
        // y(yR, yC, d2)|y(d2, yR, yC). ? = to be determined. : = across all
        // values in that axis. x(?, ?, d1) and y(yR, yC, d2) is for NHWC.
        // x(d1, ?, ?) and y(d2, yR, yC) is for NCHW.
        var value = 0.0;

        // Extract if checking out of for loop for performance.
        if (inputRowStart >= 0 && inputColStart >= 0 &&
          inputRowEnd < uniforms.inDims[0] &&
              inputColEnd < uniforms.inDims[1]) {
            for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {
              let xR = inputRowStart + wR * uniforms.dilation[0];

              for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {
                let xC = inputColStart + wC * uniforms.dilation[1];

                let xVal = ${getXSnippet};
                let wVal = getW(wR, wC, d1, q);
                value = value + xVal * wVal;
              }
            }
          } else {
            for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {
              let xR = inputRowStart + wR * uniforms.dilation[0];

              if (xR < 0 || xR >= uniforms.inDims[0]) {
                continue;
              }

              for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {
                let xC = inputColStart + wC * uniforms.dilation[1];

                if (xC < 0 || xC >= uniforms.inDims[1]) {
                  continue;
                }

                let xVal = ${getXSnippet};
                let wVal = getW(wR, wC, d1, q);
                value = value + xVal * wVal;
              }
            }
          }
          ${biasActivationSnippet(this.addBias, this.activation)}
        if (coordsInBounds4D(coords, uniforms.outShape)) {
          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function depthwiseConv2dNative(args) {
        const { inputs, backend, attrs } = args;
        const { x, filter } = inputs;
        const { strides, pad, dataFormat, dilations, dimRoundingMode } = attrs;
        const $dataFormat = tf.backend_util.convertConv2DDataFormat(dataFormat);
        let $dilations = dilations;
        if ($dilations == null) {
            $dilations = [1, 1];
        }
        const convInfo = tf.backend_util.computeConv2DInfo(x.shape, filter.shape, strides, $dilations, pad, dimRoundingMode, true /* depthwise */, $dataFormat);
        const dimensions = [
            { type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left] },
            { type: 'int32', data: [convInfo.inHeight, convInfo.inWidth] },
        ];
        const isChannelsLast = convInfo.dataFormat === 'channelsLast';
        let program;
        if (!isChannelsLast && convInfo.inHeight > 16 && convInfo.inWidth > 16 &&
            convInfo.strideHeight === 1 && convInfo.strideWidth === 1 &&
            convInfo.dilationWidth === 1 && convInfo.dilationHeight === 1 &&
            convInfo.inChannels === convInfo.outChannels) {
            program = new DepthwiseConv2DNCHWSharedProgram(convInfo.outShape, convInfo.filterHeight, convInfo.filterWidth);
        }
        else if (isChannelsLast && convInfo.inHeight > 4 && convInfo.inWidth > 4 &&
            convInfo.strideWidth <= 2 &&
            convInfo.inChannels === convInfo.outChannels &&
            convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&
            convInfo.inChannels % 4 === 0) {
            program = new DepthwiseConv2DVec4Program(convInfo);
        }
        else {
            program = new DepthwiseConv2DProgram(convInfo);
            dimensions.push({ type: 'int32', data: [convInfo.filterHeight] }, { type: 'int32', data: [convInfo.filterWidth] }, { type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth] }, {
                type: 'int32',
                data: [convInfo.dilationHeight, convInfo.dilationWidth]
            });
        }
        return backend.runWebGPUProgram(program, [x, filter], x.dtype, dimensions);
    }
    const depthwiseConv2dNativeConfig = {
        kernelName: tf.DepthwiseConv2dNative,
        backendName: 'webgpu',
        kernelFunc: depthwiseConv2dNative,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const multiplyKernelFunc = binaryKernelFunc({
        opType: BinaryOpType.MUL,
        cpuKernelImpl: multiplyImplCPU,
        supportsComplex: true
    });
    const multiplyConfig = {
        kernelName: tf.Multiply,
        backendName: 'webgpu',
        kernelFunc: multiplyKernelFunc
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sum(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis, keepDims } = attrs;
        return reduce(x, axis, keepDims, 'sum', backend);
    }
    const sumConfig = {
        kernelName: tf.Sum,
        backendName: 'webgpu',
        kernelFunc: sum
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function einsum(args) {
        const { inputs, backend, attrs } = args;
        const { equation } = attrs;
        const tensors = inputs;
        const { allDims, summedDims, idDims } = tf.backend_util.decodeEinsumEquation(equation, tensors.length);
        tf.backend_util.checkEinsumDimSizes(allDims.length, idDims, tensors);
        const { path, steps } = tf.backend_util.getEinsumComputePath(summedDims, idDims);
        const nSteps = steps.length;
        let out = null;
        let numDimsRemaining = allDims.length;
        const tensorsToDispose = [];
        for (let i = 0; i < nSteps; ++i) {
            for (const idTerm of steps[i]) {
                const { permutationIndices: perm, expandDims: dimsToExpand } = tf.backend_util.getEinsumPermutation(numDimsRemaining, idDims[idTerm]);
                let x;
                if (tf.backend_util.isIdentityPermutation(perm)) {
                    x = tensors[idTerm];
                }
                else {
                    x = transpose({ inputs: { x: tensors[idTerm] }, backend, attrs: { perm } });
                    tensorsToDispose.push(x);
                }
                const targetShape = x.shape.slice();
                for (let k = 0; k < dimsToExpand.length; ++k) {
                    targetShape.splice(dimsToExpand[k], 0, 1);
                }
                if (!tf.util.arraysEqual(x.shape, targetShape)) {
                    x = reshape({ inputs: { x }, backend, attrs: { shape: targetShape } });
                    tensorsToDispose.push(x);
                }
                if (out === null) {
                    out = x;
                }
                else {
                    // tslint:disable-next-line: no-unnecessary-type-assertion
                    out =
                        multiplyKernelFunc({ inputs: { a: x, b: out }, backend });
                    tensorsToDispose.push(out);
                }
            }
            if (i < nSteps - 1) {
                if (path[i] >= 0) {
                    out = sum({
                        inputs: { x: out },
                        backend,
                        attrs: {
                            axis: path[i] - (allDims.length - numDimsRemaining),
                            keepDims: false
                        }
                    });
                    tensorsToDispose.push(out);
                }
                numDimsRemaining--;
            }
        }
        // Clean up intermediate tensors.
        for (const tensorInfo of tensorsToDispose) {
            if (tensorInfo === out) {
                continue;
            }
            backend.disposeData(tensorInfo.dataId);
        }
        return out;
    }
    const einsumConfig = {
        kernelName: tf.Einsum,
        backendName: 'webgpu',
        kernelFunc: einsum
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const elu = unaryKernelFunc({ opType: UnaryOpType.ELU });
    const eluConfig = {
        kernelName: tf.Elu,
        backendName: 'webgpu',
        kernelFunc: elu
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const equal = binaryKernelFunc({ opType: BinaryOpType.EQUAL, dtype: 'bool', cpuKernelImpl: equalImplCPU });
    const equalConfig = {
        kernelName: tf.Equal,
        backendName: 'webgpu',
        kernelFunc: equal
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const exp = unaryKernelFunc({
        opType: UnaryOpType.EXP,
        cpuKernelImpl: expImplCPU,
        dtype: 'float32',
    });
    const expConfig = {
        kernelName: tf.Exp,
        backendName: 'webgpu',
        kernelFunc: exp
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the License);
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an AS IS BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function expandDims(args) {
        const { inputs, attrs, backend } = args;
        const { dim } = attrs;
        const { input } = inputs;
        const inputRank = input.shape.length;
        const newShape = input.shape.slice();
        let $dim = dim;
        if (dim < 0) {
            // Negative value is counted from the tail of rank.
            tf.util.assert(-(inputRank + 1) <= dim, () => `Axis must be in the interval [${-(inputRank + 1)}, ${inputRank}]`);
            $dim = inputRank + dim + 1;
        }
        newShape.splice($dim, 0, 1);
        return reshape({ inputs: { x: input }, backend, attrs: { shape: newShape } });
    }
    const expandDimsConfig = {
        kernelName: tf.ExpandDims,
        backendName: 'webgpu',
        kernelFunc: expandDims,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const expm1 = unaryKernelFunc({ opType: UnaryOpType.EXPM1, cpuKernelImpl: expm1ImplCPU });
    const expm1Config = {
        kernelName: tf.Expm1,
        backendName: 'webgpu',
        kernelFunc: expm1
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class FlipLeftRightProgram {
        constructor(imageShape) {
            this.outputShape = [];
            this.variableNames = ['x'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = imageShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = 'flipLeftRight';
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let coordX = uniforms.xShape[2] - coords[2] - 1;
          let outputValue = getX(coords[0], coords[1], coordX, coords[3]);
          setOutputAtIndex(index, outputValue);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const flipLeftRightConfig = {
        kernelName: tf.FlipLeftRight,
        backendName: 'webgpu',
        kernelFunc: ({ inputs, backend }) => {
            const { image } = inputs;
            const webgpuBackend = backend;
            const program = new FlipLeftRightProgram(image.shape);
            const output = webgpuBackend.runWebGPUProgram(program, [image], image.dtype);
            return output;
        }
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const floor = unaryKernelFunc({ opType: UnaryOpType.FLOOR, cpuKernelImpl: floorImplCPU });
    const floorConfig = {
        kernelName: tf.Floor,
        backendName: 'webgpu',
        kernelFunc: floor
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const floorDiv = binaryKernelFunc({ opType: BinaryOpType.INT_DIV, dtype: 'int32' });
    const floorDivConfig = {
        kernelName: tf.FloorDiv,
        backendName: 'webgpu',
        kernelFunc: floorDiv
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class FromPixelsProgram {
        constructor(outputShape, numChannels, importVideo = false) {
            this.isFromPixels = true;
            this.outputShape = [0];
            this.variableNames = [];
            this.workGroupSize = [256, 1, 1]; // The empirical value.
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [numChannels, 1, 1]);
            this.importVideo = importVideo;
            this.shaderKey = `fromPixels_${this.importVideo}`;
        }
        getUserCode() {
            const textureLoad = this.importVideo ?
                'textureLoad(src, vec2<i32>(coords.yx));' :
                'textureLoad(src, vec2<i32>(coords.yx), 0)';
            const textureType = this.importVideo ? 'texture_external' : 'texture_2d<f32>';
            return `
      @binding(1) @group(0) var src: ${textureType};
      ${getMainHeaderString('index')} {
        let flatIndex = index * uniforms.numChannels;
        if (flatIndex < uniforms.size) {
          let coords = getCoordsFromIndex(flatIndex);
          let values = ${textureLoad};
          for (var i = 0; i < uniforms.numChannels; i = i + 1) {
            result[flatIndex + i] = i32(floor(255.0 * values[i]));
          }
        }
      }
  `;
        }
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use backend file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const fromPixelsConfig = {
        kernelName: tf.FromPixels,
        backendName: 'webgpu',
        kernelFunc: fromPixels,
    };
    let fromPixels2DContext;
    let willReadFrequently = tf.env().getBool('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU');
    function fromPixels(args) {
        const { inputs, backend, attrs } = args;
        let { pixels } = inputs;
        const { numChannels } = attrs;
        if (pixels == null) {
            throw new Error('pixels passed to tf.browser.fromPixels() can not be null');
        }
        const isVideo = typeof (HTMLVideoElement) !== 'undefined' &&
            pixels instanceof HTMLVideoElement;
        const isImage = typeof (HTMLImageElement) !== 'undefined' &&
            pixels instanceof HTMLImageElement;
        const isCanvas = (typeof (HTMLCanvasElement) !== 'undefined' &&
            pixels instanceof HTMLCanvasElement) ||
            (typeof (OffscreenCanvas) !== 'undefined' &&
                pixels instanceof OffscreenCanvas);
        const isImageBitmap = typeof (ImageBitmap) !== 'undefined' && pixels instanceof ImageBitmap;
        const [width, height] = isVideo ?
            [
                pixels.videoWidth,
                pixels.videoHeight
            ] :
            [pixels.width, pixels.height];
        const outputShape = [height, width, numChannels];
        // Disable importExternalTexture temporarily as it has problem in spec and
        // browser impl
        const importVideo = false  ;
        const isVideoOrImage = isVideo || isImage;
        if (isImageBitmap || isCanvas || isVideoOrImage) {
            let textureInfo;
            {
                if (isVideoOrImage) {
                    const newWillReadFrequently = tf.env().getBool('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU');
                    if (fromPixels2DContext == null ||
                        newWillReadFrequently !== willReadFrequently) {
                        willReadFrequently = newWillReadFrequently;
                        fromPixels2DContext =
                            document.createElement('canvas').getContext('2d', { willReadFrequently });
                    }
                    fromPixels2DContext.canvas.width = width;
                    fromPixels2DContext.canvas.height = height;
                    fromPixels2DContext.drawImage(pixels, 0, 0, width, height);
                    pixels = fromPixels2DContext.canvas;
                }
                const usage = GPUTextureUsage.COPY_DST |
                    GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING;
                const format = 'rgba8unorm';
                const texture = backend.textureManager.acquireTexture(outputShape[1], outputShape[0], format, usage);
                backend.queue.copyExternalImageToTexture({ source: pixels }, { texture }, [outputShape[1], outputShape[0]]);
                textureInfo = { width, height, format, usage, texture };
            }
            const size = tf.util.sizeFromShape(outputShape);
            const strides = tf.util.computeStrides(outputShape);
            const program = new FromPixelsProgram(outputShape, numChannels, importVideo);
            const uniformData = [
                { type: 'uint32', data: [size] }, { type: 'uint32', data: [numChannels] },
                { type: 'uint32', data: [...strides] }
            ];
            const input = backend.makeTensorInfo([height, width], 'int32');
            const info = backend.tensorMap.get(input.dataId);
            info.resourceInfo = textureInfo;
            const result = backend.runWebGPUProgram(program, [input], 'int32', uniformData);
            backend.disposeData(input.dataId);
            return result;
        }
        // TODO: Encoding should happen on GPU once we no longer have to download
        // image data to the CPU.
        const imageData = pixels.data;
        let pixelArray = imageData;
        if (numChannels != null && numChannels !== 4) {
            pixelArray = new Uint8Array(pixels.width * pixels.height * numChannels);
            const dataLength = imageData.length;
            let j = 0;
            for (let i = 0; i < dataLength; i++) {
                if (i % 4 < numChannels) {
                    pixelArray[j++] = imageData[i];
                }
            }
        }
        const output = backend.makeTensorInfo(outputShape, 'int32', new Int32Array(pixelArray));
        backend.uploadToGPU(output.dataId);
        return output;
    }

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class BatchNormProgram {
        constructor(xShape, meanShape, varianceShape, offsetShape, scaleShape) {
            this.uniforms = 'varianceEpsilon : f32,';
            // This is an experimental value.
            this.workGroupSize = [128, 1, 1];
            this.size = true;
            this.variableNames = ['x', 'mean', 'variance'];
            tf.backend_util.assertAndGetBroadcastShape(xShape, meanShape);
            tf.backend_util.assertAndGetBroadcastShape(xShape, varianceShape);
            this.outputShape = xShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            if (offsetShape != null) {
                tf.backend_util.assertAndGetBroadcastShape(xShape, offsetShape);
                this.variableNames.push('offset');
            }
            if (scaleShape != null) {
                tf.backend_util.assertAndGetBroadcastShape(xShape, scaleShape);
                this.variableNames.push('scale');
            }
            this.offsetShape = offsetShape;
            this.scaleShape = scaleShape;
            this.shaderKey = 'batchNorm';
        }
        getUserCode() {
            let offsetSnippet = '0.0';
            if (this.offsetShape != null) {
                offsetSnippet = 'getOffsetByOutputIndex(index)';
            }
            let scaleSnippet = '1.0';
            if (this.scaleShape != null) {
                scaleSnippet = 'getScaleByOutputIndex(index)';
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size)
        {
          let xValue = getXByOutputIndex(index);
          let meanValue = getMeanByOutputIndex(index);
          let varianValue = getVarianceByOutputIndex(index);
          let offsetValue = ${offsetSnippet};
          let scaleValue = ${scaleSnippet};
          let inv = scaleValue * inverseSqrt(varianValue + f32(uniforms.varianceEpsilon));
          setOutputAtIndex(index,dot(vec3<f32>(xValue, -meanValue, offsetValue), vec3<f32>(inv, inv, 1.0)));
        }
      }
  `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const fusedBatchNormConfig = {
        kernelName: tf.FusedBatchNorm,
        backendName: 'webgpu',
        kernelFunc: ({ inputs, attrs, backend }) => {
            const { x, scale, offset, mean, variance } = inputs;
            const { varianceEpsilon } = attrs;
            const webGPUBackend = backend;
            const batchNormInputs = [x, mean, variance];
            let offsetShape = null;
            if (offset != null) {
                offsetShape = offset.shape;
                batchNormInputs.push(offset);
            }
            let scaleShape = null;
            if (scale != null) {
                scaleShape = scale.shape;
                batchNormInputs.push(scale);
            }
            const program = new BatchNormProgram(x.shape, mean.shape, variance.shape, offsetShape, scaleShape);
            const uniformData = [{ type: 'float32', data: [varianceEpsilon] }];
            return webGPUBackend.runWebGPUProgram(program, batchNormInputs, x.dtype, uniformData);
        }
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function fusedConv2d(args) {
        const { inputs, backend, attrs } = args;
        const { x, filter, bias, preluActivationWeights } = inputs;
        const { strides, pad, dataFormat, dilations, dimRoundingMode, activation, leakyreluAlpha } = attrs;
        const $dataFormat = tf.backend_util.convertConv2DDataFormat(dataFormat);
        const convInfo = tf.backend_util.computeConv2DInfo(x.shape, filter.shape, strides, dilations, pad, dimRoundingMode, false /* depthwise */, $dataFormat);
        return conv2DImpl({
            x,
            filter,
            convInfo,
            backend,
            bias,
            preluActivationWeights,
            leakyreluAlpha,
            activation
        });
    }
    const fusedConv2DConfig = {
        kernelName: tf.FusedConv2D,
        backendName: 'webgpu',
        kernelFunc: fusedConv2d,
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function fusedDepthwiseConv2D(args) {
        const { inputs, backend, attrs } = args;
        const { x, filter, bias, preluActivationWeights } = inputs;
        const { strides, pad, dilations, dimRoundingMode, activation, leakyreluAlpha } = attrs;
        let $dilations = dilations;
        if ($dilations == null) {
            $dilations = [1, 1];
        }
        tf.util.assert(tf.backend_util.eitherStridesOrDilationsAreOne(strides, $dilations), () => 'Error in depthwiseConv2d: Either strides or dilations must be ' +
            `1. Got strides ${strides} and dilations '${$dilations}'`);
        const convInfo = tf.backend_util.computeConv2DInfo(x.shape, filter.shape, strides, $dilations, pad, dimRoundingMode, true /* depthwise */);
        const programInputs = [x, filter];
        const hasBias = bias != null;
        const hasPreluActivationWeights = preluActivationWeights != null;
        if (hasBias) {
            programInputs.push(bias);
        }
        if (hasPreluActivationWeights) {
            programInputs.push(preluActivationWeights);
        }
        const dimensions = [
            { type: 'int32', data: [convInfo.padInfo.top, convInfo.padInfo.left] },
            { type: 'int32', data: [convInfo.inHeight, convInfo.inWidth] },
        ];
        let program;
        if (convInfo.inHeight > 4 && convInfo.inWidth > 4 &&
            convInfo.strideWidth <= 2 &&
            convInfo.inChannels === convInfo.outChannels &&
            convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&
            convInfo.inChannels % 4 === 0) {
            program = new DepthwiseConv2DVec4Program(convInfo, hasBias, activation, hasPreluActivationWeights);
        }
        else {
            program = new DepthwiseConv2DProgram(convInfo, hasBias, activation, hasPreluActivationWeights);
            dimensions.push({ type: 'int32', data: [convInfo.filterHeight] }, { type: 'int32', data: [convInfo.filterWidth] }, { type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth] }, {
                type: 'int32',
                data: [convInfo.dilationHeight, convInfo.dilationWidth]
            });
        }
        if (activation === 'leakyrelu') {
            dimensions.push({ type: 'float32', data: [leakyreluAlpha] });
            program.uniforms += ' alpha : f32,';
        }
        const result = backend.runWebGPUProgram(program, programInputs, 'float32', dimensions);
        return result;
    }
    const fusedDepthwiseConv2DConfig = {
        kernelName: tf.FusedDepthwiseConv2D,
        backendName: 'webgpu',
        kernelFunc: fusedDepthwiseConv2D,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class GatherNDProgram {
        constructor(sliceDim, shape) {
            this.variableNames = ['A', 'indices'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `gathernd_${sliceDim}`;
            this.sliceDim = sliceDim;
            this.uniforms = `sliceDim : i32, strides : ${getCoordsDataType(sliceDim)},`;
        }
        getUserCode() {
            let strideString;
            if (this.sliceDim > 1) {
                strideString = 'uniforms.strides[j]';
            }
            else {
                strideString = 'uniforms.strides';
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          var flattenIndex = 0;
          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {
            let indexTemp = i32(round(getIndices(coords[0], j)));
            let strideNum = ${strideString};
            flattenIndex = flattenIndex + indexTemp * strideNum;
          }

          setOutputAtIndex(index, getA(flattenIndex, coords[1]));
        }
      }
      `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function gatherNd(args) {
        const { inputs, backend } = args;
        const { params, indices } = inputs;
        const indicesShape = indices.shape;
        const sliceRank = indicesShape[indicesShape.length - 1];
        const paramsSize = tf.util.sizeFromShape(params.shape);
        const [resultShape, numSlices, sliceSize, strides] = tf.backend_util.prepareAndValidate(params, indices);
        const flattenIndices = reshape({ inputs: { x: indices }, backend, attrs: { shape: [numSlices, sliceRank] } });
        const flattenX = reshape({
            inputs: { x: params },
            backend,
            attrs: { shape: [(tf.util.sizeFromShape(params.shape) / sliceSize), sliceSize] }
        });
        if (backend.shouldExecuteOnCPU([params, indices]) ||
            params.dtype === 'string') {
            const indicesData = backend.readSync(indices.dataId);
            const paramsBuf = backend.bufferSync(params);
            const outValue = gatherNdImplCPU(indicesData, paramsBuf, params.dtype, numSlices, sliceRank, sliceSize, strides, params.shape, paramsSize);
            return backend.makeTensorInfo(resultShape, params.dtype, outValue.values);
        }
        const program = new GatherNDProgram(sliceRank, [numSlices, sliceSize]);
        const uniformData = [{ type: 'int32', data: [sliceRank] }, { type: 'int32', data: strides }];
        const res = backend.runWebGPUProgram(program, [flattenX, flattenIndices], flattenX.dtype, uniformData);
        const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape: resultShape } });
        backend.disposeData(flattenIndices.dataId);
        backend.disposeData(flattenX.dataId);
        backend.disposeData(res.dataId);
        return reshaped;
    }
    const gatherNdConfig = {
        kernelName: tf.GatherNd,
        backendName: 'webgpu',
        kernelFunc: gatherNd
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class GatherProgram {
        constructor(aShape, outputShape) {
            this.variableNames = ['A', 'indices'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = aShape.slice();
            this.aShape = aShape;
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `gather`;
        }
        getUserCode() {
            const sourceCoords = getSourceCoords$1(this.aShape);
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          let indexZ = i32(getIndices(resRC.x, resRC.z));
          let inBounds = select(0.0, 1.0, indexZ >= 0 && indexZ < uniforms.aShape[2]);
          setOutputAtIndex(index, inBounds * getA(${sourceCoords}));
        }
      }
    `;
            return userCode;
        }
    }
    // The input and output are always flattened into rank 4 tensors.
    function getSourceCoords$1(aShape) {
        const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];
        const sourceCoords = [];
        for (let i = 0; i < aShape.length; i++) {
            if (i === 2) {
                sourceCoords.push('indexZ');
            }
            else {
                sourceCoords.push(`${currentCoords[i]}`);
            }
        }
        return sourceCoords.join();
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function gatherV2(args) {
        const { inputs, backend, attrs } = args;
        const { x, indices } = inputs;
        const { axis, batchDims } = attrs;
        // Unlike WebGL, WebGPU won't check if index is out of bound by calling
        // backend.readSync() function in debug mode.
        const parsedAxis = tf.util.parseAxisParam(axis, x.shape)[0];
        const shapeInfo = tf.backend_util.segment_util.collectGatherOpShapeInfo(x, indices, parsedAxis, batchDims);
        const indicesSize = tf.util.sizeFromShape(indices.shape);
        const toDispose = [];
        const flattenX = reshape({
            inputs: { x },
            backend,
            attrs: {
                shape: [
                    shapeInfo.batchSize, shapeInfo.outerSize, shapeInfo.dimSize,
                    shapeInfo.sliceSize
                ]
            }
        });
        const flattenIndex = reshape({
            inputs: { x: indices },
            backend,
            attrs: { shape: [shapeInfo.batchSize, indicesSize / shapeInfo.batchSize] }
        });
        toDispose.push(flattenX);
        toDispose.push(flattenIndex);
        const flattenOutputShape = [
            shapeInfo.batchSize, shapeInfo.outerSize, indicesSize / shapeInfo.batchSize,
            shapeInfo.sliceSize
        ];
        if (backend.shouldExecuteOnCPU([x, indices])) {
            const indicesBufferInfo = backend.tensorMap.get(flattenIndex.dataId);
            const indicesValues = indicesBufferInfo.values;
            const indicesBuf = tf.buffer(flattenIndex.shape, flattenIndex.dtype, indicesValues);
            const xBufferInfo = backend.tensorMap.get(flattenX.dataId);
            const xValues = xBufferInfo.values;
            const xBuf = tf.buffer(flattenX.shape, flattenX.dtype, xValues);
            const outBuf = gatherV2ImplCPU(xBuf, indicesBuf, flattenOutputShape);
            toDispose.forEach(t => backend.disposeData(t.dataId));
            return backend.makeTensorInfo(shapeInfo.outputShape, outBuf.dtype, outBuf.values);
        }
        const program = new GatherProgram(flattenX.shape, flattenOutputShape);
        const res = backend.runWebGPUProgram(program, [flattenX, flattenIndex], flattenX.dtype);
        toDispose.push(res);
        const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape: shapeInfo.outputShape } });
        toDispose.forEach(t => backend.disposeData(t.dataId));
        return reshaped;
    }
    const gatherV2Config = {
        kernelName: tf.GatherV2,
        backendName: 'webgpu',
        kernelFunc: gatherV2
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const greater = binaryKernelFunc({
        opType: BinaryOpType.GREATER,
        cpuKernelImpl: greaterImplCPU,
        dtype: 'bool',
    });
    const greaterConfig = {
        kernelName: tf.Greater,
        backendName: 'webgpu',
        kernelFunc: greater
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const greaterEqual = binaryKernelFunc({
        opType: BinaryOpType.GREATER_EQUAL,
        dtype: 'bool',
        cpuKernelImpl: greaterEqualImplCPU
    });
    const greaterEqualConfig = {
        kernelName: tf.GreaterEqual,
        backendName: 'webgpu',
        kernelFunc: greaterEqual
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const isNaN = unaryKernelFunc({ opType: UnaryOpType.IS_NAN, dtype: 'bool' });
    const isNaNConfig = {
        kernelName: tf.IsNan,
        backendName: 'webgpu',
        kernelFunc: isNaN
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function leakyRelu(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { alpha } = attrs;
        const uniformData = [{ type: 'float32', data: [alpha] }];
        const program = new UnaryOpProgram(x.shape, UnaryOpType.LEAKYRELU);
        program.uniforms = 'alpha : f32,';
        return backend.runWebGPUProgram(program, [x], 'float32', uniformData);
    }
    const leakyReluConfig = {
        kernelName: tf.LeakyRelu,
        backendName: 'webgpu',
        kernelFunc: leakyRelu
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const less = binaryKernelFunc({ opType: BinaryOpType.LESS, dtype: 'bool', cpuKernelImpl: lessImplCPU });
    const lessConfig = {
        kernelName: tf.Less,
        backendName: 'webgpu',
        kernelFunc: less
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const lessEqual = binaryKernelFunc({
        opType: BinaryOpType.LESS_EQUAL,
        dtype: 'bool',
        cpuKernelImpl: lessEqualImplCPU
    });
    const lessEqualConfig = {
        kernelName: tf.LessEqual,
        backendName: 'webgpu',
        kernelFunc: lessEqual
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const log = unaryKernelFunc({ opType: UnaryOpType.LOG, cpuKernelImpl: logImplCPU });
    const logConfig = {
        kernelName: tf.Log,
        backendName: 'webgpu',
        kernelFunc: log
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const logicalAnd = binaryKernelFunc({ opType: BinaryOpType.LOGICAL_AND, dtype: 'bool' });
    const logicalAndConfig = {
        kernelName: tf.LogicalAnd,
        backendName: 'webgpu',
        kernelFunc: logicalAnd
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const logicalNot = unaryKernelFunc({ opType: UnaryOpType.LOGICAL_NOT });
    const logicalNotConfig = {
        kernelName: tf.LogicalNot,
        backendName: 'webgpu',
        kernelFunc: logicalNot
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const maximum = binaryKernelFunc({
        opType: BinaryOpType.MAX,
        cpuKernelImpl: maximumImplCPU,
    });
    const maximumConfig = {
        kernelName: tf.Maximum,
        backendName: 'webgpu',
        kernelFunc: maximum
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function maxPool(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { filterSize, strides, pad, dimRoundingMode } = attrs;
        const dilations = 1;
        const convInfo = tf.backend_util.computePool2DInfo(x.shape, filterSize, strides, dilations, pad, dimRoundingMode);
        return poolImpl(x, convInfo, 'max', backend);
    }
    const maxPoolConfig = {
        kernelName: tf.MaxPool,
        backendName: 'webgpu',
        kernelFunc: maxPool
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function min(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis, keepDims } = attrs;
        return reduce(x, axis, keepDims, 'min', backend);
    }
    const minConfig = {
        kernelName: tf.Min,
        backendName: 'webgpu',
        kernelFunc: min
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const minimum = binaryKernelFunc({
        opType: BinaryOpType.MIN,
        cpuKernelImpl: minimumImplCPU,
    });
    const minimumConfig = {
        kernelName: tf.Minimum,
        backendName: 'webgpu',
        kernelFunc: minimum
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class MirrorPadProgram {
        constructor(xShape, paddings, mode) {
            this.uniforms = '';
            this.variableNames = ['x'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = paddings.map((p, i) => p[0] /* beforePad */ + xShape[i] + p[1] /* afterPad */);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.xShape = xShape;
            paddings.map((_, i) => {
                this.uniforms += ` pad${i} : vec2<i32>,`;
            });
            this.offset = mode === 'reflect' ? 0 : 1;
            this.shaderKey = `mirrorPad_${mode}`;
        }
        getUserCode() {
            const rank = this.xShape.length;
            // The length of paddings are same with the rank of the input tensor.
            const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(',');
            const end = this.xShape
                .map((_, i) => `uniforms.pad${i}[0] + uniforms.xShape${rank > 1 ? `[${i}]` : ''}`)
                .join(',');
            const shaderStart = rank === 1 ? 'start' : 'start[i]';
            const shaderEnd = rank === 1 ? 'end' : 'end[i]';
            const shaderOutC = rank === 1 ? 'outC' : 'outC[i]';
            const dtype = getCoordsDataType(rank);
            const unpackedCoords = rank > 1 ?
                ['coords[0]', 'coords[1]', 'coords[2]', 'coords[3]'].slice(0, rank) :
                'coords';
            return `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let start = ${dtype}(${start});
          let end = ${dtype}(${end});
          var outC = getCoordsFromIndex(index);
          for (var i = 0; i < ${rank}; i = i + 1) {
            if (${shaderOutC} < ${shaderStart}) {
              ${shaderOutC} = ${shaderStart} * 2 - ${shaderOutC} - ${this.offset};
            } else if(${shaderOutC} >= ${shaderEnd}) {
              ${shaderOutC} = (${shaderEnd} - 1) * 2 - ${shaderOutC} + ${this.offset};
            }
          }
          let coords = outC - start;
          setOutputAtIndex(index, getX(${unpackedCoords}));
        }
      }
    `;
        }
    }

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const mirrorPadConfig = {
        kernelName: tf.MirrorPad,
        backendName: 'webgpu',
        kernelFunc: ({ inputs, attrs, backend }) => {
            const { x } = inputs;
            const { paddings, mode } = attrs;
            const webGPUBackend = backend;
            const uniformData = paddings.map(p => {
                return { type: 'int32', data: [p[0], p[1]] };
            });
            const program = new MirrorPadProgram(x.shape, paddings, mode);
            const output = webGPUBackend.runWebGPUProgram(program, [x], x.dtype, uniformData);
            return output;
        }
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // This doesn't use unaryKernelFunc because negImplCPU is not of type
    // SimpleUnaryKernelImplCPU.
    function neg(args) {
        const { inputs, backend } = args;
        const { x } = inputs;
        if (backend.shouldExecuteOnCPU([x])) {
            const xData = backend.tensorMap.get(x.dataId);
            const [outValues, newShape] = negImplCPU(xData.values, x.shape, x.dtype);
            return backend.makeTensorInfo(newShape, x.dtype, outValues);
        }
        const program = new UnaryOpProgram(x.shape, UnaryOpType.NEG);
        return backend.runWebGPUProgram(program, [x], x.dtype);
    }
    const negConfig = {
        kernelName: tf.Neg,
        backendName: 'webgpu',
        kernelFunc: neg
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function nonMaxSuppressionV3(args) {
        console.warn('tf.nonMaxSuppression() in webgpu locks the UI thread. ' +
            'Call tf.nonMaxSuppressionAsync() instead');
        const { inputs, backend, attrs } = args;
        const { boxes, scores } = inputs;
        const { maxOutputSize, iouThreshold, scoreThreshold } = attrs;
        const boxesVals = backend.readSync(boxes.dataId);
        const scoresVals = backend.readSync(scores.dataId);
        const { selectedIndices } = tf.kernel_impls.nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);
        return backend.makeTensorInfo([selectedIndices.length], 'int32', new Int32Array(selectedIndices));
    }
    const nonMaxSuppressionV3Config = {
        kernelName: tf.NonMaxSuppressionV3,
        backendName: 'webgpu',
        kernelFunc: nonMaxSuppressionV3
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function nonMaxSuppressionV5(args) {
        console.warn('tf.nonMaxSuppression() in webgpu locks the UI thread. ' +
            'Call tf.nonMaxSuppressionAsync() instead');
        const { inputs, backend, attrs } = args;
        const { boxes, scores } = inputs;
        const { maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma } = attrs;
        const boxesVals = backend.readSync(boxes.dataId);
        const scoresVals = backend.readSync(scores.dataId);
        const maxOutputSizeVal = maxOutputSize;
        const iouThresholdVal = iouThreshold;
        const scoreThresholdVal = scoreThreshold;
        const softNmsSigmaVal = softNmsSigma;
        const { selectedIndices, selectedScores } = tf.kernel_impls.nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSizeVal, iouThresholdVal, scoreThresholdVal, softNmsSigmaVal);
        return [
            backend.makeTensorInfo([selectedIndices.length], 'int32', new Int32Array(selectedIndices)),
            backend.makeTensorInfo([selectedScores.length], 'float32', new Float32Array(selectedScores))
        ];
    }
    const nonMaxSuppressionV5Config = {
        kernelName: tf.NonMaxSuppressionV5,
        backendName: 'webgpu',
        kernelFunc: nonMaxSuppressionV5
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function zerosLike(args) {
        const { inputs, backend } = args;
        const { x } = inputs;
        if (x.dtype === 'complex64') {
            const realPart = real({ inputs: { input: x }, backend });
            const r = zerosLike({ inputs: { x: realPart }, backend });
            const imagPart = imag({ inputs: { input: x }, backend });
            const i = zerosLike({ inputs: { x: imagPart }, backend });
            const result = complex({ inputs: { real: r, imag: i }, backend });
            backend.disposeData(realPart.dataId);
            backend.disposeData(r.dataId);
            backend.disposeData(imagPart.dataId);
            backend.disposeData(i.dataId);
            return result;
        }
        else {
            return fill({
                attrs: {
                    shape: x.shape,
                    dtype: x.dtype,
                    value: x.dtype === 'string' ? '' : 0
                },
                backend
            });
        }
    }
    const zerosLikeConfig = {
        kernelName: tf.ZerosLike,
        backendName: 'webgpu',
        kernelFunc: zerosLike
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function onesLike(args) {
        const { inputs, backend } = args;
        const { x } = inputs;
        if (x.dtype === 'string') {
            throw new Error('onesLike is not supported under string dtype');
        }
        else if (x.dtype === 'complex64') {
            const realPart = real({ inputs: { input: x }, backend });
            const r = onesLike({ inputs: { x: realPart }, backend });
            const imagPart = imag({ inputs: { input: x }, backend });
            const i = zerosLike({ inputs: { x: imagPart }, backend });
            const result = complex({ inputs: { real: r, imag: i }, backend });
            backend.disposeData(realPart.dataId);
            backend.disposeData(r.dataId);
            backend.disposeData(imagPart.dataId);
            backend.disposeData(i.dataId);
            return result;
        }
        else {
            return fill({ attrs: { shape: x.shape, dtype: x.dtype, value: 1 }, backend });
        }
    }
    const onesLikeConfig = {
        kernelName: tf.OnesLike,
        backendName: 'webgpu',
        kernelFunc: onesLike
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function pack(args) {
        const { inputs, backend, attrs } = args;
        const { axis } = attrs;
        if (inputs.length === 1) {
            return expandDims({ inputs: { input: inputs[0] }, backend, attrs: { dim: axis } });
        }
        const shape = inputs[0].shape;
        const dtype = inputs[0].dtype;
        inputs.forEach(t => {
            tf.util.assertShapesMatch(shape, t.shape, 'All tensors passed to stack must have matching shapes');
            tf.util.assert(dtype === t.dtype, () => 'All tensors passed to stack must have matching dtypes');
        });
        const intermediateTensorInfos = [];
        const expandedTensors = inputs.map(t => {
            const expandedT = expandDims({ inputs: { input: t }, backend, attrs: { dim: axis } });
            intermediateTensorInfos.push(expandedT);
            return expandedT;
        });
        const result = concat({ inputs: expandedTensors, backend, attrs: { axis } });
        intermediateTensorInfos.forEach(t => backend.disposeData(t.dataId));
        return result;
    }
    const packConfig = {
        kernelName: tf.Pack,
        backendName: 'webgpu',
        kernelFunc: pack
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class PadProgram {
        constructor(xShape, paddings) {
            this.variableNames = ['x'];
            this.uniforms = 'constantValue : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = paddings.map((p, i) => p[0] /* beforePad */ + xShape[i] + p[1] /* afterPad */);
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            paddings.map((_, i) => {
                this.uniforms += ` pad${i} : vec2<i32>,`;
            });
            this.xShape = xShape;
            this.shaderKey = 'pad';
        }
        getUserCode() {
            const rank = this.xShape.length;
            const type = getCoordsDataType(rank);
            // The length of paddings are same with the rank of the input tensor.
            const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(',');
            const end = this.xShape
                .map((_, i) => `uniforms.pad${i}[0] + uniforms.xShape${rank > 1 ? `[${i}]` : ''}`)
                .join(',');
            const startValue = rank > 1 ? `${type}(${start})` : `${start}`;
            const endValue = rank > 1 ? `${type}(${end})` : `${end}`;
            const leftPadCondition = rank > 1 ? `any(outC < start)` : `outC < start`;
            const rightPadCondition = rank > 1 ? `any(outC >= end)` : `outC >= end`;
            const unpackedCoords = rank > 1 ?
                ['coords[0]', 'coords[1]', 'coords[2]', 'coords[3]'].slice(0, rank) :
                'coords';
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let start = ${startValue};
          let end = ${endValue};
          let outC = getCoordsFromIndex(index);

          if (${leftPadCondition} || ${rightPadCondition}) {
            setOutputAtIndex(index, uniforms.constantValue);
          } else {
            let coords = outC - start;
            setOutputAtIndex(index, getX(${unpackedCoords}));
          }
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const padV2 = (args) => {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { paddings, constantValue } = attrs;
        if (paddings.every(p => tf.util.arraysEqual(p, [0, 0]))) {
            return identity({ inputs: { x }, backend });
        }
        if (tf.util.sizeFromShape(x.shape) === 0) {
            // Short-circuit the computation, since x doesn't have value, only
            // the shape is used to compute output shape to pad.
            const outputShape = paddings.map((p, i) => p[0] /* beforePad */ + x.shape[i] + p[1] /* afterPad */);
            return fill({
                backend,
                attrs: { shape: outputShape, value: constantValue, dtype: x.dtype }
            });
        }
        const uniformData = [{ type: 'float32', data: [constantValue] }];
        paddings.map(p => uniformData.push({ type: 'int32', data: [p[0], p[1]] }));
        const program = new PadProgram(x.shape, paddings);
        return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
    };
    const padV2Config = {
        kernelName: tf.PadV2,
        backendName: 'webgpu',
        kernelFunc: padV2
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const pow = binaryKernelFunc({
        opType: BinaryOpType.POW,
    });
    const powConfig = {
        kernelName: tf.Pow,
        backendName: 'webgpu',
        kernelFunc: pow
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function prelu(args) {
        const { inputs, backend } = args;
        const { x, alpha } = inputs;
        const program = new BinaryOpProgram(BinaryOpType.PRELU, x.shape, alpha.shape);
        return backend.runWebGPUProgram(program, [x, alpha], 'float32');
    }
    const preluConfig = {
        kernelName: tf.Prelu,
        backendName: 'webgpu',
        kernelFunc: prelu
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function prod(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { axis, keepDims } = attrs;
        return reduce(x, axis, keepDims, 'prod', backend);
    }
    const prodConfig = {
        kernelName: tf.Prod,
        backendName: 'webgpu',
        kernelFunc: prod
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const range = (args) => {
        const { backend, attrs } = args;
        const { start, stop, step, dtype } = attrs;
        const values = rangeImplCPU(start, stop, step, dtype);
        return backend.makeTensorInfo([values.length], dtype, values);
    };
    const rangeConfig = {
        kernelName: tf.Range,
        backendName: 'webgpu',
        kernelFunc: range
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const realDiv = binaryKernelFunc({ opType: BinaryOpType.DIV });
    const realDivConfig = {
        kernelName: tf.RealDiv,
        backendName: 'webgpu',
        kernelFunc: realDiv
    };

    /**
     * @license
     * Copyright 2022 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const reciprocal = unaryKernelFunc({ opType: UnaryOpType.RECIPROCAL });
    const reciprocalConfig = {
        kernelName: tf.Reciprocal,
        backendName: 'webgpu',
        kernelFunc: reciprocal
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const relu = unaryKernelFunc({ opType: UnaryOpType.RELU });
    const reluConfig = {
        kernelName: tf.Relu,
        backendName: 'webgpu',
        kernelFunc: relu
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const relu6 = unaryKernelFunc({ opType: UnaryOpType.RELU6 });
    const relu6Config = {
        kernelName: tf.Relu6,
        backendName: 'webgpu',
        kernelFunc: relu6
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ResizeBilinearProgram {
        constructor(inputShape, newHeight, newWidth) {
            this.variableNames = ['x'];
            this.uniforms = 'adjustHeightWidth : vec2<f32>, halfPixelCenters : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = `resizeBilinear`;
        }
        getUserCode() {
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let d = coords[3];
          let rc = coords.yz;

          let effectiveInSize = vec2<f32>(
            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveOutSize = vec2<f32>(
            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveInputOverOutputRatioRC =
              effectiveInSize / effectiveOutSize;

          // Fractional source index
          let sourceFracIndexRC =
            (vec2<f32>(rc) + vec2<f32>(uniforms.halfPixelCenters)) *
            effectiveInputOverOutputRatioRC - vec2<f32>(uniforms.halfPixelCenters);

          // Compute the four integer indices.
          let sourceFloorRC = vec2<i32>(sourceFracIndexRC);
          let sourceCeilRC = vec2<i32>(
            min(vec2<f32>(uniforms.xShape.yz) - vec2<f32>(1.0), ceil(sourceFracIndexRC)));

          let topLeft = getX(b, sourceFloorRC.x, sourceFloorRC.y, d);
          let bottomLeft = getX(b, sourceCeilRC.x, sourceFloorRC.y, d);
          let topRight = getX(b, sourceFloorRC.x, sourceCeilRC.y, d);
          let bottomRight = getX(b, sourceCeilRC.x, sourceCeilRC.y, d);

          let fracRC = sourceFracIndexRC - vec2<f32>(sourceFloorRC);

          let top = topLeft + (topRight - topLeft) * fracRC.y;
          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracRC.y;
          let newValue = top + (bottom - top) * fracRC.x;

          setOutputAtIndex(index, newValue);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function resizeBilinear(args) {
        const { inputs, backend, attrs } = args;
        const { images } = inputs;
        const { alignCorners, size, halfPixelCenters } = attrs;
        const [newHeight, newWidth] = size;
        const adjustHeight = alignCorners && newHeight > 1 ? 1.0 : 0.0;
        const adjustWidth = alignCorners && newWidth > 1 ? 1.0 : 0.0;
        const halfPixelCentersValue = halfPixelCenters ? 0.5 : 0.0;
        const uniformData = [
            { type: 'float32', data: [adjustHeight, adjustWidth] },
            { type: 'float32', data: [halfPixelCentersValue] }
        ];
        const program = new ResizeBilinearProgram(images.shape, newHeight, newWidth);
        return backend.runWebGPUProgram(program, [images], 'float32', uniformData);
    }
    const resizeBilinearConfig = {
        kernelName: tf.ResizeBilinear,
        backendName: 'webgpu',
        kernelFunc: resizeBilinear
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ResizeNearestNeighborProgram {
        constructor(inputShape, newHeight, newWidth, halfPixelCenters) {
            this.variableNames = ['x'];
            this.uniforms = 'adjustHeightWidth : vec2<f32>, roundBase : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.halfPixelCenters = halfPixelCenters;
            this.shaderKey = `resizeNearest_${halfPixelCenters}`;
        }
        getUserCode() {
            let sourceFracIndexRC;
            if (this.halfPixelCenters) {
                sourceFracIndexRC =
                    `max((vec2<f32>(rc) + vec2<f32>(0.5)) * effectiveInputOverOutputRatioRC` +
                        `, vec2<f32>(0.0))`;
            }
            else {
                sourceFracIndexRC = `vec2<f32>(rc) * effectiveInputOverOutputRatioRC`;
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let d = coords[3];
          let rc = coords.yz;

          let effectiveInSize = vec2<f32>(
            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveOutSize = vec2<f32>(
            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveInputOverOutputRatioRC =
              effectiveInSize / effectiveOutSize;

          // Fractional source index
          let sourceFracIndexRC = ${sourceFracIndexRC};

          // Compute the coordinators of nearest neighbor point.
          let inputShapeRC = vec2<f32>(f32(uniforms.xShape.y), f32(uniforms.xShape.z));
          let sourceNearestRC = vec2<i32>(
            min(inputShapeRC - 1.0, floor(sourceFracIndexRC + uniforms.roundBase)));
          let newValue = getX(b, sourceNearestRC.x, sourceNearestRC.y, d);

          setOutputAtIndex(index, newValue);
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function resizeNearestNeighbor(args) {
        const { inputs, backend, attrs } = args;
        const { images } = inputs;
        const { alignCorners, halfPixelCenters, size } = attrs;
        const [newHeight, newWidth] = size;
        const adjustHeight = alignCorners && newHeight > 1 ? 1.0 : 0.0;
        const adjustWidth = alignCorners && newWidth > 1 ? 1.0 : 0.0;
        // When align corners is false, we rounds the value with floor.
        const roundBase = alignCorners ? 0.5 : 0.0;
        const uniformData = [
            { type: 'float32', data: [adjustHeight, adjustWidth] },
            { type: 'float32', data: [roundBase] }
        ];
        const program = new ResizeNearestNeighborProgram(images.shape, newHeight, newWidth, halfPixelCenters);
        return backend.runWebGPUProgram(program, [images], images.dtype, uniformData);
    }
    const resizeNearestNeighborConfig = {
        kernelName: tf.ResizeNearestNeighbor,
        backendName: 'webgpu',
        kernelFunc: resizeNearestNeighbor
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class RotateProgram {
        constructor(imageShape, fillValue) {
            this.outputShape = [];
            this.variableNames = ['x'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = imageShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.uniforms = `centerX : f32, centerY : f32, sinRadians : f32,
          cosRadians : f32,`;
            this.shaderKey = 'rotate';
            this.outputShape = imageShape;
            if (typeof fillValue === 'number') {
                this.uniforms += ` fillValue : f32,`;
                this.fillSnippet = `var outputValue = uniforms.fillValue;`;
                this.shaderKey += '_float';
            }
            else {
                this.uniforms += ` fillValue : vec3<f32>,`;
                this.fillSnippet = `var outputValue = uniforms.fillValue[coords[3]];`;
                this.shaderKey += '_vec3';
            }
        }
        getUserCode() {
            const userCode = `
        ${getMainHeaderString('index')} {
          if (index < uniforms.size) {
            let coords = getCoordsFromIndex(index);
            let coordXFloat = (f32(coords[2]) - uniforms.centerX) *
                uniforms.cosRadians - (f32(coords[1]) - uniforms.centerY) *
                uniforms.sinRadians;
            let coordYFloat = (f32(coords[2]) - uniforms.centerX) *
                uniforms.sinRadians + (f32(coords[1]) - uniforms.centerY) *
                uniforms.cosRadians;
            let coordX = i32(round(coordXFloat + uniforms.centerX));
            let coordY = i32(round(coordYFloat + uniforms.centerY));
            ${this.fillSnippet}
            if(coordX >= 0 && coordX < uniforms.xShape[2] && coordY >= 0 &&
                coordY < uniforms.xShape[1]) {
              outputValue = getX(coords[0], coordY, coordX, coords[3]);
            }
            setOutputAtIndex(index, outputValue);
          }
        }
      `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const rotateWithOffsetConfig = {
        kernelName: tf.RotateWithOffset,
        backendName: 'webgpu',
        kernelFunc: ({ inputs, attrs, backend }) => {
            const { image } = inputs;
            const { radians, fillValue, center } = attrs;
            const webgpuBackend = backend;
            const program = new RotateProgram(image.shape, fillValue);
            const [centerX, centerY] = tf.backend_util.getImageCenter(center, image.shape[1], image.shape[2]);
            const uniformData = [
                { type: 'float32', data: [centerX] },
                { type: 'float32', data: [centerY] },
                { type: 'float32', data: [Math.sin(radians)] },
                { type: 'float32', data: [Math.cos(radians)] }
            ];
            if (typeof fillValue === 'number') {
                uniformData.push({ type: 'float32', data: [Number.parseFloat(fillValue.toFixed(2))] });
            }
            else {
                uniformData.push({ type: 'float32', data: fillValue });
            }
            const output = webgpuBackend.runWebGPUProgram(program, [image], image.dtype, uniformData);
            return output;
        }
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const rsqrt = unaryKernelFunc({ opType: UnaryOpType.RSQRT, cpuKernelImpl: rsqrtImplCPU });
    const rsqrtConfig = {
        kernelName: tf.Rsqrt,
        backendName: 'webgpu',
        kernelFunc: rsqrt
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class ScatterProgram {
        constructor(flattenXShape, sliceDim, indicesRank, updatesRank, strides, shape, outputDtype, sumDupeIndices = true) {
            this.variableNames = ['updates', 'indices'];
            this.workGroupSize = [64, 1, 1];
            this.atomic = true;
            this.outputShape = shape;
            this.type = outputDtype;
            this.sumDupeIndices = sumDupeIndices;
            this.dispatchLayout = flatDispatchLayout(flattenXShape);
            // Dispatching based on |updates| shape instead of output shape.
            this.dispatch =
                computeDispatch(this.dispatchLayout, flattenXShape, this.workGroupSize);
            this.sliceDimGreaterThanOne = sliceDim > 1;
            this.shaderKey = `scatter_${indicesRank}_${updatesRank}_${this.sliceDimGreaterThanOne}_${outputDtype}_${sumDupeIndices}`;
            const stridesType = getCoordsDataType(strides.length);
            this.uniforms = `sliceDim : i32, strides: ${stridesType}, size: i32,`;
            this.updatesRank = updatesRank;
            this.indicesRank = indicesRank;
        }
        getUserCode() {
            let indicesString = '';
            if (this.indicesRank === 1) {
                indicesString = 'coords[0]';
            }
            else if (this.indicesRank === 2) {
                indicesString = 'coords[0], j';
            }
            const indicesSnippet = `getIndices(${indicesString})`;
            const strideString = this.sliceDimGreaterThanOne ? 'uniforms.strides[j]' :
                'uniforms.strides';
            let outCoordsString = '';
            let getUpdatesCoordsFromFlatIndex = '';
            if (this.dispatchLayout.x.length === 1) {
                outCoordsString = 'flattenedIndex';
                getUpdatesCoordsFromFlatIndex = `
      fn getUpdatesCoordsFromFlatIndex(index : i32) -> i32 {
        return index;
      }
      `;
            }
            else if (this.dispatchLayout.x.length === 2) {
                outCoordsString = 'vec2<i32>(flattenedIndex, coords[1])';
                getUpdatesCoordsFromFlatIndex = `
      fn getUpdatesCoordsFromFlatIndex(index : i32) -> vec2<i32> {
        // N.B. |updates| could be a scalar tensor, conceptually representing a
        // 2D tensor with all values equal to that. By design, its size must be
        // the same as |outShape[1]| in one dimension, and |indicesShape[0]|
        // gives the other.
        let sliceSize = uniforms.outShape[1];
        let d0 = index / sliceSize;
        let d1 = index - d0 * sliceSize;
        return vec2<i32>(d0, d1);
      }
      `;
            }
            const updatesString = Array.from({ length: this.updatesRank }, (_, idx) => `coords[${idx}]`);
            const updatesSnippet = `getUpdates(${updatesString.join(', ')})`;
            const atomicRMW = (ptr, val) => {
                let atomicAddSnippet = `atomicAdd(${ptr}, bitcast<i32>(${val}))`;
                if (this.type === 'float32') {
                    atomicAddSnippet = `
          {
            var oldBits = 0;
            var newBits = bitcast<i32>(${val});
            loop {
              let info = atomicCompareExchangeWeak(${ptr}, oldBits, newBits);
              if (info.exchanged) {
                break;
              }
              oldBits = info.old_value;
              let oldValue = bitcast<f32>(oldBits);
              let newValue = oldValue + (${val});
              newBits = bitcast<i32>(newValue);
            }
          }
        `;
                }
                const atomicStoreSnippet = `atomicStore(${ptr}, bitcast<i32>(${val}));`;
                return this.sumDupeIndices ? atomicAddSnippet : atomicStoreSnippet;
            };
            const userCode = `
    ${getUpdatesCoordsFromFlatIndex}

      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let coords = getUpdatesCoordsFromFlatIndex(index);
          var flattenedIndex = 0;
          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {
            let indexInside = i32(round(${indicesSnippet}));
            flattenedIndex = flattenedIndex + indexInside * ${strideString};
          }
          let updateValue =
              ${mapToWgslTypes(this.type, false)}(${updatesSnippet});
          let flatIndex = getOutputIndexFromCoords(${outCoordsString});

          ${atomicRMW('&result[flatIndex]', 'updateValue')};
        }
      }`;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function scatterNd(args) {
        const { inputs, backend, attrs } = args;
        const { indices, updates } = inputs;
        const { shape } = attrs;
        const { sliceRank, numUpdates, sliceSize, strides, outputSize } = tf.backend_util.calculateShapes(updates, indices, shape);
        const flattenShape = [outputSize / sliceSize, sliceSize];
        if (outputSize === 0) {
            return backend.makeTensorInfo(shape, indices.dtype);
        }
        const flattenIndices = reshape({ inputs: { x: indices }, backend, attrs: { shape: [numUpdates, sliceRank] } });
        const flattenX = reshape({ inputs: { x: updates }, backend, attrs: { shape: [numUpdates, sliceSize] } });
        const type = flattenX.dtype;
        const output = fill({ backend, attrs: { shape: flattenShape, value: 0, dtype: type } });
        const size = tf.util.sizeFromShape(flattenX.shape);
        const uniformData = [
            { type: 'int32', data: [sliceRank] }, { type: 'int32', data: strides },
            { type: 'int32', data: [size] }
        ];
        const program = new ScatterProgram(flattenX.shape, sliceRank, flattenIndices.shape.length, flattenX.shape.length, strides, flattenShape, type);
        const res = backend.runWebGPUProgram(program, [flattenX, flattenIndices], type, uniformData, output);
        const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape } });
        backend.disposeData(flattenIndices.dataId);
        backend.disposeData(flattenX.dataId);
        backend.disposeData(res.dataId);
        return reshaped;
    }
    const scatterNdConfig = {
        kernelName: tf.ScatterNd,
        backendName: 'webgpu',
        kernelFunc: scatterNd
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class SelectProgram {
        constructor(cRank, shape, rank) {
            this.variableNames = ['c', 'a', 'b'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.cRank = cRank;
            this.rank = rank;
            this.shaderKey = 'select';
        }
        getUserCode() {
            // TODO(WGSL): below code can be merged with getUserCode.
            let cCoords;
            let abCoords;
            if (this.rank > 4) {
                throw Error(`Where for rank ${this.rank} is not yet supported`);
            }
            if (this.rank === 1) {
                abCoords = `resRC`;
                cCoords = `resRC`;
            }
            else {
                const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];
                const cCoordVars = [];
                const abCoordVars = [];
                for (let i = 0; i < this.outputShape.length; i++) {
                    abCoordVars.push(`${currentCoords[i]}`);
                    if (i < this.cRank) {
                        cCoordVars.push(`${currentCoords[i]}`);
                    }
                }
                cCoords = cCoordVars.join();
                abCoords = abCoordVars.join();
            }
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          let cVal = getC(${cCoords});
          if (cVal >= 1.0) {
            setOutputAtIndex(index, getA(${abCoords}));
          } else {
            setOutputAtIndex(index, getB(${abCoords}));
          }
        }
      }
    `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function select(args) {
        const { inputs, backend } = args;
        const { condition, t, e } = inputs;
        const program = new SelectProgram(condition.shape.length, t.shape, t.shape.length);
        return backend.runWebGPUProgram(program, [condition, t, e], tf.upcastType(t.dtype, e.dtype));
    }
    const selectConfig = {
        kernelName: tf.Select,
        backendName: 'webgpu',
        kernelFunc: select
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sigmoid = unaryKernelFunc({ opType: UnaryOpType.SIGMOID });
    const sigmoidConfig = {
        kernelName: tf.Sigmoid,
        backendName: 'webgpu',
        kernelFunc: sigmoid,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sin = unaryKernelFunc({ opType: UnaryOpType.SIN });
    const sinConfig = {
        kernelName: tf.Sin,
        backendName: 'webgpu',
        kernelFunc: sin
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sinh = unaryKernelFunc({ opType: UnaryOpType.SINH });
    const sinhConfig = {
        kernelName: tf.Sinh,
        backendName: 'webgpu',
        kernelFunc: sinh
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sub = binaryKernelFunc({ opType: BinaryOpType.SUB, cpuKernelImpl: subImplCPU, supportsComplex: true });
    const subConfig = {
        kernelName: tf.Sub,
        backendName: 'webgpu',
        kernelFunc: sub
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function softmax(args) {
        const { inputs, backend, attrs } = args;
        const { logits } = inputs;
        const { dim } = attrs;
        const axes = tf.util.parseAxisParam([dim], logits.shape);
        const maxLogit = max({
            inputs: { x: logits },
            backend,
            attrs: { reductionIndices: axes, keepDims: false }
        });
        const expandedShape = tf.backend_util.expandShapeToKeepDim(maxLogit.shape, axes);
        const maxLogitsReshaped = reshape({ inputs: { x: maxLogit }, backend, attrs: { shape: expandedShape } });
        const a = sub({ inputs: { a: logits, b: maxLogitsReshaped }, backend });
        const b = exp({ inputs: { x: a }, backend });
        const sumExp = sum({ inputs: { x: b }, backend, attrs: { axis: axes, keepDims: false } });
        const sumExpReshaped = reshape({ inputs: { x: sumExp }, backend, attrs: { shape: expandedShape } });
        const res = realDiv({ inputs: { a: b, b: sumExpReshaped }, backend });
        backend.disposeData(maxLogit.dataId);
        backend.disposeData(maxLogitsReshaped.dataId);
        backend.disposeData(a.dataId);
        backend.disposeData(b.dataId);
        backend.disposeData(sumExp.dataId);
        backend.disposeData(sumExpReshaped.dataId);
        return res;
    }
    const softmaxConfig = {
        kernelName: tf.Softmax,
        backendName: 'webgpu',
        kernelFunc: softmax
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const spaceToBatchND = (args) => {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { blockShape, paddings } = attrs;
        tf.util.assert(x.shape.length <= 4, () => 'spaceToBatchND for rank > 4 with a WebGPU backend not ' +
            'implemented yet');
        const prod = blockShape.reduce((a, b) => a * b);
        const completePaddings = [[0, 0]];
        completePaddings.push(...paddings);
        for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {
            completePaddings.push([0, 0]);
        }
        const toDispose = [];
        const paddedX = padV2({
            inputs: { x },
            backend,
            attrs: { paddings: completePaddings, constantValue: 0 }
        });
        const reshapedPaddedShape = tf.backend_util.getReshaped(paddedX.shape, blockShape, prod, false);
        const permutedReshapedPaddedPermutation = tf.backend_util.getPermuted(reshapedPaddedShape.length, blockShape.length, false);
        const flattenShape = tf.backend_util.getReshapedPermuted(paddedX.shape, blockShape, prod, false);
        const reshapedPaddedX = reshape({ inputs: { x: paddedX }, backend, attrs: { shape: reshapedPaddedShape } });
        const paddedXT = transpose({
            inputs: { x: reshapedPaddedX },
            backend,
            attrs: { perm: permutedReshapedPaddedPermutation }
        });
        const result = reshape({ inputs: { x: paddedXT }, backend, attrs: { shape: flattenShape } });
        toDispose.push(paddedX);
        toDispose.push(reshapedPaddedX);
        toDispose.push(paddedXT);
        toDispose.forEach(t => backend.disposeData(t.dataId));
        return result;
    };
    const spaceToBatchNDConfig = {
        kernelName: tf.SpaceToBatchND,
        backendName: 'webgpu',
        kernelFunc: spaceToBatchND
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class TileProgram {
        constructor(aShape, reps) {
            this.variableNames = ['A'];
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            const outputShape = new Array(aShape.length);
            for (let i = 0; i < outputShape.length; i++) {
                outputShape[i] = aShape[i] * reps[i];
            }
            this.outputShape = outputShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.rank = this.outputShape.length;
            this.shaderKey = 'tile';
        }
        getUserCode() {
            const sourceCoords = getSourceCoords(this.rank, 'uniforms.');
            const userCode = `
      ${getMainHeaderString('index')} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          setOutputAtIndex(index, getA(${sourceCoords}));
        }
      }
    `;
            return userCode;
        }
    }
    function getSourceCoords(rank, uniformPrefix = '') {
        if (rank >= 5) {
            throw Error(`Tile for rank ${rank} is not yet supported`);
        }
        if (rank === 1) {
            return `(resRC % ${uniformPrefix}aShape)`;
        }
        const currentCoords = ['resRC.x', 'resRC.y', 'resRC.z', 'resRC.w'];
        const sourceCoords = [];
        for (let i = 0; i < rank; i++) {
            sourceCoords.push(`(${currentCoords[i]} % ${uniformPrefix}aShape[${i}])`);
        }
        return sourceCoords.join();
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function tile(params) {
        const { inputs, backend, attrs } = params;
        const { x } = inputs;
        const { reps } = attrs;
        // tile gpu program cannot handle rank >= 5 case.
        if (backend.shouldExecuteOnCPU([x]) || x.dtype === 'string' ||
            x.shape.length >= 5) {
            // Even thought string tensor is always on CPU, just to be consistent on how
            // to access tensor data.
            const data = backend.readSync(x.dataId);
            const value = x.dtype === 'string' ?
                data.map(d => tf.util.decodeString(d)) :
                data;
            const buf = tf.buffer(x.shape, x.dtype, value);
            const outBuf = tileImplCPU(buf, reps);
            return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);
        }
        const program = new TileProgram(x.shape, reps);
        const output = backend.runWebGPUProgram(program, [x], x.dtype);
        return output;
    }
    const tileConfig = {
        kernelName: tf.Tile,
        backendName: 'webgpu',
        kernelFunc: tile,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function sparseToDense(args) {
        const { inputs, backend, attrs } = args;
        const { sparseIndices, sparseValues, defaultValue } = inputs;
        const { outputShape } = attrs;
        const { sliceRank, numUpdates, sliceSize, strides, outputSize } = tf.backend_util.calculateShapes(sparseValues, sparseIndices, outputShape);
        const sumDupeIndices = false;
        if (sparseValues.dtype === 'string') {
            const indicesBuf = backend.bufferSync(sparseIndices);
            const updatesBuf = backend.bufferSync(sparseValues);
            const $defaultValue = tf.util.decodeString(backend.readSync(defaultValue.dataId)[0]);
            const outBuf = scatterImplCPU(indicesBuf, updatesBuf, outputShape, outputSize, sliceSize, numUpdates, sliceRank, strides, $defaultValue, sumDupeIndices);
            return backend.makeTensorInfo(outputShape, outBuf.dtype, outBuf.values);
        }
        const flattenShape = [outputSize / sliceSize, sliceSize];
        const $sparseIndices = reshape({
            inputs: { x: sparseIndices },
            backend,
            attrs: { shape: [numUpdates, sliceRank] }
        });
        const $sparseValues = sparseValues.shape.length ?
            reshape({
                inputs: { x: sparseValues },
                backend,
                attrs: { shape: [numUpdates, sliceSize] }
            }) :
            identity({ inputs: { x: sparseValues }, backend });
        const type = $sparseValues.dtype;
        const zero = backend.makeTensorInfo([], type, tf.util.makeZerosTypedArray(1, type));
        // Fill output tensor with the default value.
        const $defaultValue = reshape({
            inputs: { x: defaultValue },
            backend,
            attrs: { shape: Array(flattenShape.length).fill(1) }
        });
        const $denseValues = tile({ inputs: { x: $defaultValue }, backend, attrs: { reps: flattenShape } });
        const size = tf.util.sizeFromShape([numUpdates, sliceSize]);
        const uniformData = [
            { type: 'int32', data: [sliceRank] },
            { type: 'int32', data: strides },
            { type: 'int32', data: [size] },
        ];
        switch (numUpdates) {
            case 0:
                break;
            case 1:
                {
                    const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, $sparseValues.shape.length, strides, flattenShape, type, sumDupeIndices);
                    backend.runWebGPUProgram(program, [$sparseValues, $sparseIndices], type, uniformData, $denseValues);
                }
                break;
            default:
                {
                    // First replace the default value with 0 at indices.
                    const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, zero.shape.length, strides, flattenShape, type, sumDupeIndices);
                    backend.runWebGPUProgram(program, [zero, $sparseIndices], type, uniformData, $denseValues);
                }
                {
                    // Then replace 0 with the (sum of) sparse value(s) at indices.
                    const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, $sparseValues.shape.length, strides, flattenShape, type);
                    backend.runWebGPUProgram(program, [$sparseValues, $sparseIndices], type, uniformData, $denseValues);
                }
        }
        const denseValues = reshape({ inputs: { x: $denseValues }, backend, attrs: { shape: outputShape } });
        backend.disposeData($sparseIndices.dataId);
        backend.disposeData($sparseValues.dataId);
        backend.disposeData($defaultValue.dataId);
        backend.disposeData(zero.dataId);
        backend.disposeData($denseValues.dataId);
        return denseValues;
    }
    const sparseToDenseConfig = {
        kernelName: tf.SparseToDense,
        backendName: 'webgpu',
        kernelFunc: sparseToDense
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function splitV(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { numOrSizeSplits, axis } = attrs;
        const $axis = tf.util.parseAxisParam(axis, x.shape)[0];
        const splitSizes = tf.backend_util.prepareSplitSize(x, numOrSizeSplits, $axis);
        const xRank = x.shape.length;
        const begin = new Array(xRank).fill(0);
        const size = x.shape.slice();
        return splitSizes.map(s => {
            const sliceSize = [...size];
            sliceSize[$axis] = s;
            const sliceT = slice({ inputs: { x }, backend, attrs: { begin, size: sliceSize } });
            begin[$axis] += s;
            return sliceT;
        });
    }
    const splitVConfig = {
        kernelName: tf.SplitV,
        backendName: 'webgpu',
        kernelFunc: splitV
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const sqrt = unaryKernelFunc({ opType: UnaryOpType.SQRT });
    const sqrtConfig = {
        kernelName: tf.Sqrt,
        backendName: 'webgpu',
        kernelFunc: sqrt
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const squareConfig = {
        kernelName: tf.Square,
        backendName: 'webgpu',
        kernelFunc: ({ inputs, backend }) => {
            const { x } = inputs;
            const webGPUBackend = backend;
            const program = new UnaryOpProgram(x.shape, UnaryOpType.SQUARE);
            return webGPUBackend.runWebGPUProgram(program, [x], x.dtype);
        }
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const squaredDifference = binaryKernelFunc({
        opType: BinaryOpType.SQUARED_DIFFERENCE,
    });
    const squaredDifferenceConfig = {
        kernelName: tf.SquaredDifference,
        backendName: 'webgpu',
        kernelFunc: squaredDifference
    };

    /**
     * @license
     * Copyright 2019 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class StridedSliceProgram {
        constructor(destSize) {
            this.variableNames = ['x'];
            // TODO(xing.xu): Increase the workPerThread.
            this.workPerThread = 1;
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = destSize;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize, [this.workPerThread, 1, 1]);
            const dtype = getCoordsDataType(this.outputShape.length);
            this.uniforms = `begin : ${dtype},  strides : ${dtype}, `;
            this.shaderKey = 'stridedSlice';
        }
        getUserCode() {
            const rank = this.outputShape.length;
            let newCoords = '';
            if (rank === 1) {
                newCoords = 'coords * uniforms.strides + uniforms.begin';
            }
            else {
                let outputAxis = 0;
                newCoords =
                    this.outputShape
                        .map((_, i) => {
                        outputAxis++;
                        return this.outputShape.length === 1 ?
                            `coords * uniforms.strides[${i}] + uniforms.begin[${i}]` :
                            `coords[${outputAxis - 1}] * uniforms.strides[${i}] + uniforms.begin[${i}]`;
                    })
                        .join(',');
            }
            const userCode = `
       ${getMainHeaderString('index')} {
         if (index < uniforms.size) {
           let coords = getCoordsFromIndex(index);
           setOutputAtIndex(index, getX(${newCoords}));
         }
       }
     `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function stridedSlice(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask } = attrs;
        const { finalShapeSparse, finalShape, isIdentity, sliceDim0, isSimpleSlice, begin: $begin, end: $end, strides: $strides } = tf.slice_util.sliceInfo(x.shape, begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask);
        let result;
        if (isIdentity) {
            // Optimization #1, slice is a no-op plus reshape
            result = reshape({ inputs: { x }, backend, attrs: { shape: finalShape } });
        }
        else if (sliceDim0 || isSimpleSlice) {
            // Optimization #2, slice is memory contiguous (only occurs in dim 0)
            tf.util.assert(x.shape.length >= 1, () => `Input must have rank at least 1, got: ${x.shape.length}`);
            const size = tf.slice_util.computeOutShape($begin, $end, $strides);
            // To tolerate begin[0] > end[0] (a 0-output slice), we min(begin, end).
            const sliced = slice({ inputs: { x }, backend, attrs: { begin: $begin, size } });
            result =
                reshape({ inputs: { x: sliced }, backend, attrs: { shape: finalShape } });
            backend.disposeData(sliced.dataId);
        }
        else {
            const shouldExecuteOnCPU = backend.shouldExecuteOnCPU([x]);
            if (shouldExecuteOnCPU) {
                const values = backend.readSync(x.dataId);
                const xBuf = tf.buffer(x.shape, x.dtype, values);
                const resultValues = stridedSliceImplCPU(finalShapeSparse, xBuf, $strides, $begin);
                result = backend.makeTensorInfo(finalShape, x.dtype, resultValues.values);
            }
            else {
                const program = new StridedSliceProgram(finalShapeSparse);
                const uniformData = [{ type: 'int32', data: $begin }, { type: 'int32', data: $strides }];
                const resultValues = backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
                result = reshape({ inputs: { x: resultValues }, backend, attrs: { shape: finalShape } });
                backend.disposeData(resultValues.dataId);
            }
        }
        return result;
    }
    const stridedSliceConfig = {
        kernelName: tf.StridedSlice,
        backendName: 'webgpu',
        kernelFunc: stridedSlice
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function stringNGrams(args) {
        const { inputs, backend, attrs } = args;
        const { separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences } = attrs;
        const { data, dataSplits } = inputs;
        const $data = backend.readSync(data.dataId);
        const $dataSplits = backend.readSync(dataSplits.dataId);
        const [nGrams, nGramsSplits] = stringNGramsImplCPU($data, $dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences);
        return [
            backend.makeTensorInfo([nGrams.length], 'string', nGrams),
            backend.makeTensorInfo(dataSplits.shape, 'int32', nGramsSplits),
        ];
    }
    const stringNGramsConfig = {
        kernelName: tf.StringNGrams,
        backendName: 'webgpu',
        kernelFunc: stringNGrams,
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    const tanh = unaryKernelFunc({ opType: UnaryOpType.TANH });
    const tanhConfig = {
        kernelName: tf.Tanh,
        backendName: 'webgpu',
        kernelFunc: tanh
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // Based on Algorithm 2 of Bitonic Top K, ref:
    // https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf
    // The original algorithm is based on computing the top K only, however
    // since for TFJS we require the indices of the top K values as well then the
    // algorithm found here is a bit modified. Rather than producing the values
    // at each step, the indices containing the top K are generated instead.
    // The output values are not generated to reduce the number of outputs in the
    // GPU, the values can easily be retrieved from the indices using a gather
    // op.
    class SwapProgram {
        constructor(shape) {
            this.variableNames = ['x', 'indices'];
            this.workGroupSize = [256, 1, 1];
            this.size = true;
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.uniforms = `inputSize : i32, firstPass : i32, negativeInf : f32,
        dir : i32, inc : i32,`;
            this.shaderKey = 'swap';
        }
        getUserCode() {
            const userCode = `
        ${getMainHeaderString('index')} {
          if (index < uniforms.size) {
            let outC = getCoordsFromIndex(index);
            let batch = outC[0];
            let elemIdx = outC[1];
            // We compare elements pair-wise within a group of size 2 * inc.
            // The comparing rule for each group alternates between ascending
            // and descending. Within each group, we compare each pair at
            // positions i and i+inc. To decide whether an element at position i
            // is x0 or x1, we mod it by 2 * inc, if the result is smaller than
            // inc, it is in the first half of the group, we denote it as x0,
            // otherwise we denote it as x1.
            // For example, as shown in the Bitonic top K paper referenced
            // above, Figure5(a) shows that element[1] is in the second half of
            // the group when group size is 2, but it is in the first half of
            // the group when group size is 4.
            let isFirstInPair = elemIdx % (2 * uniforms.inc) < uniforms.inc;
            var i = 0;
            if (isFirstInPair) {
              i = elemIdx;
            } else {
              i = elemIdx - uniforms.inc;
            }

            var i0 = 0;
            if (uniforms.firstPass == 1) {
              i0 = i;
            } else {
              i0 = i32(getIndices(batch, i));
            }

            var i1 = 0;
            if (uniforms.firstPass == 1) {
              i1 = i + uniforms.inc;
            } else {
              i1 = i32(getIndices(batch, i + uniforms.inc));
            }

            var x0 = f32(0.0);
            var x1 = f32(0.0);
            if (i0 < uniforms.inputSize) {
              x0 = getX(batch, i0);
            } else {
              x0 = uniforms.negativeInf;
            }
            if (i1 < uniforms.inputSize) {
              x1 = getX(batch, i1);
            } else {
              x1 = uniforms.negativeInf;
            }

            let reverse = elemIdx % (2 * uniforms.dir) >= uniforms.dir;
            let isGreater = x0 > x1 || (x0 == x1 && i1 > i0);
            if (reverse == isGreater) {
              // Elements in opposite order of direction
              let iTemp = i0;
              i0 = i1;
              i1 = iTemp;
            }
            if (isFirstInPair) {
              setOutputAtIndex(index, f32(i0));
            } else {
              setOutputAtIndex(index, f32(i1));
            }
          }
        }
      `;
            return userCode;
        }
    }
    class MergeProgram {
        constructor(shape) {
            this.variableNames = ['x', 'indices'];
            this.workGroupSize = [256, 1, 1];
            this.size = true;
            this.outputShape = shape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            // |n| Size of the original input of TopK
            // |firstPass| indicates if this is the first time swap is being used which
            // means no indices input containing the top K is present yet.
            // |k| Top k elements desired
            this.uniforms = `inputSize : i32, firstPass : i32, k : i32,`;
            this.shaderKey = 'merge';
        }
        getUserCode() {
            const userCode = `
        ${getMainHeaderString('index')} {
          if (index < uniforms.size) {
            let outC = getCoordsFromIndex(index);
            let batch = outC[0];
            let elemIdx = outC[1];
            // The output size is half of the previous size.
            // If the previous sequence is | | | | _ _ _ _  | | | |  _ _ _ _
            // (k=4), we only need to output the indices at positions |, the
            // indices at positions _ can be thrown away, see Figure5(b) After
            // Phase 2 (Merge phase) in the Bitonic Top K paper referenced
            // above.
            // For example, the paper shows we only need to output the orange
            // bars. The output sequence should look like this | | | | | | | |.
            // Because the sequence is halved, to map the output index back to
            // the previous sequence to find the corresponding value, we need
            // to double the index. When we double the index, we basically
            // interpolate a position, so 2i looks like
            // | _ | _ | _ | _ | _ | _ | _. We move the | to the first k
            // position of each 2k positions by - elemIdx % k. E.g. for output
            // at index 4,5,6,7, we want to get the corresponding element at
            // original index 8,9,10,11, for output at index 8,9,10,11,
            // we want to get the corresponding element at original index
            // 16,17,18,19, so on and so forth.

            var i = 0;
            if (elemIdx < uniforms.k) {
              i = elemIdx;
            } else {
              i = elemIdx * 2 - elemIdx % uniforms.k;
            }
            var i0 = 0;
            if (uniforms.firstPass == 1) {
              i0 = i;
            } else {
              i0 = i32(getIndices(batch, i));
            }
            var i1 = 0;
            if (uniforms.firstPass == 1) {
              i1 = i + uniforms.k;
            } else {
              i1 = i32(getIndices(batch, i + uniforms.k));
            }

            let x0 = getX(batch, i0);
            var x1 = f32(0.0);
            if (i1 < uniforms.inputSize) {
              x1 = getX(batch, i1);
            } else {
              x1 = x0;
            }

            if (x0 >= x1) {
              setOutputAtIndex(index, f32(i0));
            } else {
              setOutputAtIndex(index, f32(i1));
            }
          }
        }
      `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function disposeIntermediateTensorInfoOrNull(backend, tensorInfo) {
        if (tensorInfo !== null) {
            backend.disposeData(tensorInfo.dataId);
        }
    }
    function roundUpToPow2(num) {
        let pow2 = 1;
        while (pow2 < num) {
            pow2 *= 2;
        }
        return pow2;
    }
    // Based on Algorithm 2 of Bitonic Top K, ref:
    // https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf
    function topK(args) {
        const { inputs, backend, attrs } = args;
        const { x } = inputs;
        const { k, sorted } = attrs;
        const xShape = x.shape;
        const lastDim = xShape[xShape.length - 1];
        if (backend.shouldExecuteOnCPU([x])) {
            const xVals = backend.readSync(x.dataId);
            const [allTopKVals, allTopKIndices] = topKImplCPU(xVals, xShape, x.dtype, k, sorted);
            return [
                backend.makeTensorInfo(allTopKVals.shape, allTopKVals.dtype, allTopKVals.values),
                backend.makeTensorInfo(allTopKIndices.shape, allTopKIndices.dtype, allTopKIndices.values)
            ];
        }
        if (k === 0) {
            xShape[xShape.length - 1] = 0;
            return [
                backend.makeTensorInfo(xShape, x.dtype, []),
                backend.makeTensorInfo(xShape, 'int32', [])
            ];
        }
        if (lastDim === 1 /* firstPass */) {
            return [
                x, fill({ attrs: { shape: xShape, dtype: 'int32', value: 0 }, backend })
            ];
        }
        // Reshape into a 2d tensor [batch, lastDim] and compute topk along lastDim.
        const xSize = tf.util.sizeFromShape(xShape);
        const batch = xSize / lastDim;
        const x2D = reshape({ inputs: { x }, attrs: { shape: [batch, lastDim] }, backend });
        const kPow2 = roundUpToPow2(k);
        const lastDimPow2 = roundUpToPow2(lastDim);
        // Only the indices containing the top K are kept at every step to reduce
        // number of outputs in the GPU algorithms, so once the final set of indices
        // is computed then gather is used to grab the corresponding values
        // from the original input.
        let indices = null;
        // GPU algorithm always takes in an indices input but this input is not used
        // on the first run of a GPU algorithm, therefore if indices is null we simply
        // pass in x2D instead of it but the value will not actually be used
        const getInputs = () => indices === null ? [x2D, x2D] : [x2D, indices];
        const runSwap = (dir, inc, shape) => {
            const inputs = getInputs();
            const program = new SwapProgram(shape);
            const firstPass = indices === null ? 1 : 0;
            const uniformDataSwap = [
                { type: 'int32', data: [lastDim] },
                { type: 'int32', data: [firstPass] },
                { type: 'float32', data: [Number.NEGATIVE_INFINITY] },
                { type: 'int32', data: [dir] },
                { type: 'int32', data: [inc] }
            ];
            const prevIndices = indices;
            indices = backend.runWebGPUProgram(program, inputs, 'int32', uniformDataSwap);
            disposeIntermediateTensorInfoOrNull(backend, prevIndices);
        };
        // Step 1: local sort
        for (let len = 1; len < kPow2; len *= 2) {
            const dir = len * 2;
            for (let inc = len; inc >= 1; inc /= 2) {
                runSwap(dir, inc, [batch, lastDimPow2]);
            }
        }
        // Step 2: merge
        for (let indicesSize = lastDimPow2; indicesSize > kPow2; indicesSize /= 2) {
            const inputs = getInputs();
            const mergeProgram = new MergeProgram([batch, indicesSize / 2]);
            const firstPass = indices === null ? 1 : 0;
            const uniformDataMerge = [
                { type: 'int32', data: [lastDim] },
                { type: 'int32', data: [firstPass] },
                { type: 'int32', data: [kPow2] }
            ];
            const prevIndices = indices;
            indices = backend.runWebGPUProgram(mergeProgram, inputs, 'int32', uniformDataMerge);
            disposeIntermediateTensorInfoOrNull(backend, prevIndices);
            // Step 3: rebuild
            const len = kPow2 / 2;
            const dir = len * 2;
            for (let inc = len; inc >= 1; inc /= 2) {
                runSwap(dir, inc, indices.shape);
            }
        }
        // Keep only the requested top K results instead of kPow2
        let prevIndices = indices;
        indices = slice({ inputs: { x: indices }, backend, attrs: { begin: 0, size: [batch, k] } });
        disposeIntermediateTensorInfoOrNull(backend, prevIndices);
        // Gather values on last dimension
        let values = gatherV2({ inputs: { x: x2D, indices }, backend, attrs: { axis: 1, batchDims: 1 } });
        disposeIntermediateTensorInfoOrNull(backend, x2D);
        // Reshape back to the original input shape, except that the last
        // dimension is k.
        const newShape = xShape.slice(0, -1);
        newShape.push(k);
        prevIndices = indices;
        indices = reshape({ inputs: { x: indices }, attrs: { shape: newShape }, backend });
        disposeIntermediateTensorInfoOrNull(backend, prevIndices);
        const prevValues = values;
        values = reshape({ inputs: { x: values }, attrs: { shape: newShape }, backend });
        disposeIntermediateTensorInfoOrNull(backend, prevValues);
        return [values, indices];
    }
    const topKConfig = {
        kernelName: tf.TopK,
        backendName: 'webgpu',
        kernelFunc: topK
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    class TransformProgram {
        constructor(outShape) {
            this.variableNames = ['Image', 'Transforms'];
            this.uniforms = 'interpolationModeId : i32, fillModeId : i32, fillValue : f32,';
            this.workGroupSize = [64, 1, 1];
            this.size = true;
            this.outputShape = outShape;
            this.dispatchLayout = flatDispatchLayout(this.outputShape);
            this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workGroupSize);
            this.shaderKey = 'transform';
        }
        getUserCode() {
            const userCode = `
          fn mapCoord(outCoord : f32, len : f32) -> f32{
            var inCoord = outCoord;
            if(uniforms.fillModeId == 2) {
              if (inCoord < 0.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz2 = 2.0 * len;
                  if (inCoord < sz2) {
                    inCoord = sz2 * f32(i32(f32(-inCoord / sz2))) +
                    inCoord;
                  }
                  if (inCoord < -len) {
                    inCoord = inCoord + sz2;
                  } else {
                    inCoord = -inCoord - 1.0;
                  }
                }
              } else if (inCoord > len - 1.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz2 = 2.0 * len;
                  inCoord = inCoord - sz2 * f32(i32(f32(inCoord / sz2)));
                  if (inCoord >= len) {
                    inCoord = sz2 - inCoord - 1.0;
                  }
                }
              }
              return clamp(inCoord, 0.0, len - 1.0);
            } else if (uniforms.fillModeId == 3) {
              if (inCoord < 0.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz = len - 1.0;
                  inCoord = inCoord + len * (f32(i32(f32(-inCoord / sz))) + 1.0);
                }
              } else if (inCoord > len - 1.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz = len - 1.0;
                  inCoord = inCoord - len * f32(i32(f32(inCoord / sz)));
                }
              }
              return clamp(inCoord, 0.0, len - 1.0);
            } else if (uniforms.fillModeId == 4) {
              return clamp(outCoord, 0.0, len - 1.0);
            }
            return outCoord;
          }
          fn readWithFillValue(batch : i32, coordY : i32, coordX : i32,
            channel : i32) -> f32 {
            var outputValue : f32;
            if (0 <= coordY && coordY < uniforms.imageShape[1] && 0 <= coordX && coordX < uniforms.imageShape[2]) {
                outputValue = getImage(batch, coordY, coordX, channel);
            } else {
              outputValue = uniforms.fillValue;
            }
            return outputValue;
          }

          ${getMainHeaderString('index')} {
            if (index < uniforms.size) {
              let coords = getCoordsFromIndex(index);
              var outputValue : f32;
              let batch = coords[0];
              let x = coords[2];
              let y = coords[1];
              let channel = coords[3];
              let xf = f32(x);
              let yf = f32(y);
              let a1 = getTransforms(batch, 0);
              let a2 = getTransforms(batch, 1);
              let a3 = getTransforms(batch, 2);
              let b1 = getTransforms(batch, 3);
              let b2 = getTransforms(batch, 4);
              let b3 = getTransforms(batch, 5);
              let c1 = getTransforms(batch, 6);
              let c2 = getTransforms(batch, 7);
              let projection = c1 * xf + c2 * yf + 1.0;
              if (projection == 0.0) {
                outputValue = uniforms.fillValue;
              } else {
                let inX = (a1 * xf + a2 * yf + a3) / projection;
                let inY = (b1 * xf + b2 * yf + b3) / projection;
                let mapX = mapCoord(inX, f32(uniforms.imageShape[2]));
                let mapY = mapCoord(inY, f32(uniforms.imageShape[1]));

                if (uniforms.interpolationModeId == 1) {
                  let coordY = i32(round(mapY));
                  let coordX = i32(round(mapX));
                  outputValue = readWithFillValue(batch, coordY, coordX,
                    channel);
                } else {
                  let yFloor = floor(mapY);
                  let xFloor = floor(mapX);
                  let yCeil = yFloor + 1.0;
                  let xCeil = xFloor + 1.0;
                  let valueYFloor = (xCeil - mapX) *
                  readWithFillValue(batch, i32(yFloor), i32(xFloor), channel) +
                  (mapX - xFloor) *
                  readWithFillValue(batch, i32(yFloor), i32(xCeil), channel);
                  let valueYCeil = (xCeil - mapX) *
                  readWithFillValue(batch, i32(yCeil), i32(xFloor), channel) +
                  (mapX - xFloor) *
                  readWithFillValue(batch, i32(yCeil), i32(xCeil), channel);
                  outputValue = (yCeil - mapY) * valueYFloor +
                  (mapY - yFloor) * valueYCeil;
                }
              }
              setOutputAtIndex(index, outputValue);
            }
          }
        `;
            return userCode;
        }
    }

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function transform(args) {
        const { inputs, backend, attrs } = args;
        const { image, transforms } = inputs;
        const { interpolation, fillMode, fillValue, outputShape } = attrs;
        const [batch, imageHeight, imageWidth, numChannels] = image.shape;
        const [outHeight, outWidth] = outputShape != null ? outputShape : [imageHeight, imageWidth];
        const outShape = [batch, outHeight, outWidth,
            numChannels];
        const program = new TransformProgram(outShape);
        const interpolationModeId = interpolation === 'nearest' ? 1 : 2;
        let fillModeId;
        switch (fillMode) {
            case 'constant':
                fillModeId = 1;
                break;
            case 'reflect':
                fillModeId = 2;
                break;
            case 'wrap':
                fillModeId = 3;
                break;
            case 'nearest':
                fillModeId = 4;
                break;
            default:
                fillModeId = 1;
                break;
        }
        const uniformData = [
            { type: 'int32', data: [interpolationModeId] },
            { type: 'int32', data: [fillModeId] }, { type: 'float32', data: [fillValue] }
        ];
        return backend.runWebGPUProgram(program, [image, transforms], 'float32', uniformData);
    }
    const transformConfig = {
        kernelName: tf.Transform,
        backendName: 'webgpu',
        kernelFunc: transform
    };

    /**
     * @license
     * Copyright 2021 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    function unpack(args) {
        const { inputs, backend, attrs } = args;
        const { value } = inputs;
        let { axis } = attrs;
        if (axis < 0) {
            axis += value.shape.length;
        }
        const x = value;
        const xRank = x.shape.length;
        const num = value.shape[axis];
        const outShape = new Array(xRank - 1);
        let outIndex = 0;
        for (let i = 0; i < xRank; i++) {
            if (i !== axis) {
                outShape[outIndex++] = x.shape[i];
            }
        }
        const toDispose = [];
        const begin = new Array(xRank).fill(0);
        const size = x.shape.slice();
        size[axis] = 1;
        const res = new Array(num);
        for (let i = 0; i < res.length; i++) {
            begin[axis] = i;
            const sliced = slice({ inputs: { x }, backend, attrs: { begin, size } });
            const reshaped = reshape({ inputs: { x: sliced }, backend, attrs: { shape: outShape } });
            res[i] = reshaped;
            toDispose.push(sliced);
        }
        toDispose.forEach(t => backend.disposeData(t.dataId));
        return res;
    }
    const unpackConfig = {
        kernelName: tf.Unpack,
        backendName: 'webgpu',
        kernelFunc: unpack
    };

    /**
     * @license
     * Copyright 2020 Google LLC. All Rights Reserved.
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     * http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     * =============================================================================
     */
    // List all kernel configs here
    const kernelConfigs = [
        _fusedMatMulConfig,
        absConfig,
        addConfig,
        addNConfig,
        argMaxConfig,
        argMinConfig,
        atan2Config,
        avgPoolConfig,
        batchMatMulConfig,
        batchToSpaceNDConfig,
        castConfig,
        ceilConfig,
        clipByValueConfig,
        complexConfig,
        concatConfig,
        conv2DConfig,
        conv2DBackpropInputConfig,
        cosConfig,
        coshConfig,
        cropAndResizeConfig,
        cumprodConfig,
        cumsumConfig,
        depthToSpaceConfig,
        depthwiseConv2dNativeConfig,
        einsumConfig,
        eluConfig,
        equalConfig,
        expConfig,
        expandDimsConfig,
        expm1Config,
        fillConfig,
        flipLeftRightConfig,
        fromPixelsConfig,
        floorConfig,
        floorDivConfig,
        fusedBatchNormConfig,
        fusedConv2DConfig,
        fusedDepthwiseConv2DConfig,
        gatherNdConfig,
        gatherV2Config,
        greaterConfig,
        greaterEqualConfig,
        identityConfig,
        imagConfig,
        isNaNConfig,
        leakyReluConfig,
        lessConfig,
        lessEqualConfig,
        logConfig,
        logicalAndConfig,
        logicalNotConfig,
        maxConfig,
        maximumConfig,
        maxPoolConfig,
        meanConfig,
        minConfig,
        minimumConfig,
        mirrorPadConfig,
        multiplyConfig,
        negConfig,
        nonMaxSuppressionV3Config,
        nonMaxSuppressionV5Config,
        notEqualConfig,
        onesLikeConfig,
        packConfig,
        padV2Config,
        powConfig,
        preluConfig,
        prodConfig,
        rangeConfig,
        realConfig,
        realDivConfig,
        reciprocalConfig,
        reluConfig,
        relu6Config,
        reshapeConfig,
        resizeBilinearConfig,
        resizeNearestNeighborConfig,
        rotateWithOffsetConfig,
        rsqrtConfig,
        scatterNdConfig,
        selectConfig,
        sigmoidConfig,
        sinConfig,
        sinhConfig,
        sliceConfig,
        stridedSliceConfig,
        stringNGramsConfig,
        softmaxConfig,
        spaceToBatchNDConfig,
        sparseToDenseConfig,
        splitVConfig,
        sqrtConfig,
        squareConfig,
        squaredDifferenceConfig,
        subConfig,
        sumConfig,
        tanhConfig,
        tileConfig,
        topKConfig,
        transformConfig,
        transposeConfig,
        unpackConfig,
        zerosLikeConfig
    ];
    for (const kernelConfig of kernelConfigs) {
        tf.registerKernel(kernelConfig);
    }

    exports.WebGPUBackend = WebGPUBackend;
    exports.webgpu_util = webgpu_util;

    Object.defineProperty(exports, '__esModule', { value: true });

})));
//# sourceMappingURL=tf-backend-webgpu.es2017.js.map
